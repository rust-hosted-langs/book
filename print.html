<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Writing Interpreters in Rust: a Guide</title>
        
        <meta name="robots" content="noindex" />
        
        


        <!-- Custom HTML head -->
        


        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        
        <link rel="icon" href="favicon.svg">
        
        
        <link rel="shortcut icon" href="favicon.png">
        
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        
        <link rel="stylesheet" href="css/print.css" media="print">
        

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        
        <link rel="stylesheet" href="fonts/fonts.css">
        

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="introduction.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li class="chapter-item expanded "><a href="part-allocators.html"><strong aria-hidden="true">2.</strong> Allocation</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="chapter-alignment.html"><strong aria-hidden="true">2.1.</strong> Alignment</a></li><li class="chapter-item expanded "><a href="chapter-blocks.html"><strong aria-hidden="true">2.2.</strong> Obtaining blocks of memory</a></li><li class="chapter-item expanded "><a href="chapter-what-is-alloc.html"><strong aria-hidden="true">2.3.</strong> The type of allocation</a></li></ol></li><li class="chapter-item expanded "><a href="part-stickyimmix.html"><strong aria-hidden="true">3.</strong> An allocator: Sticky Immix</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="chapter-simple-bump.html"><strong aria-hidden="true">3.1.</strong> Bump allocation</a></li><li class="chapter-item expanded "><a href="chapter-managing-blocks.html"><strong aria-hidden="true">3.2.</strong> Allocating into multiple blocks</a></li><li class="chapter-item expanded "><a href="chapter-allocation-api.html"><strong aria-hidden="true">3.3.</strong> Defining the allocation API</a></li><li class="chapter-item expanded "><a href="chapter-allocation-impl.html"><strong aria-hidden="true">3.4.</strong> Implementing the API</a></li></ol></li><li class="chapter-item expanded "><a href="part-interpreter.html"><strong aria-hidden="true">4.</strong> An interpreter: Eval-rs</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="chapter-interp-alloc.html"><strong aria-hidden="true">4.1.</strong> Allocating objects and dereferencing safely</a></li><li class="chapter-item expanded "><a href="chapter-interp-tagged-ptrs.html"><strong aria-hidden="true">4.2.</strong> Tagged pointers and object headers</a></li><li class="chapter-item expanded "><a href="chapter-interp-symbols-and-pairs.html"><strong aria-hidden="true">4.3.</strong> Symbols and Pairs</a></li><li class="chapter-item expanded "><a href="chapter-interp-parsing.html"><strong aria-hidden="true">4.4.</strong> Parsing s-expressions</a></li><li class="chapter-item expanded "><a href="chapter-interp-arrays.html"><strong aria-hidden="true">4.5.</strong> Arrays</a></li><li class="chapter-item expanded "><a href="chapter-interp-bytecode.html"><strong aria-hidden="true">4.6.</strong> Bytecode</a></li><li class="chapter-item expanded "><a href="chapter-interp-dicts.html"><strong aria-hidden="true">4.7.</strong> Dicts</a></li><li class="chapter-item expanded "><a href="chapter-interp-vm-design.html"><strong aria-hidden="true">4.8.</strong> Virtual Machine: Design</a></li><li class="chapter-item expanded "><a href="chapter-interp-vm-impl.html"><strong aria-hidden="true">4.9.</strong> Virtual Machine: Implementation</a></li><li class="chapter-item expanded "><a href="chapter-interp-compiler-design.html"><strong aria-hidden="true">4.10.</strong> Compiler: Design</a></li><li class="chapter-item expanded "><a href="chapter-interp-compiler-impl.html"><strong aria-hidden="true">4.11.</strong> Compiler: Implementation</a></li></ol></li><li class="chapter-item expanded "><a href="404.html"><strong aria-hidden="true">5.</strong> Garbage collection</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="404.html"><strong aria-hidden="true">5.1.</strong> TODO - Tracing</a></li><li class="chapter-item expanded "><a href="404.html"><strong aria-hidden="true">5.2.</strong> TODO - Sweeping</a></li><li class="chapter-item expanded "><a href="404.html"><strong aria-hidden="true">5.3.</strong> TODO - Recycling blocks</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        
                    </div>

                    <h1 class="menu-title">Writing Interpreters in Rust: a Guide</h1>

                    <div class="right-buttons">
                        
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                        
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1><a class="header" href="#writing-interpreters-in-rust-a-guide" id="writing-interpreters-in-rust-a-guide">Writing Interpreters in Rust: a Guide</a></h1>
<h2><a class="header" href="#welcome" id="welcome">Welcome!</a></h2>
<p>In this book we will walk through the basics of interpreted language
implementation in Rust with a focus on the challenges that are specific
to using Rust.</p>
<p>At a glance, these are:</p>
<ul>
<li>A custom allocator for use in an interpreter</li>
<li>A safe-Rust wrapper over allocation</li>
<li>A compiler and VM that interact with the above two layers</li>
</ul>
<p>The goal of this book is not to cover a full featured language but rather to
provide a solid foundation on which you can build further features. Along
the way we'll implement as much as possible in terms of our own memory
management abstractions rather than using Rust std collections.</p>
<h3><a class="header" href="#level-of-difficulty" id="level-of-difficulty">Level of difficulty</a></h3>
<p>Bob Nystrom's <a href="http://craftinginterpreters.com/">Crafting Interpreters</a>
is recommended <em>introductory</em> reading to this book for beginners to the topic.
Bob has produced a high quality, accessible work and while there is
considerable overlap, in some ways this book builds on Bob's work with some
additional complexity, optimizations and discussions of Rust's safe vs unsafe.</p>
<p><strong>We hope you find this book to be informative!</strong></p>
<h2><a class="header" href="#further-reading-and-other-projects-to-study" id="further-reading-and-other-projects-to-study">Further reading and other projects to study:</a></h2>
<p>All the links below are acknowledged as inspiration or prior art.</p>
<h3><a class="header" href="#interpreters" id="interpreters">Interpreters</a></h3>
<ul>
<li>Bob Nystrom's <a href="http://craftinginterpreters.com/">Crafting Interpreters</a></li>
<li><a href="https://inko-lang.org/">The Inko programming language</a></li>
<li>kyren - <a href="https://github.com/kyren/luster">luster</a> and <a href="https://github.com/kyren/gc-arena">gc-arena</a></li>
</ul>
<h3><a class="header" href="#memory-management" id="memory-management">Memory management</a></h3>
<ul>
<li>Richard Jones, Anthony Hosking, Elliot Moss - <a href="http://gchandbook.org/">The Garbage Collection Handbook</a></li>
<li>Stephen M. Blackburn &amp; Kathryn S. McKinley -
<a href="http://users.cecs.anu.edu.au/%7Esteveb/pubs/papers/immix-pldi-2008.pdf">Immix: A Mark-Region Garbage Collector with Space Efficiency, Fast Collection, and Mutator Performance</a></li>
<li>Felix S Klock II - <a href="http://blog.pnkfx.org/blog/2015/10/27/gc-and-rust-part-0-how-does-gc-work/">GC and Rust Part 0: Garbage Collection Background</a></li>
<li>Felix S Klock II - <a href="http://blog.pnkfx.org/blog/2015/11/10/gc-and-rust-part-1-specing-the-problem/">GC and Rust Part 1: Specifying the Problem</a></li>
<li>Felix S Klock II - <a href="http://blog.pnkfx.org/blog/2016/01/01/gc-and-rust-part-2-roots-of-the-problem/">GC and Rust Part 2: The Roots of the Problem</a></li>
</ul>
<h1><a class="header" href="#allocators" id="allocators">Allocators</a></h1>
<p>This section gives an overview and implementation detail of allocating blocks
of memory.</p>
<p><em>What this is not: a custom allocator to replace the global Rust allocator</em></p>
<h1><a class="header" href="#alignment" id="alignment">Alignment</a></h1>
<p>There are subtleties in memory access alignment:</p>
<ul>
<li>Some hardware architectures and implementations may fault on unaligned
memory access.</li>
<li>Atomic operations require word-aligned access.</li>
<li>SIMD operations typically require double-word-aligned access.</li>
<li>In practice on 64 bit architectures, allocators align objects to 8 byte
boundaries for 64 bit objects and smaller and 16 byte boundaries for larger
objects for performance optimization and the above reasons.</li>
</ul>
<p>Intel 32 and 64 bit x86 architectures allow general access to be unaligned but
will probably incur an access penalty. The story on 32bit ARM and aarch64 is
sufficiently similar but there is a higher chance that an ARM core is
configured to raise a bus error on a misaligned access.</p>
<p>Another very important factor is atomic memory operations.
Atomic access works on a whole word basis - any unaligned access by nature
cannot be guaranteed to be atomic as it will probably involve more than one
access.  To support atomic operations, alignment must be minmally on word
boundaries.</p>
<p>SIMD operations, tending to be 128 bits wide or higher, should be
aligned to 16 byte boundaries for optimal code generation and performance.
Unaligned loads and stores may be allowed but normally these incur
performance penalties.</p>
<p><img src="img/alignment.png" alt="Common alignments" /></p>
<p>While Intel allows unaligned access (that is, alignment on any byte boundary),
the
<a href="https://software.intel.com/sites/default/files/managed/9e/bc/64-ia-32-architectures-optimization-manual.pdf?wapkw=248966">recommended</a>
(see section 3.6.4) alignment for objects larger than 64 bits is to 16 byte
boundaries.</p>
<p>Apparently system <code>malloc()</code> implementations
<a href="http://www.erahm.org/2016/03/24/minimum-alignment-of-allocation-across-platforms/">tend to comply</a>
with the 16 byte boundary.</p>
<p>To verify the above, a rough test of both the system allocator and jemalloc
on x86_64 by using <code>Box::new()</code> on a set of types (<code>u8</code>, <code>u16</code>, <code>u32</code>, <code>u64</code>,
<code>String</code> and a larger <code>struct</code>) confirms a minimum of 8 byte alignment for
anything word size or smaller and 16 byte alignment for everything bigger.
Sample pointer printouts below are for jemalloc but Linux libc malloc produced
the same pattern:</p>
<pre><code>p=0x7fb78b421028 u8
p=0x7fb78b421030 u16
p=0x7fb78b421038 u32
p=0x7fb78b421050 u64
p=0x7fb78b420060 &quot;spam&quot;
p=0x7fb78b4220f0 Hoge { y: 2, z: &quot;ほげ&quot;, x: 1 }
</code></pre>
<p>Compare with <code>std::mem::align_of&lt;T&gt;()</code> which, on x86_64 for example,
returns alignment values:</p>
<ul>
<li><code>u8</code>: 1 byte</li>
<li><code>u16</code>: 2 bytes</li>
<li><code>u32</code>: 4 bytes</li>
<li><code>u64</code>: 8 bytes</li>
<li>any bigger struct: 8</li>
</ul>
<p>Thus despite the value of <code>std::mem::align_of::&lt;T&gt;()</code>, mature allocators will
do what is most pragmatic and follow recommended practice in support of optimal
performance.</p>
<p>With all that in mind, to keep things simple, we'll align everything to a
double-word boundaries. When we add in prepending an object header, the minimum
memory required for an object will be two words anyway.</p>
<p>Thus, the allocated size of an object will be calculated<sup class="footnote-reference"><a href="#1">1</a></sup> by</p>
<pre><code class="language-rust ignore">let alignment = size_of::&lt;usize&gt;() * 2;
// mask out the least significant bits that correspond to the alignment - 1
// then add the full alignment
let size = (size_of::&lt;T&gt;() &amp; !(alignment - 1)) + alignment;
</code></pre>
<hr />
<div class="footnote-definition" id="1"><sup class="footnote-definition-label">1</sup>
<p>For a more detailed explanation of alignment adjustment calculations, see
<a href="https://github.com/phil-opp">phil-opp</a>'s kernel
<a href="https://os.phil-opp.com/kernel-heap/#alignment">heap allocator</a>.</p>
</div>
<h1><a class="header" href="#obtaining-blocks-of-memory" id="obtaining-blocks-of-memory">Obtaining Blocks of Memory</a></h1>
<p>When requesting blocks of memory at a time, one of the questions is <em>what
is the desired block alignment?</em></p>
<ul>
<li>In deciding, one factor is that using an alignment that is a multiple of the
page size can make it easier to return memory to the operating system.</li>
<li>Another factor is that if the block is aligned to it's size, it is fast to
do bitwise arithmetic on a pointer to an object in a block to compute the
block boundary and therefore the location of any block metadata.</li>
</ul>
<p>With both these in mind we'll look at how to allocate blocks that are
aligned to the size of the block.</p>
<h2><a class="header" href="#a-basic-crate-interface" id="a-basic-crate-interface">A basic crate interface</a></h2>
<p>A block of memory is defined as a base address and a size, so we need a struct
that contains these elements.</p>
<p>To wrap the base address pointer, we'll use the recommended type for building
collections, <a href="https://doc.rust-lang.org/std/ptr/struct.NonNull.html"><code>std::ptr::NonNull&lt;T&gt;</code></a>,
which is available on stable.</p>
<pre><code class="language-rust ignore">pub struct Block {
    ptr: BlockPtr,
    size: BlockSize,
}
</code></pre>
<p>Where <code>BlockPtr</code> and <code>BlockSize</code> are defined as:</p>
<pre><code class="language-rust ignore">pub type BlockPtr = NonNull&lt;u8&gt;;
pub type BlockSize = usize;
</code></pre>
<p>To obtain a <code>Block</code>, we'll create a <code>Block::new()</code> function which, along with
<code>Block::drop()</code>, is implemented internally by wrapping the stabilized Rust alloc
routines:</p>
<pre><code class="language-rust ignore">    pub fn new(size: BlockSize) -&gt; Result&lt;Block, BlockError&gt; {
        if !size.is_power_of_two() {
            return Err(BlockError::BadRequest);
        }

        Ok(Block {
            ptr: internal::alloc_block(size)?,
            size,
        })
    }
</code></pre>
<p>Where parameter <code>size</code> must be a power of two, which is validated on the first
line of the function.  Requiring the block size to be a power of two means
simple bit arithmetic can be used to find the beginning and end of a block in
memory, if the block size is always the same.</p>
<p>Errors take one of two forms, an invalid block-size or out-of-memory, both
of which may be returned by <code>Block::new()</code>.</p>
<pre><code class="language-rust ignore">#[derive(Debug, PartialEq)]
pub enum BlockError {
    /// Usually means requested block size, and therefore alignment, wasn't a
    /// power of two
    BadRequest,
    /// Insufficient memory, couldn't allocate a block
    OOM,
}
</code></pre>
<p>Now on to the platform-specific implementations.</p>
<h2><a class="header" href="#custom-aligned-allocation-on-stable-rust" id="custom-aligned-allocation-on-stable-rust">Custom aligned allocation on stable Rust</a></h2>
<p>On the stable rustc channel we have access to some features of the
<a href="https://doc.rust-lang.org/std/alloc/index.html">Alloc</a> API.</p>
<p>This is the ideal option since it abstracts platform specifics for us, we do
not need to write different code for Unix and Windows ourselves.</p>
<p>Fortunately there is enough stable functionality to
fully implement what we need.</p>
<p>With an appropriate underlying implementation this code should compile and
execute for any target. The allocation function, implemented in the <code>internal</code>
mod, reads:</p>
<pre><code class="language-rust ignore">    pub fn alloc_block(size: BlockSize) -&gt; Result&lt;BlockPtr, BlockError&gt; {
        unsafe {
            let layout = Layout::from_size_align_unchecked(size, size);

            let ptr = alloc(layout);
            if ptr.is_null() {
                Err(BlockError::OOM)
            } else {
                Ok(NonNull::new_unchecked(ptr))
            }
        }
    }
</code></pre>
<p>Once a block has been allocated, there is no safe abstraction at this level
to access the memory. The <code>Block</code> will provide a bare pointer to the beginning
of the memory and it is up to the user to avoid invalid pointer arithmetic
and reading or writing outside of the block boundary.</p>
<pre><code class="language-rust ignore">    pub fn as_ptr(&amp;self) -&gt; *const u8 {
        self.ptr.as_ptr()
    }
</code></pre>
<h2><a class="header" href="#deallocation" id="deallocation">Deallocation</a></h2>
<p>Again, using the stable Alloc functions:</p>
<pre><code class="language-rust ignore">    pub fn dealloc_block(ptr: BlockPtr, size: BlockSize) {
        unsafe {
            let layout = Layout::from_size_align_unchecked(size, size);

            dealloc(ptr.as_ptr(), layout);
        }
    }
</code></pre>
<p>The implementation of <code>Block::drop()</code> calls the deallocation function
for us so we can create and drop <code>Block</code> instances without leaking memory.</p>
<h2><a class="header" href="#testing" id="testing">Testing</a></h2>
<p>We want to be sure that the system level allocation APIs do indeed return
block-size-aligned blocks. Checking for this is straightforward.</p>
<p>A correctly aligned block should have it's low bits
set to <code>0</code> for a number of bits that represents the range of the block
size - that is, the block size minus one. A bitwise XOR will highlight any
bits that shouldn't be set:</p>
<pre><code class="language-rust ignore">        // the block address bitwise AND the alignment bits (size - 1) should
        // be a mutually exclusive set of bits
        let mask = size - 1;
        assert!((block.ptr.as_ptr() as usize &amp; mask) ^ mask == mask);
</code></pre>
<h1><a class="header" href="#the-type-of-allocation" id="the-type-of-allocation">The type of allocation</a></h1>
<p>Before we start writing objects into <code>Block</code>s, we need to know the nature of
the interface in Rust terms.</p>
<p>If we consider the global allocator in Rust, implicitly available via
<code>Box::new()</code>, <code>Vec::new()</code> and so on, we'll notice that since the global
allocator is available on every thread and allows the creation of new
objects on the heap (that is, mutation of the heap) from any code location
without needing to follow the rules of borrowing and mutable aliasing,
it is essentially a container that implements <code>Sync</code> and the interior
mutability pattern.</p>
<p>We need to follow suit, but we'll leave <code>Sync</code> for advanced chapters.</p>
<p>An interface that satisfies the interior mutability property, by borrowing
the allocator instance immutably, might look like:</p>
<pre><code class="language-rust ignore">trait AllocRaw {
    fn alloc&lt;T&gt;(&amp;self, object: T) -&gt; *const T;
}
</code></pre>
<p>naming it <code>AllocRaw</code> because when layering on top of <code>Block</code> we'll
work with raw pointers and not concern ourselves with the lifetime of
allocated objects.</p>
<p>It will become a little more complex than this but for now, this captures
the essence of the interface.</p>
<h1><a class="header" href="#an-allocator-sticky-immix" id="an-allocator-sticky-immix">An allocator: Sticky Immix</a></h1>
<p>Quickly, some terminology:</p>
<ul>
<li>Mutator: the thread of execution that writes and modifies objects on the heap.</li>
<li>Live objects: the graph of objects that the mutator can reach, either directly
from it's stack or indirectly through other reachable objects.</li>
<li>Dead objects: any object that is disconnected from the mutator's graph of live
objects.</li>
<li>Collector: the thread of execution that identifies objects that are no longer
reachable by the mutator and marks them as free space that can be reused</li>
<li>Fragmentation: as objects have many different sizes, after allocating and
freeing many objects, gaps of unused memory appear between objects that are
too small for most objects but that add up to a measurable percentage of
wasted space.</li>
<li>Evacuation: when the collector <em>moves</em> live objects to another block of memory
so that the originating block can be <em>de_fragmented</em></li>
</ul>
<h2><a class="header" href="#about-immix" id="about-immix">About Immix</a></h2>
<p>Immix is a memory management scheme that considers blocks of fixed size at a time.
Each block is divided into lines. In the original paper, blocks are sized at 32k
and lines at 128 bytes.  Objects are allocated into blocks using bump allocation
and objects can cross line boundaries.</p>
<p><img src="img/stickyimmix_block.png" alt="StickyImmix Block" /></p>
<p>During tracing to discover live objects, objects are marked as live, but the
line, or lines, that each object occupies are also marked as live. This can mean, of
course, that a line may contain a dead object and a live object but the whole
line is marked as live.</p>
<p>To mark lines as live, a portion of the block is set aside for line mark bits,
usually one byte per mark bit. If <em>any</em> line is marked as live, the whole block
is also marked as live. There must also, therefore, be a bit that indicates
block liveness.</p>
<h3><a class="header" href="#conservative-marking" id="conservative-marking">Conservative marking</a></h3>
<p>The Immix authors found that marking <em>every</em> line that contains a live object
could be expensive. For example, many small objects might cross line boundaries,
requiring two lines to be marked as live. This would require looking up the
object size and calculating whether the object crosses the boundary into the
next line. To save CPU cycles, they simplified the algorithm by saying that
any object that fits in a line <em>might</em> cross into the next line so we will
conservatively <em>consider</em> the next line marked just in case. This sped up
marking at little fragmentation expense.</p>
<h3><a class="header" href="#collection" id="collection">Collection</a></h3>
<p>During collection, only lines not marked as live are considered available for
re-use. Inevitably then, there is acceptance of some amount of fragmentation
at this point.</p>
<p><em>Full</em> Immix implements evacuating objects out of the most fragmented blocks
into fresh, empty blocks, for defragmentation.</p>
<p>For simplicity of implementation, we'll leave out this evacuation operation
in this guide. This is called <em>Sticky</em> Immix.</p>
<p>We'll also stick to a single thread for the mutator and collector to avoid the
complexity overhead of a multi-threaded implementation for now.</p>
<p>Recommended reading: <a href="http://users.cecs.anu.edu.au/%7Esteveb/pubs/papers/immix-pldi-2008.pdf">Stephen M. Blackburn &amp; Kathryn S. McKinley - Immix: A Mark-Region Garbage Collector with Space Efficiency, Fast Collection, and Mutator Performance</a></p>
<h2><a class="header" href="#about-this-part-of-the-book" id="about-this-part-of-the-book">About this part of the book</a></h2>
<p>This section will describe a Rust crate that implements a Sticky Immix heap.
As part of this implementation we will dive into the crate API details to
understand how we can define an interface between the heap and the language
VM that will come later.</p>
<p><em>What this is not: custom memory management to replace the global Rust
allocator! The APIs we arrive at will be substantially incompatible with the
global Rust allocator.</em></p>
<h1><a class="header" href="#bump-allocation" id="bump-allocation">Bump allocation</a></h1>
<p>Now that we can get blocks of raw memory, we need to write objects into it. The
simplest way to do this is to write objects into a block one after the other
in consecutive order. This is bump allocation - we have a pointer, the bump
pointer, which points at the space in the block after the last object that
was written. When the next object is written, the bump pointer is incremented
to point to the space after <em>that</em> object.</p>
<p>In a twist of mathematical convenience, though, it is <a href="https://fitzgeraldnick.com/2019/11/01/always-bump-downwards.html">more efficient</a> to
bump allocate from a high memory location <em>downwards</em>. We will do that.</p>
<p>We will used a fixed power-of-two block size. The benefit of this is that
given a pointer to an object, by zeroing the bits of the pointer that represent
the block size, the result points to the beginning of the block. This will
be useful later when implementing garbage collection.</p>
<p>Our block size will be 32k, a reasonably optimal size arrived at in the
original <a href="http://www.cs.utexas.edu/users/speedway/DaCapo/papers/immix-pldi-2008.pdf">Immix paper</a>. This size can be any power of two though and
different use cases may show different optimal sizes.</p>
<pre><code class="language-rust ignore">pub const BLOCK_SIZE_BITS: usize = 15;
pub const BLOCK_SIZE: usize = 1 &lt;&lt; BLOCK_SIZE_BITS;
</code></pre>
<p>Now we'll define a struct that wraps the block with a bump pointer and garbage
collection metadata:</p>
<pre><code class="language-rust ignore">pub struct BumpBlock {
    cursor: *const u8,
    limit: *const u8,
    block: Block,
    meta: BlockMeta,
}
</code></pre>
<h2><a class="header" href="#bump-allocation-basics" id="bump-allocation-basics">Bump allocation basics</a></h2>
<p>In this struct definition, there are two members that we are interested in
to begin with. The other two, <code>limit</code> and <code>meta</code>, will be discussed in the
next section.</p>
<ul>
<li><code>cursor</code>: this is the bump pointer. In our implementation it is the index
into the block where the last object was written.</li>
<li><code>block</code>: this is the <code>Block</code> itself in which objects will be written.</li>
</ul>
<p>Below is a start to a bump allocation function:</p>
<pre><code class="language-rust ignore">impl BumpBlock {
    pub fn inner_alloc(&amp;mut self, alloc_size: usize) -&gt; Option&lt;*const u8&gt; {
        let block_start_ptr = self.block.as_ptr() as usize;
        let cursor_ptr = self.cursor as usize;

        // align to word boundary
        let align_mask: usize = !(size_of::&lt;usize&gt;() - 1);

        let next_ptr = cursor_ptr.checked_sub(alloc_size)? &amp; align_mask;

        if next_ptr &lt; block_start_ptr {
            // allocation would start lower than block beginning, which means
            // there isn't space in the block for this allocation
            None
        } else {
            self.cursor = next_ptr as *const u8;
            Some(next_ptr as *const u8)
        }
    }
}
</code></pre>
<p>In our function, the <code>alloc_size</code> parameter should be a number of bytes of
memory requested.</p>
<p>The value of <code>alloc_size</code> may produce an unaligned pointer at which to write the
object. Fortunately, by bump allocating downward we can apply a simple mask to the
pointer to align it down to the nearest word:</p>
<pre><code class="language-rust ignore">        let align_mask: usize = !(size_of::&lt;usize&gt;() - 1);
</code></pre>
<p>In initial implementation, allocation will simply return <code>None</code> if the block
does not have enough capacity for the requested <code>alloc_size</code>. If there <em>is</em>
space, it will be returned as a <code>Some(*const u8)</code> pointer.</p>
<p>Note that this function does not <em>write</em> the object to memory, it merely
returns a pointer to an available space.  Writing the object will require
invoking the <code>std::ptr::write</code> function. We will do that in a separate module
but for completeness of this chapter, this might look something like:</p>
<pre><code class="language-rust ignore">use std::ptr::write;

unsafe fn write&lt;T&gt;(dest: *const u8, object: T) {
    write(dest as *mut T, object);
}
</code></pre>
<h2><a class="header" href="#some-time-passes" id="some-time-passes">Some time passes...</a></h2>
<p>After allocating and freeing objects, we will have gaps between objects in a
block that can be reused. The above bump allocation algorithm is unaware of
these gaps so we'll have to modify it before it can allocate into fragmented
blocks.</p>
<p>To recap, in Immix, a block is divided into lines and only whole lines are
considered for reuse. When objects are marked as live, so are the lines that an
object occupies. Therefore, only lines that are <em>not</em> marked as live are usable
for allocation into. Even if a line is only partially allocated into, it is not
a candidate for further allocation.</p>
<p>In our implementation we will use the high bytes of the <code>Block</code> to represent
these line mark bits, where each line is represented by a single byte.</p>
<p>We'll need a data structure to represent this. we'll call it <code>BlockMeta</code>,
but first some constants that we need in order to know</p>
<ul>
<li>how big a line is</li>
<li>how many lines are in a block</li>
<li>how many bytes remain in the <code>Block</code> for allocating into</li>
</ul>
<pre><code class="language-rust ignore">pub const LINE_SIZE_BITS: usize = 7;
pub const LINE_SIZE: usize = 1 &lt;&lt; LINE_SIZE_BITS;

// How many total lines are in a block
pub const LINE_COUNT: usize = BLOCK_SIZE / LINE_SIZE;

// We need LINE_COUNT number of bytes for marking lines, so the capacity of a block
// is reduced by that number of bytes.
pub const BLOCK_CAPACITY: usize = BLOCK_SIZE - LINE_COUNT;
</code></pre>
<p>For clarity, let's put some numbers to the definitions we've made so far:</p>
<ul>
<li>A block size is 32Kbytes</li>
<li>A line is 128 bytes long</li>
<li>The number of lines within a 32Kbyte <code>Block</code> is 256</li>
</ul>
<p>Therefore the top 256 bytes of a <code>Block</code> are used for line mark bits. Since
these line mark bits do not need to be marked themselves, the last <em>two bytes</em>
of the <code>Block</code> are not needed to mark lines.</p>
<p>This leaves one last thing to mark: the entire <code>Block</code>. If <em>any</em> line in the
<code>Block</code> is marked, then the <code>Block</code> is considered to be live and must be marked
as such.</p>
<p>We use the final byte of the <code>Block</code> to store the <code>Block</code> mark bit.</p>
<p>The definition of <code>BumpBlock</code> contains member <code>meta</code> which is of type
<code>BlockMeta</code>. We can now introduce the definition of <code>BlockMeta</code> which we simply
need to represent a pointer to the line mark section at the end of the <code>Block</code>:</p>
<pre><code class="language-rust ignore">pub struct BlockMeta {
    lines: *mut u8,
}
</code></pre>
<p>This pointer could be easily calculated, of course, so this is just a handy
shortcut.</p>
<h3><a class="header" href="#allocating-into-a-fragmented-block" id="allocating-into-a-fragmented-block">Allocating into a fragmented Block</a></h3>
<p><img src="img/fragmented_block.png" alt="StickyImmix Fragmented Block" /></p>
<p>The struct <code>BlockMeta</code> contains one function we will study:</p>
<pre><code class="language-rust ignore">    /// When it comes to finding allocatable holes, we bump-allocate downward.
    pub fn find_next_available_hole(
        &amp;self,
        starting_at: usize,
        alloc_size: usize,
    ) -&gt; Option&lt;(usize, usize)&gt; {
        // The count of consecutive avaliable holes. Must take into account a conservatively marked
        // hole at the beginning of the sequence.
        let mut count = 0;
        let starting_line = starting_at / constants::LINE_SIZE;
        let lines_required = (alloc_size + constants::LINE_SIZE - 1) / constants::LINE_SIZE;
        // Counting down from the given search start index
        let mut end = starting_line;

        for index in (0..starting_line).rev() {
            let marked = unsafe { *self.lines.add(index) };

            if marked == 0 {
                // count unmarked lines
                count += 1;

                if index == 0 &amp;&amp; count &gt;= lines_required {
                    let limit = index * constants::LINE_SIZE;
                    let cursor = end * constants::LINE_SIZE;
                    return Some((cursor, limit));
                }
            } else {
                // This block is marked
                if count &gt; lines_required {
                    // But at least 2 previous blocks were not marked. Return the hole, considering the
                    // immediately preceding block as conservatively marked
                    let limit = (index + 2) * constants::LINE_SIZE;
                    let cursor = end * constants::LINE_SIZE;
                    return Some((cursor, limit));
                }

                // If this line is marked and we didn't return a new cursor/limit pair by now,
                // reset the hole search state
                count = 0;
                end = index;
            }
        }

        None
    }
</code></pre>
<p>The purpose of this function is to locate a gap of unmarked lines of sufficient
size to allocate an object of size <code>alloc_size</code> into.</p>
<p>The input to this function, <code>starting_at</code>, is the offset into the block to start
looking for a hole.</p>
<p>If no suitable hole is found, the return value is <code>None</code>.</p>
<p>If there are unmarked lines lower in memory than the <code>starting_at</code> point (bump
allocating downwards), the return value will be a pair of numbers: <code>(cursor, limit)</code> where:</p>
<ul>
<li><code>cursor</code> will be the new bump pointer value</li>
<li><code>limit</code> will be the lower bound of the available hole.</li>
</ul>
<h4><a class="header" href="#a-deeper-dive" id="a-deeper-dive">A deeper dive</a></h4>
<p>Our first variable is a counter of consecutive available lines. This count will
always assume that the first line in the sequence is conservatively marked and
won't count toward the total, unless it is line 0.</p>
<pre><code class="language-rust ignore">        let mut count = 0;
</code></pre>
<p>Next, the <code>starting_at</code> and <code>alloc_size</code> arguments have units of bytes but we
want to use line count math, so conversion must be done.</p>
<pre><code class="language-rust ignore">         let starting_line = starting_at / constants::LINE_SIZE;
         let lines_required = (alloc_size + constants::LINE_SIZE - 1) / constants::LINE_SIZE;
</code></pre>
<p>Our final variable will be the end line that, together with <code>starting_line</code>,
will mark the boundary of the hole we hope to find.</p>
<pre><code class="language-rust ignore">        let mut end = starting_line;
</code></pre>
<p>Now for the loop that identifies holes and ends the function if either:</p>
<ul>
<li>a large enough hole is found</li>
<li>no suitable hole is found</li>
</ul>
<p>We iterate over lines in decreasing order from <code>starting_line</code> down to line zero
and fetch the mark bit into variable <code>marked</code>.</p>
<pre><code class="language-rust ignore">        for index in (0..starting_line).rev() {
            let marked = unsafe { *self.lines.add(index) };
</code></pre>
<p>If the line is unmarked, we increment our consecutive-unmarked-lines counter.</p>
<p>Then we reach the first termination condition: we reached line zero and we have
a large enough hole for our object. The hole extents can be returned, converting
back to byte offsets.</p>
<pre><code class="language-rust ignore">            if marked == 0 {
                count += 1;

                if index == 0 &amp;&amp; count &gt;= lines_required {
                    let limit = index * constants::LINE_SIZE;
                    let cursor = end * constants::LINE_SIZE;
                    return Some((cursor, limit));
                }
            } else {
</code></pre>
<p>Otherwise if the line is marked, we've reached the end of the current hole (if
we were even over one.)</p>
<p>Here, we have the second possible termination condition: we have a large enough
hole for our object. The hole extents can be returned, taking the last line as
conservatively marked.</p>
<p>This is seen in adding 2 to <code>index</code>:</p>
<ul>
<li>1 for walking back from the current marked line</li>
<li>plus 1 for walking back from the previous conservatively marked line</li>
</ul>
<p>If this condition isn't met, our search is reset - <code>count</code> is back to zero and
we keep iterating.</p>
<pre><code class="language-rust ignore">            } else {
                if count &gt; lines_required {
                    let limit = (index + 2) * constants::LINE_SIZE;
                    let cursor = end * constants::LINE_SIZE;
                    return Some((cursor, limit));
                }

                count = 0;
                end = index;
            }
</code></pre>
<p>Finally, if iterating over lines reached line zero without finding a hole, we
return <code>None</code> to indicate failure.</p>
<pre><code class="language-rust ignore">        }

        None
    }
</code></pre>
<h4><a class="header" href="#making-use-of-the-hole-finder" id="making-use-of-the-hole-finder">Making use of the hole finder</a></h4>
<p>We'll return to the <code>BumpBlock::inner_alloc()</code> function now to make use of
<code>BlockMeta</code> and its hole finding operation.</p>
<p>The <code>BumpBlock</code> struct contains two more members: <code>limit</code> and <code>meta</code>. These
should now be obvious - <code>limit</code> is the known byte offset limit into which
we can allocate, and <code>meta</code> is the <code>BlockMeta</code> instance associated with the
block.</p>
<p>We need to update <code>inner_alloc()</code> with a new condition:</p>
<ul>
<li>the size being requested must fit between <code>self.cursor</code> and <code>self.limit</code></li>
</ul>
<p>(Note that for a fresh, new block, <code>self.limit</code> is set to the block size.)</p>
<p>If the above condition is not met, we will call
<code>BlockMeta::find_next_available_hole()</code> to get a new <code>cursor</code> and <code>limit</code>
to try, and repeat that until we've either <em>found</em> a big enough hole or
reached the end of the block, exhausting our options.</p>
<p>The new definition of <code>BumpBlock::inner_alloc()</code> reads as follows:</p>
<pre><code class="language-rust ignore">    pub fn inner_alloc(&amp;mut self, alloc_size: usize) -&gt; Option&lt;*const u8&gt; {
        let ptr = self.cursor as usize;
        let limit = self.limit as usize;

        let next_ptr = ptr.checked_sub(alloc_size)? &amp; constants::ALLOC_ALIGN_MASK;

        if next_ptr &lt; limit {
            let block_relative_limit =
                unsafe { self.limit.sub(self.block.as_ptr() as usize) } as usize;

            if block_relative_limit &gt; 0 {
                if let Some((cursor, limit)) = self
                    .meta
                    .find_next_available_hole(block_relative_limit, alloc_size)
                {
                    self.cursor = unsafe { self.block.as_ptr().add(cursor) };
                    self.limit = unsafe { self.block.as_ptr().add(limit) };
                    return self.inner_alloc(alloc_size);
                }
            }

            None
        } else {
            self.cursor = next_ptr as *const u8;
            Some(self.cursor)
        }
    }
</code></pre>
<p>and as you can see, this implementation is recursive.</p>
<h2><a class="header" href="#wrapping-this-up" id="wrapping-this-up">Wrapping this up</a></h2>
<p>At the beginning of this chapter we stated that given a pointer to an object, by
zeroing the bits of the pointer that represent the block size, the result points
to the beginning of the block.</p>
<p>We'll make use of that now.</p>
<p>During the mark phase of garbage collection, we will need to know which line or
lines to mark, in addition to marking the object itself. We will make a copy of
the <code>BlockMeta</code> instance pointer in the 0th word of the memory block so that
given any object pointer, we can obtain the <code>BlockMeta</code> instance.</p>
<p>In the next chapter we'll handle multiple <code>BumpBlock</code>s so that we can keep
allocating objects after one block is full.</p>
<h1><a class="header" href="#allocating-into-multiple-blocks" id="allocating-into-multiple-blocks">Allocating into Multiple Blocks</a></h1>
<p>Let's now zoom out of the fractal code soup one level and begin arranging multiple
blocks so we can allocate - in theory - indefinitely.</p>
<h2><a class="header" href="#lists-of-blocks" id="lists-of-blocks">Lists of blocks</a></h2>
<p>We'll need a new struct for wrapping multiple blocks:</p>
<pre><code class="language-rust ignore">struct BlockList {
    head: Option&lt;BumpBlock&gt;,
    overflow: Option&lt;BumpBlock&gt;,
    rest: Vec&lt;BumpBlock&gt;,
}
</code></pre>
<p>Immix maintains several lists of blocks. We won't include them all in the first
iteration but in short they are:</p>
<ul>
<li><code>free</code>: a list of blocks that contain no objects. These blocks are held at the
ready to allocate into on demand</li>
<li><code>recycle</code>: a list of blocks that contain some objects but also at least one
line that can be allocated into</li>
<li><code>large</code>: not a list of blocks, necessarily, but a list of objects larger than
the block size, or some other method of accounting for large objects</li>
<li><code>rest</code>: the rest of the blocks that have been allocated into but are not
suitable for recycling</li>
</ul>
<p>In our first iteration we'll only keep the <code>rest</code> list of blocks and two blocks
to immediately allocate into. Why two? To understand why, we need to understand
how Immix thinks about object sizes.</p>
<h3><a class="header" href="#immix-and-object-sizes" id="immix-and-object-sizes">Immix and object sizes</a></h3>
<p>We've seen that there are two numbers that define granularity in Immix: the
block size and the line size.  These numbers give us the ability to categorize
object sizes:</p>
<ul>
<li>small: those that (with object header and alignment overhead) fit inside a
line</li>
<li>medium: those that (again with object header and alignment overhead) are
larger than one line but smaller than a block</li>
<li>large: those that are larger than a block</li>
</ul>
<p>In the previous chapter we described the basic allocation algorithm: when
an object is being allocated, the current block is scanned for a hole between
marked lines large enough to allocate into. This does seem like it could
be inefficient. We could spend a lot of CPU cycles looking for a big enough
hole, especially for a medium sized object.</p>
<p>To avoid this, Immix maintains a second block, an overflow block, to allocate
medium objects into that don't fit the first available hole in the
main block being allocated into.</p>
<p>Thus two blocks to immediately allocate into:</p>
<ul>
<li><code>head</code>: the current block being allocated into</li>
<li><code>overflow</code>: a block kept handy for writing medium objects into that don't
fit the <code>head</code> block's current hole</li>
</ul>
<p>We'll be ignoring large objects for now and attending only to allocating small
and medium objects into blocks.</p>
<p>Instead of recycling blocks with holes, or maintaining a list of pre-allocated
free blocks, we'll allocate a new block on demand whenever we need more space.
We'll get to identifying holes and recyclable blocks in a later chapter.</p>
<h3><a class="header" href="#managing-the-overflow-block" id="managing-the-overflow-block">Managing the overflow block</a></h3>
<p>Generally in our code for this book, we will try to default to not allocating
memory unless it is needed. For example, when an array is instantiated,
the backing storage will remain unallocated until a value is pushed on to
it.</p>
<p>Thus in the definition of <code>BlockList</code>, <code>head</code> and <code>overflow</code> are <code>Option</code>
types and won't be instantiated except on demand.</p>
<p>For allocating into the overflow block we'll define a function in the
<code>BlockList</code> impl:</p>
<pre><code class="language-rust ignore">impl BlockList {
    fn overflow_alloc(&amp;mut self, alloc_size: usize) -&gt; Result&lt;*const u8, AllocError&gt; {
        ...
    }
}
</code></pre>
<p>The input constraint is that, since overflow is for medium objects, <code>alloc_size</code>
must be less than the block size.</p>
<p>The logic inside will divide into three branches:</p>
<ol>
<li>We haven't got an overflow block yet - <code>self.overflow</code> is <code>None</code>. In this
case we have to instantiate a new block (since we're not maintaining
a list of preinstantiated free blocks yet) and then, since that block
is empty and we have a medium sized object, we can expect the allocation
to succeed.
<pre><code class="language-rust ignore">    match self.overflow {
        Some ...,
        None =&gt; {
             let mut overflow = BumpBlock::new()?;

             // object size &lt; block size means we can't fail this expect
             let space = overflow
                 .inner_alloc(alloc_size)
                 .expect(&quot;We expected this object to fit!&quot;);

             self.overflow = Some(overflow);

             space
         }
    }
</code></pre>
</li>
<li>We <em>have</em> an overflow block and the object fits. Easy.
<pre><code class="language-rust ignore">     match self.overflow {
         // We already have an overflow block to try to use...
         Some(ref mut overflow) =&gt; {
             // This is a medium object that might fit in the current block...
             match overflow.inner_alloc(alloc_size) {
                 // the block has a suitable hole
                 Some(space) =&gt; space,
                 ...
             }
         },
         None =&gt; ...
     }
</code></pre>
</li>
<li>We have an overflow block but the object does not fit. Now we simply
instantiate a <em>new</em> overflow block, adding the old one to the <code>rest</code>
list (in future it will make a good candidate for recycing!). Again,
since we're writing a medium object into a block, we can expect allocation
to succeed.
<pre><code class="language-rust ignore">     match self.overflow {
         // We already have an overflow block to try to use...
         Some(ref mut overflow) =&gt; {
             // This is a medium object that might fit in the current block...
             match overflow.inner_alloc(alloc_size) {
                 Some ...,
                 // the block does not have a suitable hole
                 None =&gt; {
                     let previous = replace(overflow, BumpBlock::new()?);

                     self.rest.push(previous);

                     overflow.inner_alloc(alloc_size).expect(&quot;Unexpected error!&quot;)
                 }
             }
         },
         None =&gt; ...
     }
</code></pre>
</li>
</ol>
<p>In this logic, the only error can come from failing to create a new block.</p>
<p>On success, at this level of interface we continue to return a <code>*const u8</code>
pointer to the available space as we're not yet handling the type of the
object being allocated.</p>
<p>You may have noticed that the function signature for <code>overflow_alloc</code> takes a
<code>&amp;mut self</code>.  This isn't compatible with the interior mutability model
of allocation.  We'll have to wrap the <code>BlockList</code> struct in another struct
that handles this change of API model.</p>
<h2><a class="header" href="#the-heap-struct" id="the-heap-struct">The heap struct</a></h2>
<p>This outer struct will provide the external crate interface and some further
implementation of block management.</p>
<p>The crate interface will require us to consider object headers and so in the
struct definition below there is reference to a generic type <code>H</code> that
the <em>user</em> of the heap will define as the object header.</p>
<pre><code class="language-rust ignore">pub struct StickyImmixHeap&lt;H&gt; {
    blocks: UnsafeCell&lt;BlockList&gt;,

    _header_type: PhantomData&lt;*const H&gt;,
}
</code></pre>
<p>Since object headers are not owned directly by the heap struct, we need a
<code>PhantomData</code> instance to associate with <code>H</code>.  We'll discuss object headers
in a later chapter.</p>
<p>Now let's focus on the use of the <code>BlockList</code>.</p>
<p>The instance of <code>BlockList</code> in the <code>StickyImmixHeap</code> struct is wrapped in an
<code>UnsafeCell</code> because we need interior mutability. We need to be able to
borrow the <code>BlockList</code> mutably while presenting an immutable interface to
the outside world.  Since we won't be borrowing the <code>BlockList</code> in multiple
places in the same call tree, we don't need <code>RefCell</code> and we can avoid it's
runtime borrow checking.</p>
<h3><a class="header" href="#allocating-into-the-head-block" id="allocating-into-the-head-block">Allocating into the head block</a></h3>
<p>We've already taken care of the overflow block, now we'll handle allocation
into the <code>head</code> block. We'll define a new function:</p>
<pre><code class="language-rust ignore">impl StickyImmixHeap {
    fn find_space(
        &amp;self,
        alloc_size: usize,
        size_class: SizeClass,
    ) -&gt; Result&lt;*const u8, AllocError&gt; {
        let blocks = unsafe { &amp;mut *self.blocks.get() };
        ...
    }
}
</code></pre>
<p>This function is going to look almost identical to the <code>alloc_overflow()</code>
function defined earlier. It has more or less the same cases to walk through:</p>
<ol>
<li><code>head</code> block is <code>None</code>, i.e. we haven't allocated a head block yet. Allocate
one and write the object into it.</li>
<li>We have <code>Some(ref mut head)</code> in <code>head</code>.  At this point we divert from the
<code>alloc_overflow()</code> function and query the size of the object - if this is
is a medium object and the current hole between marked lines in the <code>head</code>
block is too small, call into <code>alloc_overflow()</code> and return.
<pre><code class="language-rust ignore">             if size_class == SizeClass::Medium &amp;&amp; alloc_size &gt; head.current_hole_size() {
                 return blocks.overflow_alloc(alloc_size);
             }
</code></pre>
Otherwise, continue to allocate into <code>head</code> and return.</li>
<li>We have <code>Some(ref mut head)</code> in <code>head</code> but this block is unable to
accommodate the object, whether medium or small. We must append the current
head to the <code>rest</code> list and create a new <code>BumpBlock</code> to allocate into.</li>
</ol>
<p>There is one more thing to mention. What about large objects? We'll cover those
in a later chapter. Right now we'll make it an error to try to allocate a large
object by putting this at the beginning of the <code>StickyImmixHeap::inner_alloc()</code>
function:</p>
<pre><code class="language-rust ignore">        if size_class == SizeClass::Large {
            return Err(AllocError::BadRequest);
        }

</code></pre>
<h2><a class="header" href="#where-to-next" id="where-to-next">Where to next?</a></h2>
<p>We have a scheme for finding space in blocks for small and medium objects
and so, in the next chapter we will define the external interface to the crate.</p>
<h1><a class="header" href="#defining-the-allocation-api" id="defining-the-allocation-api">Defining the allocation API</a></h1>
<p>Let's look back at the allocator prototype API we defined in the introductory
chapter.</p>
<pre><code class="language-rust ignore">trait AllocRaw {
    fn alloc&lt;T&gt;(&amp;self, object: T) -&gt; *const T;
}
</code></pre>
<p>This will quickly prove to be inadequate and non-idiomatic. For starters, there
is no way to report that allocation failed except for perhaps returning a null
pointer. That is certainly a workable solution but is not going to feel
idiomatic or ergonomic for how we want to use the API. Let's make a couple
changes:</p>
<pre><code class="language-rust ignore">trait AllocRaw {
    fn alloc&lt;T&gt;(&amp;self, object: T) -&gt; Result&lt;RawPtr&lt;T&gt;, AllocError&gt;;
}
</code></pre>
<p>Now we're returning a <code>Result</code>, the failure side of which is an error type
where we can distinguish between different allocation failure modes. This is
often not that useful but working with <code>Result</code> is far more idiomatic Rust
than checking a pointer for being null. We'll allow for distinguishing between
Out Of Memory and an allocation request that for whatever reason is invalid.</p>
<pre><code class="language-rust ignore">#[derive(Copy, Clone, Debug, PartialEq)]
pub enum AllocError {
    /// Some attribute of the allocation, most likely the size requested,
    /// could not be fulfilled
    BadRequest,
    /// Out of memory - allocating the space failed
    OOM,
}
</code></pre>
<p>The second change is that instead of a <code>*const T</code> value in the success
discriminant we'll wrap a pointer in a new struct: <code>RawPtr&lt;T&gt;</code>. This wrapper
will amount to little more than containing a <code>std::ptr::NonNull</code> instance
and some functions to access the pointer.</p>
<pre><code class="language-rust ignore">pub struct RawPtr&lt;T: Sized&gt; {
    ptr: NonNull&lt;T&gt;,
}
</code></pre>
<p>This'll be better to work with on the user-of-the-crate side.</p>
<p>It'll also make it easier to modify internals or even swap out entire
implementations. This is a motivating factor for the design of this interface
as we'll see as we continue to amend it to account for object headers now.</p>
<h2><a class="header" href="#object-headers" id="object-headers">Object headers</a></h2>
<p>The purpose of an object header is to provide the allocator, the language
runtime and the garbage collector with information about the object that
is needed at runtime. Typical data points that are stored might include:</p>
<ul>
<li>object size</li>
<li>some kind of type identifier</li>
<li>garbage collection information such as a mark flag</li>
</ul>
<p>We want to create a flexible interface to a language while also ensuring that
the interpreter will provide the information that the allocator and garbage
collector in <em>this</em> crate need.</p>
<p>We'll define a trait for the user to implement.</p>
<pre><code class="language-rust ignore">pub trait AllocHeader: Sized {
    /// Associated type that identifies the allocated object type
    type TypeId: AllocTypeId;

    /// Create a new header for object type O
    fn new&lt;O: AllocObject&lt;Self::TypeId&gt;&gt;(size: u32, size_class: SizeClass, mark: Mark) -&gt; Self;

    /// Create a new header for an array type
    fn new_array(size: ArraySize, size_class: SizeClass, mark: Mark) -&gt; Self;

    /// Set the Mark value to &quot;marked&quot;
    fn mark(&amp;mut self);

    /// Get the current Mark value
    fn is_marked(&amp;self) -&gt; bool;

    /// Get the size class of the object
    fn size_class(&amp;self) -&gt; SizeClass;

    /// Get the size of the object in bytes
    fn size(&amp;self) -&gt; u32;

    /// Get the type of the object
    fn type_id(&amp;self) -&gt; Self::TypeId;
}
</code></pre>
<p>Now we have a bunch more questions to answer! Some of these trait methods are
straightforward - <code>fn size(&amp;self) -&gt; u32</code> returns the object size; <code>mark()</code>
and <code>is_marked()</code> must be GC related. Some are less obvious, such as
<code>new_array()</code> which we'll cover at the end of this chapter.</p>
<p>But this struct references some more types that must be defined and explained.</p>
<h3><a class="header" href="#type-identification" id="type-identification">Type identification</a></h3>
<p>What follows is a set of design trade-offs made for the purposes of this book;
there are many ways this could be implemented.</p>
<p>The types described next are all about sharing compile-time and runtime object
type information between the allocator, the GC and the interpreter.</p>
<p>We ideally want to make it difficult for the user to make mistakes with this
and leak undefined behavior. We would also prefer this to be a safe-Rust
interface, while at the same time being flexible enough for the user to make
interpreter-appropriate decisions about the header design.</p>
<p>First up, an object header implementation must define an associated type</p>
<pre><code class="language-rust ignore">pub trait AllocHeader: Sized {
    type TypeId: AllocTypeId;
}
</code></pre>
<p>where <code>AllocTypeId</code> is define simply as:</p>
<pre><code class="language-rust ignore">pub trait AllocTypeId: Copy + Clone {}
</code></pre>
<p>This means the interpreter is free to implement a type identifier type however
it pleases, the only constraint is that it implements this trait.</p>
<p>Next, the definition of the header constructor,</p>
<pre><code class="language-rust ignore">pub trait AllocHeader: Sized {
    ...

    fn new&lt;O: AllocObject&lt;Self::TypeId&gt;&gt;(
        size: u32,
        size_class: SizeClass,
        mark: Mark
    ) -&gt; Self;

    ...
}
</code></pre>
<p>refers to a type <code>O</code> that must implement <code>AllocObject</code> which in turn must refer
to the common <code>AllocTypeId</code>. The generic type <code>O</code> is the object for which the
header is being instantiated for.</p>
<p>And what is <code>AllocObject</code>? Simply:</p>
<pre><code class="language-rust ignore">pub trait AllocObject&lt;T: AllocTypeId&gt; {
    const TYPE_ID: T;
}
</code></pre>
<p>In summary, we have:</p>
<ul>
<li><code>AllocHeader</code>: a trait that the header type must implement</li>
<li><code>AllocTypeId</code>: a trait that a type identifier must implement</li>
<li><code>AllocObject</code>: a trait that objects that can be allocated must implement</li>
</ul>
<h3><a class="header" href="#an-example" id="an-example">An example</a></h3>
<p>Let's implement a couple of traits to make it more concrete.</p>
<p>The simplest form of type identifier is an enum. Each discriminant describes
a type that the interpreter will use at runtime.</p>
<pre><code class="language-rust ignore">#[derive(PartialEq, Copy, Clone)]
enum MyTypeId {
    Number,
    String,
    Array,
}

impl AllocTypeId for MyTypeId {}
</code></pre>
<p>A hypothetical numeric type for our interpreter with the type identifier as
associated constant:</p>
<pre><code class="language-rust ignore">struct BigNumber {
    value: i64
}

impl AllocObject&lt;MyTypeId&gt; for BigNumber {
    const TYPE_ID: MyTypeId = MyTypeId::Number;
}
</code></pre>
<p>And finally, here is a possible object header struct and the implementation of
<code>AllocHeader::new()</code>:</p>
<pre><code class="language-rust ignore">struct MyHeader {
    size: u32,
    size_class: SizeClass,
    mark: Mark,
    type_id: MyTypeId,
}

impl AllocHeader for MyHeader {
    type TypeId = MyTypeId;

    fn new&lt;O: AllocObject&lt;Self::TypeId&gt;&gt;(
        size: u32,
        size_class: SizeClass,
        mark: Mark
    ) -&gt; Self {
        MyHeader {
            size,
            size_class,
            mark,
            type_id: O::TYPE_ID,
        }
    }

    ...
}
</code></pre>
<p>These would all be defined and implemented in the interpreter and are not
provided by the Sticky Immix crate, while all the functions in the trait
<code>AllocHeader</code> are intended to be called internally by the allocator itself,
not on the interpreter side.</p>
<p>The types <code>SizeClass</code> and <code>Mark</code> <em>are</em> provided by this crate and are enums.</p>
<p>The one drawback to this scheme is that it's possible to associate an incorrect
type id constant with an object. This would result in objects being misidentified
at runtime and accessed incorrectly, likely leading to panics.</p>
<p>Fortunately, this kind of trait implementation boilerplate is ideal for derive
macros. Since the language side will be implementing these structs and traits,
we'll defer until the relevant interpreter chapter to go over that.</p>
<h2><a class="header" href="#back-to-allocraw" id="back-to-allocraw">Back to AllocRaw</a></h2>
<p>Now that we have some object and header definitions and constraints, we need to
apply them to the <code>AllocRaw</code> API. We can't allocate an object unless it
implements <code>AllocObject</code> and has an associated constant that implements
<code>AllocTypeId</code>.  We also need to expand the interface with functions that the
interpreter can use to reliably get the header for an object and the object
for a header.</p>
<p>We will add an associated type to tie the allocator
API to the header type and indirectly to the type identification that will be
used.</p>
<pre><code class="language-rust ignore">pub trait AllocRaw {

    type Header: AllocHeader;

    ...
}
</code></pre>
<p>Then we can update the <code>alloc()</code> function definition to constrain the types
that can be allocated to only those that implement the appropriate traits.</p>
<pre><code class="language-rust ignore">pub trait AllocRaw {
    ...

    fn alloc&lt;T&gt;(&amp;self, object: T) -&gt; Result&lt;RawPtr&lt;T&gt;, AllocError&gt;
    where
        T: AllocObject&lt;&lt;Self::Header as AllocHeader&gt;::TypeId&gt;;

    ...
}
</code></pre>
<p>We need the user and the garbage collector to be able to access the header,
so we need a function that will return the header given an object pointer.</p>
<p>The garbage collector does not know about concrete types, it will need to
be able to get the header without knowing the object type. It's likely
that the interpreter will, at times, also not know the type at runtime.</p>
<p>Indeed, one of the functions of an object header is to, at runtime, given
an object pointer, derive the type of the object.</p>
<p>The function signature therefore cannot refer to the type. That is,
we can't write</p>
<pre><code class="language-rust ignore">pub trait AllocRaw {
    ...

    // looks good but won't work in all cases
    fn get_header&lt;T&gt;(object: RawPtr&lt;T&gt;) -&gt; NonNull&lt;Self::Header&gt;
    where
        T: AllocObject&lt;&lt;Self::Header as AllocHeader&gt;::TypeId&gt;;

    ...
}
</code></pre>
<p>even though it seems this would be good and right. Instead this function will
have to be much simpler:</p>
<pre><code class="language-rust ignore">pub trait AllocRaw {
    ...

    fn get_header(object: NonNull&lt;()&gt;) -&gt; NonNull&lt;Self::Header&gt;;

    ...
}
</code></pre>
<p>We also need a function to get the object <em>from</em> the header:</p>
<pre><code class="language-rust ignore">pub trait AllocRaw {
    ...

    fn get_object(header: NonNull&lt;Self::Header&gt;) -&gt; NonNull&lt;()&gt;;

    ...
}
</code></pre>
<p>These functions are not unsafe but they do return <code>NonNull</code> which implies that
dereferencing the result should be considered unsafe - there is no protection
against passing in garbage and getting garbage out.</p>
<p>Now we have an object allocation function, traits that constrain what can be
allocated, allocation header definitions and functions for switching
between an object and it's header.</p>
<p>There's one missing piece: we can allocate objects of type <code>T</code>, but
such objects always have compile-time defined size. <code>T</code> is constrained to
<code>Sized</code> types in the <code>RawPtr</code> definition. So how do we allocate dynamically
sized objects, such as arrays?</p>
<h2><a class="header" href="#dynamically-sized-types" id="dynamically-sized-types">Dynamically sized types</a></h2>
<p>Since we can allocate objects of type <code>T</code>, and each <code>T</code> must derive
<code>AllocObject</code> and have an associated const of type <code>AllocTypeId</code>, dynamically
sized allocations must fit into this type identification scheme.</p>
<p>Allocating dynamically sized types, or in short, arrays, means there's some
ambiguity about the type at compile time as far as the allocator is concerned:</p>
<ul>
<li>Are we allocating one object or an array of objects? If we're allocating an
array of objects, we'll have to initialize them all. Perhaps we don't want to
impose that overhead up front?</li>
<li>If the allocator knows how many objects compose an array, do we want to bake
fat pointers into the interface to carry that number around?</li>
</ul>
<p>In the same way, then, that the underlying implementation of <code>std::vec::Vec</code> is
backed by an array of <code>u8</code>, we'll do the same. We shall define the return type
of an array allocation to be of type <code>RawPtr&lt;u8&gt;</code> and the size requested to be
in bytes. We'll leave it to the interpreter to build layers on top of this to
handle the above questions.</p>
<p>As the definition of <code>AllocTypeId</code> is up to the interpreter, this crate can't
know the type id of an array. Instead, we will require the interpreter to
implement a function on the <code>AllocHeader</code> trait:</p>
<pre><code class="language-rust ignore">pub trait AllocHeader: Sized {
    ...

    fn new_array(size: ArraySize, size_class: SizeClass, mark: Mark) -&gt; Self;

    ...
}
</code></pre>
<p>This function should return a new object header for an array of u8 with the
appropriate type identifier.</p>
<p>We will also add a function to the <code>AllocRaw</code> trait for allocating arrays that
returns the <code>RawPtr&lt;u8&gt;</code> type.</p>
<pre><code class="language-rust ignore">pub trait AllocRaw {
    ...

    fn alloc_array(&amp;self, size_bytes: ArraySize) -&gt; Result&lt;RawPtr&lt;u8&gt;, AllocError&gt;;

    ...
}
</code></pre>
<p>Our complete <code>AllocRaw</code> trait definition now looks like this:</p>
<pre><code class="language-rust ignore">pub trait AllocRaw {
    /// An implementation of an object header type
    type Header: AllocHeader;

    /// Allocate a single object of type T.
    fn alloc&lt;T&gt;(&amp;self, object: T) -&gt; Result&lt;RawPtr&lt;T&gt;, AllocError&gt;
    where
        T: AllocObject&lt;&lt;Self::Header as AllocHeader&gt;::TypeId&gt;;

    /// Allocating an array allows the client to put anything in the resulting data
    /// block but the type of the memory block will simply be 'Array'. No other
    /// type information will be stored in the object header.
    /// This is just a special case of alloc&lt;T&gt;() for T=u8 but a count &gt; 1 of u8
    /// instances.  The caller is responsible for the content of the array.
    fn alloc_array(&amp;self, size_bytes: ArraySize) -&gt; Result&lt;RawPtr&lt;u8&gt;, AllocError&gt;;

    /// Given a bare pointer to an object, return the expected header address
    fn get_header(object: NonNull&lt;()&gt;) -&gt; NonNull&lt;Self::Header&gt;;

    /// Given a bare pointer to an object's header, return the expected object address
    fn get_object(header: NonNull&lt;Self::Header&gt;) -&gt; NonNull&lt;()&gt;;
}
</code></pre>
<p>In the next chapter we'll build out the <code>AllocRaw</code> trait implementation.</p>
<h1><a class="header" href="#implementing-the-allocation-api" id="implementing-the-allocation-api">Implementing the Allocation API</a></h1>
<p>In this final chapter of the allocation part of the book, we'll cover the
<code>AllocRaw</code> trait implementation.</p>
<p>This trait is implemented on the <code>StickyImmixHeap</code> struct:</p>
<pre><code class="language-rust ignore">impl&lt;H: AllocHeader&gt; AllocRaw for StickyImmixHeap&lt;H&gt; {
    type Header = H;

    ...
}
</code></pre>
<p>Here the associated header type is provided as the generic type <code>H</code>, leaving it
up to the interpreter to define.</p>
<h2><a class="header" href="#allocating-objects" id="allocating-objects">Allocating objects</a></h2>
<p>The first function to implement is <code>AllocRaw::alloc&lt;T&gt;()</code>. This function must:</p>
<ul>
<li>calculate how much space in bytes is required by the object and header</li>
<li>allocate that space</li>
<li>instantiate an object header and write it to the first bytes of the space</li>
<li>copy the object itself to the remaining bytes of the space</li>
<li>return a pointer to where the object lives in this space</li>
</ul>
<p>Let's look at the implementation.</p>
<pre><code class="language-rust ignore">impl&lt;H: AllocHeader&gt; AllocRaw for StickyImmixHeap&lt;H&gt; {
    fn alloc&lt;T&gt;(&amp;self, object: T) -&gt; Result&lt;RawPtr&lt;T&gt;, AllocError&gt;
    where
        T: AllocObject&lt;&lt;Self::Header as AllocHeader&gt;::TypeId&gt;,
    {
        // calculate the total size of the object and it's header
        let header_size = size_of::&lt;Self::Header&gt;();
        let object_size = size_of::&lt;T&gt;();
        let total_size = header_size + object_size;

        // round the size to the next word boundary to keep objects aligned and get the size class
        // TODO BUG? should this be done separately for header and object?
        //  If the base allocation address is where the header gets placed, perhaps
        //  this breaks the double-word alignment object alignment desire?
        let alloc_size = alloc_size_of(total_size);
        let size_class = SizeClass::get_for_size(alloc_size)?;

        // attempt to allocate enough space for the header and the object
        let space = self.find_space(alloc_size, size_class)?;

        // instantiate an object header for type T, setting the mark bit to &quot;allocated&quot;
        let header = Self::Header::new::&lt;T&gt;(object_size as ArraySize, size_class, Mark::Allocated);

        // write the header into the front of the allocated space
        unsafe {
            write(space as *mut Self::Header, header);
        }

        // write the object into the allocated space after the header
        let object_space = unsafe { space.offset(header_size as isize) };
        unsafe {
            write(object_space as *mut T, object);
        }

        // return a pointer to the object in the allocated space
        Ok(RawPtr::new(object_space as *const T))
    }
}
</code></pre>
<p>This, hopefully, is easy enough to follow after the previous chapters -</p>
<ul>
<li><code>self.find_space()</code> is the function described in the chapter
<a href="./chapter-managing-blocks.html#allocating-into-the-head-block">Allocating into multiple blocks</a></li>
<li><code>Self::Header::new()</code> will be implemented by the interpreter</li>
<li><code>write(space as *mut Self::Header, header)</code> calls the std function
<code>std::ptr::write</code></li>
</ul>
<h2><a class="header" href="#allocating-arrays" id="allocating-arrays">Allocating arrays</a></h2>
<p>We need a similar (but awkwardly different enough) implementation for array
allocation. The key differences are that the type is fixed to a <code>u8</code> pointer
and the array is initialized to zero bytes. It is up to the interpreter to
write into the array itself.</p>
<pre><code class="language-rust ignore">impl&lt;H: AllocHeader&gt; AllocRaw for StickyImmixHeap&lt;H&gt; {
    fn alloc_array(&amp;self, size_bytes: ArraySize) -&gt; Result&lt;RawPtr&lt;u8&gt;, AllocError&gt; {
        // calculate the total size of the array and it's header
        let header_size = size_of::&lt;Self::Header&gt;();
        let total_size = header_size + size_bytes as usize;

        // round the size to the next word boundary to keep objects aligned and get the size class
        let alloc_size = alloc_size_of(total_size);
        let size_class = SizeClass::get_for_size(alloc_size)?;

        // attempt to allocate enough space for the header and the array
        let space = self.find_space(alloc_size, size_class)?;

        // instantiate an object header for an array, setting the mark bit to &quot;allocated&quot;
        let header = Self::Header::new_array(size_bytes, size_class, Mark::Allocated);

        // write the header into the front of the allocated space
        unsafe {
            write(space as *mut Self::Header, header);
        }

        // calculate where the array will begin after the header
        let array_space = unsafe { space.offset(header_size as isize) };

        // Initialize object_space to zero here.
        // If using the system allocator for any objects (SizeClass::Large, for example),
        // the memory may already be zeroed.
        let array = unsafe { from_raw_parts_mut(array_space as *mut u8, size_bytes as usize) };
        // The compiler should recognize this as optimizable
        for byte in array {
            *byte = 0;
        }

        // return a pointer to the array in the allocated space
        Ok(RawPtr::new(array_space as *const u8))
    }
}
</code></pre>
<h2><a class="header" href="#switching-between-header-and-object" id="switching-between-header-and-object">Switching between header and object</a></h2>
<p>As stated in the previous chapter, these functions are essentially pointer
operations that do not dereference the pointers. Thus they are not unsafe
to call, but the types they operate <em>on</em> should have a suitably unsafe API.</p>
<p><code>NonNull</code> is the chosen parameter and return type and the pointer arithmetic
for obtaining the header from an object pointer of unknown type is shown
below.</p>
<p>For our Immix implementation, since headers are placed immediately
ahead of an object, we simply subtract the header size from the object
pointer.</p>
<pre><code class="language-rust ignore">impl&lt;H: AllocHeader&gt; AllocRaw for StickyImmixHeap&lt;H&gt; {
    fn get_header(object: NonNull&lt;()&gt;) -&gt; NonNull&lt;Self::Header&gt; {
        unsafe { NonNull::new_unchecked(object.cast::&lt;Self::Header&gt;().as_ptr().offset(-1)) }
    }
}
</code></pre>
<p>Getting the object from a header is the reverse - adding the header size
to the header pointer results in the object pointer:</p>
<pre><code class="language-rust ignore">impl&lt;H: AllocHeader&gt; AllocRaw for StickyImmixHeap&lt;H&gt; {
    fn get_object(header: NonNull&lt;Self::Header&gt;) -&gt; NonNull&lt;()&gt; {
        unsafe { NonNull::new_unchecked(header.as_ptr().offset(1).cast::&lt;()&gt;()) }
    }
}
</code></pre>
<h2><a class="header" href="#conclusion" id="conclusion">Conclusion</a></h2>
<p>Thus ends the first part of our Immix implementation. In the next part of the
book we will jump over the fence to the interpreter and begin using the
interfaces we've defined in this part.</p>
<h1><a class="header" href="#an-interpreter-eval-rs" id="an-interpreter-eval-rs">An interpreter: Eval-rs</a></h1>
<p>In this part of the book we'll dive into creating:</p>
<ul>
<li>a safe Rust layer on top of the Sticky Immix API of the previous part</li>
<li>a compiler for a primitive s-expression syntax language</li>
<li>a bytecode based virtual machine</li>
</ul>
<p>So what kind of interpreter will we implement? This book is a guide to help
you along your own journey and not not intended to provide an exhaustive
language ecosystem. The direction we'll take is to support John McCarthy's
classic s-expression based meta-circular evaluator<sup class="footnote-reference"><a href="#1">1</a></sup>.</p>
<p>Along the way we'll need to implement fundamental data types and structures
from scratch upon our safe layer - symbols, pairs, arrays and dicts - with
each chapter building upon the previous ones.</p>
<p>While this will not result in an exhaustive language implementation,
you'll see that we <em>will</em> end up with all the building blocks for you to take
it the rest of the way!</p>
<p>We shall name our interpreter &quot;Eval-rs&quot;, for which we have an appropriate
illustration generously provided by the author's then 10 year old daughter.</p>
<p><img src="evalrus-medium.png" alt="The Evalrus" /></p>
<p>We'll begin by defining the safe abstration over the Sticky Immix interface.
Then we'll put that to use in parsing s-expressions into a very simple data
structure.</p>
<p>Once we've covered those basics, we'll build arrays and dicts and then
use those in the compiler and virtual machine.</p>
<div class="footnote-definition" id="1"><sup class="footnote-definition-label">1</sup>
<p>These days this is cliché but that is substantially to our benefit. We're
not trying to create yet another Lisp, rather the fact that there is a
preexisting design of some elegance and historical interest is a convenience.
For a practical, accessible introduction to the topic, do see Paul
Graham's <a href="http://www.paulgraham.com/rootsoflisp.html">The Roots of Lisp</a></p>
</div>
<h1><a class="header" href="#allocating-objects-and-dereferencing-safely" id="allocating-objects-and-dereferencing-safely">Allocating objects and dereferencing safely</a></h1>
<p>In this chapter we'll build some safe Rust abstractions over the allocation API
defined in the Sticky Immix crate.</p>
<p>Let's first recall this interface:</p>
<pre><code class="language-rust ignore">pub trait AllocRaw {
    /// An implementation of an object header type
    type Header: AllocHeader;

    /// Allocate a single object of type T.
    fn alloc&lt;T&gt;(&amp;self, object: T) -&gt; Result&lt;RawPtr&lt;T&gt;, AllocError&gt;
    where
        T: AllocObject&lt;&lt;Self::Header as AllocHeader&gt;::TypeId&gt;;

    /// Allocating an array allows the client to put anything in the resulting data
    /// block but the type of the memory block will simply be 'Array'. No other
    /// type information will be stored in the object header.
    /// This is just a special case of alloc&lt;T&gt;() for T=u8 but a count &gt; 1 of u8
    /// instances.  The caller is responsible for the content of the array.
    fn alloc_array(&amp;self, size_bytes: ArraySize) -&gt; Result&lt;RawPtr&lt;u8&gt;, AllocError&gt;;

    /// Given a bare pointer to an object, return the expected header address
    fn get_header(object: NonNull&lt;()&gt;) -&gt; NonNull&lt;Self::Header&gt;;

    /// Given a bare pointer to an object's header, return the expected object address
    fn get_object(header: NonNull&lt;Self::Header&gt;) -&gt; NonNull&lt;()&gt;;
}
</code></pre>
<p>These are the functions we'll be calling. When we allocate an object, we'll get
back a <code>RawPtr&lt;T&gt;</code> which has no safe way to dereference it. This is impractical,
we very much do not want to wrap every dereferencing in <code>unsafe { ... }</code>.
We'll need a layer over <code>RawPtr&lt;T&gt;</code> where we can guarantee safe dereferencing.</p>
<h2><a class="header" href="#pointers" id="pointers">Pointers</a></h2>
<p>In safe Rust, mutable (<code>&amp;mut</code>) and immutable (<code>&amp;</code>) references are passed around
to access objects. These reference types are compile-time constrained pointers
where the constraints are</p>
<ol>
<li>the mutability of the access</li>
<li>the lifetime of the access</li>
</ol>
<p>For our layer over <code>RawPtr&lt;T&gt;</code> we'll have to consider both these constraints.</p>
<h3><a class="header" href="#mutability" id="mutability">Mutability</a></h3>
<p>This constraint is concerned with shared access to an object. In other words,
it cares about how many pointers there are to an object at any time and whether
they allow mutable or immutable access.</p>
<p>The short of it is:</p>
<ul>
<li>Either only one <code>&amp;mut</code> reference may be held in a scope</li>
<li>Or many <code>&amp;</code> immutable references may be held in a scope</li>
</ul>
<p>The compiler must be able to determine that a <code>&amp;mut</code> reference is the only
live reference in it's scope that points at an object in order
for mutable access to that object to be safe of data races.</p>
<p>In a runtime memory managed language such as the interpreter we are building,
we will not have compile time knowledge of shared access to objects. We
won't know at compile time how many pointers to an object we may have at
any time. This is the normal state of things in languages such as Python,
Ruby or Javascript.</p>
<p>This means that we can't allow <code>&amp;mut</code> references in our safe layer at all!</p>
<p>If we're restricted to <code>&amp;</code> immutable references everywhere, that then means
we must apply the interior mutability pattern everywhere in our design in
order to comply with the laws of safe Rust.</p>
<h3><a class="header" href="#lifetime" id="lifetime">Lifetime</a></h3>
<p>The second aspect to references is their lifetime. This concerns the
duration of the reference, from inception until it goes out of scope.</p>
<p>The key concept to think about now is &quot;scope.&quot;</p>
<p>In an interpreted language there are two major operations on the objects
in memory:</p>
<pre><code class="language-rust ignore">fn run_mutator() {
    parse_source_code();
    compile();
    execute_bytecode();
}
</code></pre>
<p>and</p>
<pre><code class="language-rust ignore">fn run_garbage_collection() {
   trace_objects();
   free_dead_objects();
}
</code></pre>
<p>A few paragraphs earlier we determined that we can't have <code>&amp;mut</code> references
to objects in our interpreter.</p>
<p>By extension, we can't safely hold a mutable reference to the entire heap
as a data structure.</p>
<p>Except, that is exactly what garbage collection requires. The nature of
garbage collection is that it views the entire heap as a single data structure
in it's own right that it needs to traverse and modify. It wants the
heap to be <code>&amp;mut</code>.</p>
<p>Consider, especially, that some garbage collectors <em>move</em> objects, so that
pointers to moved objects, wherever they may be, must be modified by the
garbage collector without breaking the mutator! The garbage collector must
be able to reliably discover <em>every single pointer to moved objects</em> to avoid
leaving invalid pointers scattered around<sup class="footnote-reference"><a href="#1">1</a></sup>.</p>
<p>Thus we have two mutually exclusive interface requirements, one that must
only hold <code>&amp;</code> object references and applies <em>interior</em> mutability to the heap
and the other that wants the whole heap to be <code>&amp;mut</code>.</p>
<p>For this part of the book, we'll focus on the use of the allocator and save
garbage collection for a later part.</p>
<p>This mutual exclusivity constraint on the allocator results in the statements:</p>
<ul>
<li>When garbage collection is running, it is not safe to run the mutator<sup class="footnote-reference"><a href="#2">2</a></sup></li>
<li>When garbage collection is not running, it is safe to run the mutator</li>
</ul>
<p>Thus our abstraction must encapsulate a concept of a time when &quot;it is safe to
run the mutator&quot; and since we're working with safe Rust, this must be a
compile time concept.</p>
<p>Scopes and lifetimes are perfect for this abstraction. What we'll need is
some way to define a lifetime (that is, a scope) within which access to the
heap by the mutator is safe.</p>
<h3><a class="header" href="#some-pointer-types" id="some-pointer-types">Some pointer types</a></h3>
<p>First, let's define a simple pointer type that can wrap an allocated type <code>T</code>
in a lifetime:</p>
<pre><code class="language-rust ignore">pub struct ScopedPtr&lt;'guard, T: Sized&gt; {
    value: &amp;'guard T,
}
</code></pre>
<p>This type will implement <code>Clone</code>, <code>Copy</code> and <code>Deref</code> - it can be passed around
freely within the scope and safely dereferenced.</p>
<p>As you can see we have a lifetime <code>'guard</code> that we'll use to restrict the
scope in which this pointer can be accessed. We need a mechanism to restrict
this scope.</p>
<p>The guard pattern is what we'll use, if the hint wasn't strong enough.</p>
<p>We'll construct some types that ensure that safe pointers such as
<code>ScopedPtr&lt;T&gt;</code>, and access to the heap at in any way, are mediated by an
instance of a guard type that can provide access.</p>
<p>We will end up passing a reference to the guard instance around everywhere. In
most cases we won't care about the instance type itself so much as the lifetime
that it carries with it. As such, we'll define a trait for this type to
implement that so that we can refer to the guard instance by this trait rather
than having to know the concrete type. This'll also allow other types to
proxy the main scope-guarding instance.</p>
<pre><code class="language-rust ignore">pub trait MutatorScope {}
</code></pre>
<p>You may have noticed that we've jumped from <code>RawPtr&lt;T&gt;</code> to <code>ScopedPtr&lt;T&gt;</code> with
seemingly nothing to bridge the gap. How do we <em>get</em> a <code>ScopedPtr&lt;T&gt;</code>?</p>
<p>We'll create a wrapper around <code>RawPtr&lt;T&gt;</code> that will complete the picture. This
wrapper type is what will hold pointers at rest inside any data structures.</p>
<pre><code class="language-rust ignore">#[derive(Clone)]
pub struct CellPtr&lt;T: Sized&gt; {
    inner: Cell&lt;RawPtr&lt;T&gt;&gt;,
}
</code></pre>
<p>This is straightforwardly a <code>RawPtr&lt;T&gt;</code> in a <code>Cell</code> to allow for modifying the
pointer. We won't allow dereferencing from this type either though.</p>
<p>Remember that dereferencing a heap object pointer is only safe when we are
in the right scope? We need to create a <code>ScopedPtr&lt;T&gt;</code> <em>from</em> a <code>CellPtr&lt;T&gt;</code>
to be able to use it.</p>
<p>First we'll add a helper function to <code>RawPtr&lt;T&gt;</code> in our interpreter crate so
we can safely dereference a <code>RawPtr&lt;T&gt;</code>. This code says that, given an instance
of a <code>MutatorScope</code>-implementing type, give me back a reference type with
the same lifetime as the guard that I can safely use. Since the <code>_guard</code>
parameter is never used except to define a lifetime, it should be optimized
out by the compiler!</p>
<pre><code class="language-rust ignore">pub trait ScopedRef&lt;T&gt; {
    fn scoped_ref&lt;'scope&gt;(&amp;self, guard: &amp;'scope dyn MutatorScope) -&gt; &amp;'scope T;
}

impl&lt;T&gt; ScopedRef&lt;T&gt; for RawPtr&lt;T&gt; {
    fn scoped_ref&lt;'scope&gt;(&amp;self, _guard: &amp;'scope dyn MutatorScope) -&gt; &amp;'scope T {
        unsafe { &amp;*self.as_ptr() }
    }
}
</code></pre>
<p>We'll use this in our <code>CellPtr&lt;T&gt;</code> to obtain a <code>ScopedPtr&lt;T&gt;</code>:</p>
<pre><code class="language-rust ignore">impl&lt;T: Sized&gt; CellPtr&lt;T&gt; {
    pub fn get&lt;'guard&gt;(&amp;self, guard: &amp;'guard dyn MutatorScope) -&gt; ScopedPtr&lt;'guard, T&gt; {
        ScopedPtr::new(guard, self.inner.get().scoped_ref(guard))
    }
}
</code></pre>
<p>Thus, anywhere (structs, enums) that needs to store a pointer to something on
the heap will use <code>CellPtr&lt;T&gt;</code> and any code that accesses these pointers
during the scope-guarded mutator code will obtain <code>ScopedPtr&lt;T&gt;</code> instances
that can be safely dereferenced.</p>
<h2><a class="header" href="#the-heap-and-the-mutator" id="the-heap-and-the-mutator">The heap and the mutator</a></h2>
<p>The next question is: where do we get an instance of <code>MutatorScope</code> from?</p>
<p>The lifetime of an instance of a <code>MutatorScope</code> will define the lifetime
of any safe object accesses. By following the guard pattern, we will find
we have:</p>
<ul>
<li>a heap struct that contains an instance of the Sticky Immix heap</li>
<li>a guard struct that proxies the heap struct for the duration of a scope</li>
<li>a mechanism to enforce the scope limit</li>
</ul>
<h3><a class="header" href="#a-heap-struct" id="a-heap-struct">A heap struct</a></h3>
<p>Let's make a type alias for the Sticky Immix heap so we aren't referring
to it as such throughout the interpreter:</p>
<pre><code class="language-rust ignore">pub type HeapStorage = StickyImmixHeap&lt;ObjectHeader&gt;;
</code></pre>
<p>The let's put that into a heap struct, along with any other
interpreter-global storage:</p>
<pre><code class="language-rust ignore">struct Heap {
    heap: HeapStorage,
    syms: SymbolMap,
}
</code></pre>
<p>We'll discuss the <code>SymbolMap</code> type in the next chapter.</p>
<p>Now, since we've wrapped the Sticky Immix heap in our own <code>Heap</code> struct,
we'll need to <code>impl</code> an <code>alloc()</code> method to proxy the Sticky Immix
allocation function.</p>
<pre><code class="language-rust ignore">impl Heap {
    fn alloc&lt;T&gt;(&amp;self, object: T) -&gt; Result&lt;RawPtr&lt;T&gt;, RuntimeError&gt;
    where
        T: AllocObject&lt;TypeList&gt;,
    {
        Ok(self.heap.alloc(object)?)
    }
}
</code></pre>
<p>A couple things to note about this function:</p>
<ul>
<li>It returns <code>RuntimeError</code> in the error case, this type converts <code>From</code> the
Sticky Immix crate's error type.</li>
<li>The <code>where</code> constraint is similar to that of <code>AllocRaw::alloc()</code> but in now
we have a concrete <code>TypeList</code> type to bind to. We'll look at <code>TypeList</code>
in the next chapter along with <code>SymbolMap</code>.</li>
</ul>
<h3><a class="header" href="#a-guard-struct" id="a-guard-struct">A guard struct</a></h3>
<p>This next struct will be used as a scope-limited proxy for the <code>Heap</code> struct
with one major difference: function return types will no longer be <code>RawPtr&lt;T&gt;</code>
but <code>ScopedPtr&lt;T&gt;</code>.</p>
<pre><code class="language-rust ignore">pub struct MutatorView&lt;'memory&gt; {
    heap: &amp;'memory Heap,
}
</code></pre>
<p>Here in this struct definition, it becomes clear that all we are doing is
borrowing the <code>Heap</code> instance for a limited lifetime. Thus, the lifetime of
the <code>MutatorView</code> instance <em>will be</em> the lifetime that all safe object
access is constrained to.</p>
<p>A look at the <code>alloc()</code> function now:</p>
<pre><code class="language-rust ignore">impl&lt;'memory&gt; MutatorView&lt;'memory&gt; {
    pub fn alloc&lt;T&gt;(&amp;self, object: T) -&gt; Result&lt;ScopedPtr&lt;'_, T&gt;, RuntimeError&gt;
    where
        T: AllocObject&lt;TypeList&gt;,
    {
        Ok(ScopedPtr::new(
            self,
            self.heap.alloc(object)?.scoped_ref(self),
        ))
    }
}
</code></pre>
<p>Very similar to <code>Heap::alloc()</code> but the return type is now a <code>ScopedPtr&lt;T&gt;</code>
whose lifetime is the same as the <code>MutatorView</code> instance.</p>
<h3><a class="header" href="#enforcing-a-scope-limit" id="enforcing-a-scope-limit">Enforcing a scope limit</a></h3>
<p>We now have a <code>Heap</code> and a guard, <code>MutatorView</code>, but we want one more thing:
to prevent an instance of <code>MutatorView</code> from being returned from anywhere -
that is, enforcing a scope within which an instance of <code>MutatorView</code> will
live and die. This will make it easier to separate mutator operations and
garbage collection operations.</p>
<p>First we'll apply a constraint on how a mutator <em>gains</em> heap access: through
a trait.</p>
<pre><code class="language-rust ignore">pub trait Mutator: Sized {
    type Input;
    type Output;

    fn run(&amp;self, mem: &amp;MutatorView, input: Self::Input) -&gt; Result&lt;Self::Output, RuntimeError&gt;;

    // TODO
    // function to return iterator that iterates over roots
}
</code></pre>
<p>If a piece of code wants to access the heap, it <em>must</em> implement this trait!</p>
<p>Secondly, we'll apply another wrapper struct, this time to the <code>Heap</code> type.
This is so that we can borrow the <code>heap</code> member instance.</p>
<pre><code class="language-rust ignore">pub struct Memory {
    heap: Heap,
}
</code></pre>
<p>This <code>Memory</code> struct and the <code>Mutator</code> trait are now tied together with a
function:</p>
<pre><code class="language-rust ignore">impl Memory {
    pub fn mutate&lt;M: Mutator&gt;(&amp;self, m: &amp;M, input: M::Input) -&gt; Result&lt;M::Output, RuntimeError&gt; {
        let mut guard = MutatorView::new(self);
        m.run(&amp;mut guard, input)
    }

}
</code></pre>
<p>The key to the scope limitation mechanism is that this <code>mutate</code> function is
the only way to gain access to the heap. It creates an instance of
<code>MutatorView</code> that goes out of scope at the end of the function and thus
can't leak outside of the call stack.</p>
<h2><a class="header" href="#an-example-1" id="an-example-1">An example</a></h2>
<p>Let's construct a simple example to demonstrate these many parts. This
will omit defining a <code>TypeId</code> and any other types that we didn't discuss
above.</p>
<pre><code class="language-rust ignore">struct Stack {}

impl Stack {
    fn say_hello(&amp;self) {
        println!(&quot;I'm the stack!&quot;);
    }
}

struct Roots {
    stack: CellPtr&lt;Stack&gt;
}

impl Roots {
    fn new(stack: ScopedPtr&lt;'_, Stack&gt;) -&gt; Roots {
        Roots {
            stack: CellPtr::new_with(stack)
        }
    }
}

struct Interpreter {}

impl Mutator for Interpreter {
    type Input: ();
    type Output: Roots;

    fn run(&amp;self, mem: &amp;MutatorView, input: Self::Input) -&gt; Result&lt;Self::Output, RuntimeError&gt; {
        let stack = mem.alloc(Stack {})?;   // returns a ScopedPtr&lt;'_, Stack&gt;
        stack.say_hello();

        let roots = Roots::new(stack);

        let stack_ptr = roots.stack.get(mem);  // returns a ScopedPtr&lt;'_, Stack&gt;
        stack_ptr.say_hello();

        Ok(roots)
    }
}

fn main() {
    ...
    let interp = Interpreter {};

    let result = memory.mutate(&amp;interp, ());

    let roots = result.unwrap();

    // no way to do this - compile error
    let stack = roots.stack.get();
    ...
}
</code></pre>
<p>In this simple, contrived example, we instantiated a <code>Stack</code> on the heap.
An instance of <code>Roots</code> is created on the native stack and given a pointer
to the <code>Stack</code> instance. The mutator returns the <code>Roots</code> object, which
continues to hold a pointer to a heap object. However, outside of the <code>run()</code>
function, the <code>stack</code> member can't be safely accesed.</p>
<p>Up next: using this framework to implement parsing!</p>
<hr />
<div class="footnote-definition" id="1"><sup class="footnote-definition-label">1</sup>
<p>This is the topic of discussion in Felix Klock's series
<a href="http://blog.pnkfx.org/blog/categories/gc/">GC and Rust</a> which is recommended
reading.</p>
</div>
<div class="footnote-definition" id="2"><sup class="footnote-definition-label">2</sup>
<p>while this distinction exists at the interface level, in reality there
are multiple phases in garbage collection and not all of them require
exclusive access to the heap. This is an advanced topic that we won't
bring into consideration yet.</p>
</div>
<h1><a class="header" href="#tagged-pointers-and-object-headers" id="tagged-pointers-and-object-headers">Tagged pointers and object headers</a></h1>
<p>Since our virtual machine will support a dynamic language where the compiler
does no type checking, all the type information will be managed at runtime.</p>
<p>In the previous chapter, we introduced a pointer type <code>ScopedPtr&lt;T&gt;</code>. This
pointer type has compile time knowledge of the type it is pointing at.</p>
<p>We need an alternative to <code>ScopedPtr&lt;T&gt;</code> that can represent all the
runtime-visible types so they can be resolved <em>at</em> runtime.</p>
<p>As we'll see, carrying around type information or looking it up in the
header on every access will be inefficient space and performance-wise.</p>
<p>We'll implement a common optimization: tagged pointers.</p>
<h2><a class="header" href="#runtime-type-identification" id="runtime-type-identification">Runtime type identification</a></h2>
<p>The object header can always give us the type id for an object, given a pointer
to the object. However, it requires us to do some arithmetic on the pointer
to get the location of the type identifier, then dereference the pointer to get
the type id value. This dereference can be expensive if the object being
pointed at is not in the CPU cache. Since getting an object type is a very
common operation in a dynamic language, these lookups become expensive,
time-wise.</p>
<p>Rust itself doesn't have runtime type <em>identification</em> but does have runtime
dispatch through trait objects. In this scheme a pointer consists of two words:
the pointer to the object itself and a second pointer to the vtable where the
concrete object type's methods can be looked up. The generic name for this form
of pointer is a <em>fat</em> pointer.</p>
<p>We could easily use a fat pointer type for runtime type identification
in our interpreter. Each pointer could carry with it an additional word with
the type id in it, or we could even just use trait objects!</p>
<p>A dynamically typed language will manage many pointers that must be type
identified at runtime. Carrying around an extra word per pointer is expensive,
space-wise, however.</p>
<h2><a class="header" href="#tagged-pointers" id="tagged-pointers">Tagged pointers</a></h2>
<p>Many runtimes implement <a href="https://en.wikipedia.org/wiki/Tagged_pointer">tagged pointers</a> to avoid the space overhead, while
partially improving the time overhead of the header type-id lookup.</p>
<p>In a pointer to any object on the heap, the least most significant bits turn out
to always be zero due to word or double-word alignment.</p>
<p>On a 64 bit platform, a pointer is a 64 bit word. Since objects are
at least word-aligned, a pointer is always be a multiple of 8 and
the 3 least significant bits are always 0. On 32 bit platforms, the 2 least
significant bits are always 0.</p>
<pre><code>  64..............48..............32..............16...........xxx
0b1111111111111111111111111111111111111111111111111111111111111000
                                                               / |
                                                              /  |
                                                            unused
</code></pre>
<p>When dereferencing a pointer, these bits must always be zero. But we <em>can</em> use
them in pointers at rest to store a limited type identifier! We'll limit
ourselves to 2 bits of type identifier so as to not complicate our code in
distinguishing between 32 and 64 bit platforms<sup class="footnote-reference"><a href="#1">1</a></sup>.</p>
<p>Given we'll only have 4 possible types we can id directly from a pointer,
we'll still need to fall back on the object header for types that don't fit
into this range.</p>
<h2><a class="header" href="#encoding-this-in-rust" id="encoding-this-in-rust">Encoding this in Rust</a></h2>
<p>Flipping bits on a pointer directly definitely constitutes a big Unsafe. We'll
need to make a tagged pointer type that will fundamentally be <code>unsafe</code> because
it won't be safe to dereference it. Then we'll need a safe abstraction over
that type to make it safe to dereference.</p>
<p>But first we need to understand the object header and how we get an object's
type from it.</p>
<h3><a class="header" href="#the-object-header" id="the-object-header">The object header</a></h3>
<p>We introduced the object header traits in the earlier chapter
<a href="./chapter-allocation-api.html">Defining the allocation API</a>. The chapter
explained how the object header is the responsibility of the interpreter to
implement.</p>
<p>Now that we need to implement type identification, we need the object header.</p>
<p>The allocator API requires that the type identifier implement the
<code>AllocTypeId</code> trait. We'll use an <code>enum</code> to identify for all our runtime types:</p>
<pre><code class="language-rust ignore">#[repr(u16)]
#[derive(Debug, Copy, Clone, PartialEq)]
pub enum TypeList {
    ArrayBackingBytes,
    ArrayOpcode,
    ArrayU8,
    ArrayU16,
    ArrayU32,
    ByteCode,
    CallFrameList,
    Dict,
    Function,
    InstructionStream,
    List,
    NumberObject,
    Pair,
    Partial,
    Symbol,
    Text,
    Thread,
    Upvalue,
}

// Mark this as a Stickyimmix type-identifier type
impl AllocTypeId for TypeList {}
</code></pre>
<p>Given that the allocator API requires every object that can be allocated to
have an associated type id <code>const</code>, this <code>enum</code> represents every type that
can be allocated and that we will go on to describe in this book.</p>
<p>It is a member of the <code>ObjectHeader</code> struct along with a few other members
that our Immix implementation requires:</p>
<pre><code class="language-rust ignore">pub struct ObjectHeader {
    mark: Mark,
    size_class: SizeClass,
    type_id: TypeList,
    size_bytes: u32,
}
</code></pre>
<p>The rest of the header members will be the topic of the later garbage
collection part of the book.</p>
<h3><a class="header" href="#a-safe-pointer-abstraction" id="a-safe-pointer-abstraction">A safe pointer abstraction</a></h3>
<p>A type that can represent one of multiple types at runtime is obviously the
<code>enum</code>. We can wrap possible <code>ScopedPtr&lt;T&gt;</code> types like so:</p>
<pre><code class="language-rust ignore">#[derive(Copy, Clone)]
pub enum Value&lt;'guard&gt; {
    ArrayU8(ScopedPtr&lt;'guard, ArrayU8&gt;),
    ArrayU16(ScopedPtr&lt;'guard, ArrayU16&gt;),
    ArrayU32(ScopedPtr&lt;'guard, ArrayU32&gt;),
    Dict(ScopedPtr&lt;'guard, Dict&gt;),
    Function(ScopedPtr&lt;'guard, Function&gt;),
    List(ScopedPtr&lt;'guard, List&gt;),
    Nil,
    Number(isize),
    NumberObject(ScopedPtr&lt;'guard, NumberObject&gt;),
    Pair(ScopedPtr&lt;'guard, Pair&gt;),
    Partial(ScopedPtr&lt;'guard, Partial&gt;),
    Symbol(ScopedPtr&lt;'guard, Symbol&gt;),
    Text(ScopedPtr&lt;'guard, Text&gt;),
    Upvalue(ScopedPtr&lt;'guard, Upvalue&gt;),
}
</code></pre>
<p>Note that this definition does <em>not</em> include all the same types that were
listed above in <code>TypeList</code>. Only the types that can be passed dynamically at
runtime need to be represented here. The types not included here are always
referenced directly by <code>ScopedPtr&lt;T&gt;</code> and are therefore known types at
compile and run time.</p>
<p>You probably also noticed that <code>Value</code> <em>is</em> the fat pointer we discussed
earlier. It is composed of a set of <code>ScopedPtr&lt;T&gt;</code>s, each of which should
only require a single word, and an <code>enum</code> discriminant integer, which will
also, due to alignment, require a word.</p>
<p>This <code>enum</code>, since it wraps <code>ScopedPtr&lt;T&gt;</code> and has the same requirement
for an explicit lifetime, is Safe To Dereference.</p>
<p>As this type occupies the same space as a fat pointer, it isn't the type
we want for storing pointers at rest, though. For that type, let's look at
the compact tagged pointer type now.</p>
<h3><a class="header" href="#what-lies-beneath" id="what-lies-beneath">What lies beneath</a></h3>
<p>Below we have a <code>union</code> type, making this an unsafe representation of a pointer.
The <code>tag</code> value will be constrained to the values 0, 1, 2 or 3, which will
determine which of the next four possible members should be accessed. Members
will have to be bit-masked to access their correct values.</p>
<pre><code class="language-rust ignore">#[derive(Copy, Clone)]
pub union TaggedPtr {
    tag: usize,
    number: isize,
    symbol: NonNull&lt;Symbol&gt;,
    pair: NonNull&lt;Pair&gt;,
    object: NonNull&lt;()&gt;,
}
</code></pre>
<p>As you can see, we've allocated a tag for a <code>Symbol</code> type, a <code>Pair</code> type and
one for a numeric type. The fourth member indicates an object whose type
must be determined from the type id in the object header.</p>
<blockquote>
<p><em><strong>Note:</strong></em> Making space for an inline integer is a common use of a tag. It
means any integer arithmetic that fits within the available bits will not
require memory lookups into the heap to retrieve operands. In our case we've
defined the numeric type as an <code>isize</code>. Since the 2 least significant bits
are used for the tag, we will have to right-shift the value by 2 to extract
the correct integer value. We'll go into this implementation in more depth
in a later chapter.</p>
</blockquote>
<p>The tags and masks are defined as:</p>
<pre><code class="language-rust ignore">const TAG_MASK: usize = 0x3;
pub const TAG_SYMBOL: usize = 0x0;
pub const TAG_PAIR: usize = 0x1;
pub const TAG_OBJECT: usize = 0x2;
pub const TAG_NUMBER: usize = 0x3;
const PTR_MASK: usize = !0x3;
</code></pre>
<p>Thus you can see from the choice of embedded tag values, we've optimized for
fast identification of <code>Pair</code>s and <code>Symbol</code>s and integer math. If we decide to,
it will be easy to switch to other types to represent in the 2 tag bits.</p>
<h3><a class="header" href="#connecting-into-the-allocation-api" id="connecting-into-the-allocation-api">Connecting into the allocation API</a></h3>
<p>Translating between <code>Value</code> and <code>TaggedPtr</code> will be made easier by creating
an intermediate type that represents all types as an <code>enum</code> but doesn't require
a valid lifetime. This type will be useful because it is most closely
ergonomic with the allocator API and the object header type information.</p>
<pre><code class="language-rust.ignore">#[derive(Copy, Clone)]
pub enum FatPtr {
    ArrayU8(RawPtr&lt;ArrayU8&gt;),
    ArrayU16(RawPtr&lt;ArrayU16&gt;),
    ArrayU32(RawPtr&lt;ArrayU32&gt;),
    Dict(RawPtr&lt;Dict&gt;),
    Function(RawPtr&lt;Function&gt;),
    List(RawPtr&lt;List&gt;),
    Nil,
    Number(isize),
    NumberObject(RawPtr&lt;NumberObject&gt;),
    Pair(RawPtr&lt;Pair&gt;),
    Partial(RawPtr&lt;Partial&gt;),
    Symbol(RawPtr&lt;Symbol&gt;),
    Text(RawPtr&lt;Text&gt;),
    Upvalue(RawPtr&lt;Upvalue&gt;),
}
</code></pre>
<p>We'll extend <code>Heap</code> (see previous chapter) with a method to return a tagged
pointer on request:</p>
<pre><code class="language-rust ignore">impl Heap {
    fn alloc_tagged&lt;T&gt;(&amp;self, object: T) -&gt; Result&lt;TaggedPtr, RuntimeError&gt;
    where
        FatPtr: From&lt;RawPtr&lt;T&gt;&gt;,
        T: AllocObject&lt;TypeList&gt;,
    {
        Ok(TaggedPtr::from(FatPtr::from(self.heap.alloc(object)?)))
    }
}
</code></pre>
<p>In this method it's clear that we implemented <code>From&lt;T&gt;</code> to convert
between pointer types. Next we'll look at how these conversions are
implemented.</p>
<h2><a class="header" href="#type-conversions" id="type-conversions">Type conversions</a></h2>
<p>We have three pointer types: <code>Value</code>, <code>FatPtr</code> and <code>TaggedPtr</code>, each which
has a distinct flavor. We need to be able to convert from one to the other:</p>
<pre><code>TaggedPtr &lt;-&gt; FatPtr -&gt; Value
</code></pre>
<h3><a class="header" href="#fatptr-to-value" id="fatptr-to-value">FatPtr to Value</a></h3>
<p>We can implement <code>From&lt;FatPtr&gt;</code> for <code>TaggedPtr</code> and <code>Value</code>
to convert to the final two possible pointer representations.
Well, not exactly - the function signature</p>
<pre><code class="language-rust ignore">impl From&lt;FatPtr&gt; for Value&lt;'guard&gt; {
    fn from(ptr: FatPtr) -&gt; Value&lt;'guard&gt; { ... }
}
</code></pre>
<p>is not able to define the <code>'guard</code> lifetime, so we have to implement a
similar method that can:</p>
<pre><code class="language-rust ignore">impl FatPtr {
    pub fn as_value&lt;'guard&gt;(&amp;self, guard: &amp;'guard dyn MutatorScope) -&gt; Value&lt;'guard&gt; {
        match self {
            FatPtr::ArrayU8(raw_ptr) =&gt; {
                Value::ArrayU8(ScopedPtr::new(guard, raw_ptr.scoped_ref(guard)))
            }
            FatPtr::ArrayU16(raw_ptr) =&gt; {
                Value::ArrayU16(ScopedPtr::new(guard, raw_ptr.scoped_ref(guard)))
            }
            FatPtr::ArrayU32(raw_ptr) =&gt; {
                Value::ArrayU32(ScopedPtr::new(guard, raw_ptr.scoped_ref(guard)))
            }
            FatPtr::Dict(raw_ptr) =&gt; Value::Dict(ScopedPtr::new(guard, raw_ptr.scoped_ref(guard))),
            FatPtr::Function(raw_ptr) =&gt; {
                Value::Function(ScopedPtr::new(guard, raw_ptr.scoped_ref(guard)))
            }
            FatPtr::List(raw_ptr) =&gt; Value::List(ScopedPtr::new(guard, raw_ptr.scoped_ref(guard))),
            FatPtr::Nil =&gt; Value::Nil,
            FatPtr::Number(num) =&gt; Value::Number(*num),
            FatPtr::NumberObject(raw_ptr) =&gt; {
                Value::NumberObject(ScopedPtr::new(guard, raw_ptr.scoped_ref(guard)))
            }
            FatPtr::Pair(raw_ptr) =&gt; Value::Pair(ScopedPtr::new(guard, raw_ptr.scoped_ref(guard))),
            FatPtr::Partial(raw_ptr) =&gt; {
                Value::Partial(ScopedPtr::new(guard, raw_ptr.scoped_ref(guard)))
            }
            FatPtr::Symbol(raw_ptr) =&gt; {
                Value::Symbol(ScopedPtr::new(guard, raw_ptr.scoped_ref(guard)))
            }
            FatPtr::Text(raw_ptr) =&gt; Value::Text(ScopedPtr::new(guard, raw_ptr.scoped_ref(guard))),
            FatPtr::Upvalue(raw_ptr) =&gt; {
                Value::Upvalue(ScopedPtr::new(guard, raw_ptr.scoped_ref(guard)))
            }
        }
    }
}
</code></pre>
<h3><a class="header" href="#fatptr-to-taggedptr" id="fatptr-to-taggedptr">FatPtr to TaggedPtr</a></h3>
<p>For converting down to a single-word <code>TaggedPtr</code> type we will introduce a helper
trait and methods to work with tag values and <code>RawPtr&lt;T&gt;</code> types from the
allocator:</p>
<pre><code class="language-rust ignore">pub trait Tagged&lt;T&gt; {
    fn tag(self, tag: usize) -&gt; NonNull&lt;T&gt;;
    fn untag(from: NonNull&lt;T&gt;) -&gt; RawPtr&lt;T&gt;;
}

impl&lt;T&gt; Tagged&lt;T&gt; for RawPtr&lt;T&gt; {
    fn tag(self, tag: usize) -&gt; NonNull&lt;T&gt; {
        unsafe { NonNull::new_unchecked((self.as_word() | tag) as *mut T) }
    }

    fn untag(from: NonNull&lt;T&gt;) -&gt; RawPtr&lt;T&gt; {
        RawPtr::new((from.as_ptr() as usize &amp; PTR_MASK) as *const T)
    }
}
</code></pre>
<p>This will help convert from <code>RawPtr&lt;T&gt;</code> values in <code>FatPtr</code> to the <code>NonNull&lt;T&gt;</code>
based <code>TaggedPtr</code> discriminants.</p>
<p>Because <code>TaggedPtr</code> is a <code>union</code> type and because it has to apply the
appropriate tag value inside the pointer itself, we can't work with it as
ergnomically as an <code>enum</code>. We'll create some more helper functions for
instantiating <code>TaggedPtr</code>s appropriately.</p>
<p>Remember that for storing an integer in the pointer we have to left-shift it 2
bits to allow for the tag. We'll apply proper range checking in a later chapter.</p>
<pre><code class="language-rust ignore">impl TaggedPtr {
    pub fn nil() -&gt; TaggedPtr {
        TaggedPtr { tag: 0 }
    }

    pub fn number(value: isize) -&gt; TaggedPtr {
        TaggedPtr {
            number: (((value as usize) &lt;&lt; 2) | TAG_NUMBER) as isize,
        }
    }

    pub fn symbol(ptr: RawPtr&lt;Symbol&gt;) -&gt; TaggedPtr {
        TaggedPtr {
            symbol: ptr.tag(TAG_SYMBOL),
        }
    }

    fn pair(ptr: RawPtr&lt;Pair&gt;) -&gt; TaggedPtr {
        TaggedPtr {
            pair: ptr.tag(TAG_PAIR),
        }
    }
}
</code></pre>
<p>Finally, we can use the above methods to implement <code>From&lt;FatPtr</code> for <code>TaggedPtr</code>:</p>
<pre><code class="language-rust ignore">impl From&lt;FatPtr&gt; for TaggedPtr {
    fn from(ptr: FatPtr) -&gt; TaggedPtr {
        match ptr {
            FatPtr::ArrayU8(raw) =&gt; TaggedPtr::object(raw),
            FatPtr::ArrayU16(raw) =&gt; TaggedPtr::object(raw),
            FatPtr::ArrayU32(raw) =&gt; TaggedPtr::object(raw),
            FatPtr::Dict(raw) =&gt; TaggedPtr::object(raw),
            FatPtr::Function(raw) =&gt; TaggedPtr::object(raw),
            FatPtr::List(raw) =&gt; TaggedPtr::object(raw),
            FatPtr::Nil =&gt; TaggedPtr::nil(),
            FatPtr::Number(value) =&gt; TaggedPtr::number(value),
            FatPtr::NumberObject(raw) =&gt; TaggedPtr::object(raw),
            FatPtr::Pair(raw) =&gt; TaggedPtr::pair(raw),
            FatPtr::Partial(raw) =&gt; TaggedPtr::object(raw),
            FatPtr::Text(raw) =&gt; TaggedPtr::object(raw),
            FatPtr::Symbol(raw) =&gt; TaggedPtr::symbol(raw),
            FatPtr::Upvalue(raw) =&gt; TaggedPtr::object(raw),
        }
    }
}
</code></pre>
<h3><a class="header" href="#taggedptr-to-fatptr" id="taggedptr-to-fatptr">TaggedPtr to FatPtr</a></h3>
<p>To convert from a <code>TaggedPtr</code> to the intermediate type is implemented in two
parts: identifying object types from the tag; identifying object types from the
header where the tag is insufficient.</p>
<p>Part the first, which requires <code>unsafe</code> due to accessing a <code>union</code> type and
dereferencing the object header for the <code>TAG_OBJECT</code> discriminant:</p>
<pre><code class="language-rust ignore">impl From&lt;TaggedPtr&gt; for FatPtr {
    fn from(ptr: TaggedPtr) -&gt; FatPtr {
        ptr.into_fat_ptr()
    }
}

impl TaggedPtr {
    fn into_fat_ptr(&amp;self) -&gt; FatPtr {
        unsafe {
            if self.tag == 0 {
                FatPtr::Nil
            } else {
                match get_tag(self.tag) {
                    TAG_NUMBER =&gt; FatPtr::Number(self.number &gt;&gt; 2),
                    TAG_SYMBOL =&gt; FatPtr::Symbol(RawPtr::untag(self.symbol)),
                    TAG_PAIR =&gt; FatPtr::Pair(RawPtr::untag(self.pair)),

                    TAG_OBJECT =&gt; {
                        let untyped_object_ptr = RawPtr::untag(self.object).as_untyped();
                        let header_ptr = HeapStorage::get_header(untyped_object_ptr);

                        header_ptr.as_ref().get_object_fatptr()
                    }

                    _ =&gt; panic!(&quot;Invalid TaggedPtr type tag!&quot;),
                }
            }
        }
    }
}
</code></pre>
<p>And part two, the object header method <code>get_object_fatptr()</code> as seen in the
code above:</p>
<pre><code class="language-rust ignore">impl ObjectHeader {
    pub unsafe fn get_object_fatptr(&amp;self) -&gt; FatPtr {
        let ptr_to_self = self.non_null_ptr();
        let object_addr = HeapStorage::get_object(ptr_to_self);

        match self.type_id {
            TypeList::ArrayU8 =&gt; FatPtr::ArrayU8(RawPtr::untag(object_addr.cast::&lt;ArrayU8&gt;())),
            TypeList::ArrayU16 =&gt; FatPtr::ArrayU16(RawPtr::untag(object_addr.cast::&lt;ArrayU16&gt;())),
            TypeList::ArrayU32 =&gt; FatPtr::ArrayU32(RawPtr::untag(object_addr.cast::&lt;ArrayU32&gt;())),
            TypeList::Dict =&gt; FatPtr::Dict(RawPtr::untag(object_addr.cast::&lt;Dict&gt;())),
            TypeList::Function =&gt; FatPtr::Function(RawPtr::untag(object_addr.cast::&lt;Function&gt;())),
            TypeList::List =&gt; FatPtr::List(RawPtr::untag(object_addr.cast::&lt;List&gt;())),
            TypeList::NumberObject =&gt; {
                FatPtr::NumberObject(RawPtr::untag(object_addr.cast::&lt;NumberObject&gt;()))
            }
            TypeList::Pair =&gt; FatPtr::Pair(RawPtr::untag(object_addr.cast::&lt;Pair&gt;())),
            TypeList::Partial =&gt; FatPtr::Partial(RawPtr::untag(object_addr.cast::&lt;Partial&gt;())),
            TypeList::Symbol =&gt; FatPtr::Symbol(RawPtr::untag(object_addr.cast::&lt;Symbol&gt;())),
            TypeList::Text =&gt; FatPtr::Text(RawPtr::untag(object_addr.cast::&lt;Text&gt;())),
            TypeList::Upvalue =&gt; FatPtr::Upvalue(RawPtr::untag(object_addr.cast::&lt;Upvalue&gt;())),

            // Other types not represented by FatPtr are an error to id here
            _ =&gt; panic!(&quot;Invalid ObjectHeader type tag {:?}!&quot;, self.type_id),
        }
    }
}
</code></pre>
<p>This method contains no unsafe code and yet we've declared it unsafe!</p>
<p>Manipulating pointer types is not unsafe in of itself, only dereferencing them
is unsafe and we are not dereferencing them here.</p>
<p>While we have the safety rails of the <code>enum</code> types to prevent
<em>invalid</em> types from being returned, we could easily mismatch a <code>TypeList</code> value
with an incorrect <code>FatPtr</code> value and return an <em>incorrect</em> type. Additionally
we could forget to untag a pointer, leaving it as an invalid pointer value.</p>
<p>These possible mistakes could cause undefined behavior and quite likely crash
the interpreter.</p>
<p>The compiler will not catch these cases and so this is an area for critical
scrutiny of correctness! Hence the method is marked unsafe to draw attention.</p>
<h2><a class="header" href="#using-tagged-pointers-in-data-structures" id="using-tagged-pointers-in-data-structures">Using tagged pointers in data structures</a></h2>
<p>Finally, we need to see how to use these types in data structures that we'll
create.</p>
<p>In the previous chapter, we defined a <code>CellPtr</code> type that wrapped a <code>RawPtr&lt;T&gt;</code>
in a <code>Cell&lt;T&gt;</code> so that data structures can contain mutable pointers to other
objects. Similarly, we'll want something to wrap tagged pointers:</p>
<pre><code class="language-rust ignore">#[derive(Clone)]
pub struct TaggedCellPtr {
    inner: Cell&lt;TaggedPtr&gt;,
}
</code></pre>
<p>We'll also wrap <code>Value</code> in a type <code>TaggedScopedPtr</code> that we'll use similarly
to <code>ScopedPtr&lt;T&gt;</code>.</p>
<pre><code class="language-rust ignore">#[derive(Copy, Clone)]
pub struct TaggedScopedPtr&lt;'guard&gt; {
    ptr: TaggedPtr,
    value: Value&lt;'guard&gt;,
}
</code></pre>
<p>This <code>TaggedScopedPtr</code> carries an instance of <code>TaggedPtr</code> <em>and</em> a <code>Value</code>.
This tradeoff means that while this type has three words to heft around,
the <code>TaggedPtr</code> member can be quickly accessed for copying into a
<code>TaggedCellPtr</code> without needing to down-convert from <code>Value</code>.</p>
<p>The type is only suitable for handling pointers that actively need to be
dereferenced due to it's size.</p>
<blockquote>
<p><em><strong>Note:</strong></em> Redundancy: TaggedScopedPtr and Value are almost
identical in requirement and functionality.
TODO: consider merging into one type.
See issue <a href="https://github.com/rust-hosted-langs/book/issues/30">https://github.com/rust-hosted-langs/book/issues/30</a></p>
</blockquote>
<p>A <code>TaggedScopedPtr</code> can be obtained by:</p>
<ul>
<li>calling <code>TaggedCellPtr::get()</code></li>
<li>or the <code>MutatorView::alloc_tagged()</code> method</li>
</ul>
<p>The <code>get()</code> method on <code>TaggedCellPtr</code> returns a <code>TaggedScopedPtr</code>:</p>
<pre><code class="language-rust ignore">impl TaggedCellPtr {
    pub fn get&lt;'guard&gt;(&amp;self, guard: &amp;'guard dyn MutatorScope) -&gt; TaggedScopedPtr&lt;'guard&gt; {
        TaggedScopedPtr::new(guard, self.inner.get())
    }
}
</code></pre>
<p>The <code>MutatorView</code> method to allocate a new object and get back a tagged
pointer (a <code>TaggedScopedPtr</code>) looks simply like this:</p>
<pre><code class="language-rust ignore">impl MutatorView {
    pub fn alloc_tagged&lt;T&gt;(&amp;self, object: T) -&gt; Result&lt;TaggedScopedPtr&lt;'_&gt;, RuntimeError&gt;
    where
        FatPtr: From&lt;RawPtr&lt;T&gt;&gt;,
        T: AllocObject&lt;TypeList&gt;,
    {
        Ok(TaggedScopedPtr::new(self, self.heap.alloc_tagged(object)?))
    }
}
</code></pre>
<h2><a class="header" href="#quick-recap" id="quick-recap">Quick recap</a></h2>
<p>In summary, what we created here was a set of pointer types:</p>
<ul>
<li>types suitable for storing a pointer at rest - <code>TaggedPtr</code> and <code>TaggedCellPtr</code></li>
<li>types suitable for dereferencing a pointer - <code>Value</code> and <code>TaggedScopedPtr</code></li>
<li>a type suitable for intermediating between the two - <code>FatPtr</code> - that the
heap allocation interface can return</li>
</ul>
<p>We now have the basic pieces to start defining data structures for our
interpreter, so that is what we shall do next!</p>
<hr />
<div class="footnote-definition" id="1"><sup class="footnote-definition-label">1</sup>
<p>There are other pointer tagging schemes, notably the use of &quot;spare&quot; NaN
bit patterns in 64 bit floating point values. Further, <em>which</em> types are
best represented by the tag bits is highly language dependent. Some languages
use them for garbage collection information while others may use them for
still other types hidden from the language user. In the interest of clarity,
we'll stick to a simple scheme.</p>
</div>
<h1><a class="header" href="#symbols-and-pairs" id="symbols-and-pairs">Symbols and Pairs</a></h1>
<p>To bootstrap our compiler, we'll parse s-expressions into <code>Symbol</code> ad <code>Pair</code>
types, where a <code>Pair</code> is essentially a Lisp cons cell.</p>
<p>The definition of <code>Symbol</code> is just the raw components of a <code>&amp;str</code>:</p>
<pre><code class="language-rust ignore">#[derive(Copy, Clone)]
pub struct Symbol {
    name_ptr: *const u8,
    name_len: usize,
}
</code></pre>
<p>Why this is how <code>Symbol</code> is defined and how we handle these raw components will
be covered in just a bit. First though, we'll delve into the <code>Pair</code> type.</p>
<h2><a class="header" href="#pairs-of-pointers" id="pairs-of-pointers">Pairs of pointers</a></h2>
<p>The definition of <code>Pair</code> is</p>
<pre><code class="language-rust ignore">#[derive(Clone)]
pub struct Pair {
    pub first: TaggedCellPtr,
    pub second: TaggedCellPtr,
    // Possible source code positions of the first and second values
    pub first_pos: Cell&lt;Option&lt;SourcePos&gt;&gt;,
    pub second_pos: Cell&lt;Option&lt;SourcePos&gt;&gt;,
}
</code></pre>
<p>The type of <code>first</code> and <code>second</code> is <code>TaggedCellPtr</code>, as seen in the previous
chapter. This pointer type can point at any dynamic type. By the
end of this chapter we'll be able to build a nested linked list of <code>Pair</code>s
and <code>Symbol</code>s.</p>
<p>Since this structure will be used for parsing and compiling, the <code>Pair</code>
<code>struct</code> has a couple of extra members that optionally describe the source
code line and character number of the values pointed at by <code>first</code> and
<code>second</code>. These will be useful for reporting error messages. We'll come back
to these in the chapter on parsing.</p>
<p>To instantiate a <code>Pair</code> function with <code>first</code> and <code>second</code> set to nil, let's
create a <code>new()</code> function:</p>
<pre><code class="language-rust ignore">impl Pair {
    pub fn new() -&gt; Pair {
        Pair {
            first: TaggedCellPtr::new_nil(),
            second: TaggedCellPtr::new_nil(),
            first_pos: Cell::new(None),
            second_pos: Cell::new(None),
        }
    }
}
</code></pre>
<p>That function, as it's not being allocated into the heap, doesn't require the
lifetime guard. Let's look at a more interesting function: <code>cons()</code>, which
assigns a value to <code>first</code> and <code>second</code> and puts the <code>Pair</code> on to the heap:</p>
<pre><code class="language-rust ignore">pub fn cons&lt;'guard&gt;(
    mem: &amp;'guard MutatorView,
    head: TaggedScopedPtr&lt;'guard&gt;,
    rest: TaggedScopedPtr&lt;'guard&gt;,
) -&gt; Result&lt;TaggedScopedPtr&lt;'guard&gt;, RuntimeError&gt; {
    let pair = Pair::new();
    pair.first.set(head);
    pair.second.set(rest);
    mem.alloc_tagged(pair)
}
</code></pre>
<p>Here we have the lifetime <code>'guard</code> associated with the <code>MutatorView</code> instance
which grants access to the allocator <code>alloc_tagged()</code> method and the getter
and setter on <code>TaggedScopedPtr</code>.</p>
<p>The other two args, <code>head</code> and <code>rest</code> are required to share the same <code>'guard</code>
lifetime as the <code>MutatorView</code> instance, or rather, <code>'guard</code> must at least be
a subtype of their lifetimes. Their values, of type <code>TaggedScopedPtr&lt;'guard&gt;</code>,
can be written directly to the <code>first</code> and <code>second</code> members of <code>Pair</code> with
the setter <code>TaggedCellPtr::set()</code>.</p>
<p>We'll also add a couple <code>impl</code> methods for appending an object to a <code>Pair</code>
in linked-list fashion:</p>
<pre><code class="language-rust ignore">impl Pair {
    pub fn append&lt;'guard&gt;(
        &amp;self,
        mem: &amp;'guard MutatorView,
        value: TaggedScopedPtr&lt;'guard&gt;,
    ) -&gt; Result&lt;TaggedScopedPtr&lt;'guard&gt;, RuntimeError&gt; {
        let pair = Pair::new();
        pair.first.set(value);

        let pair = mem.alloc_tagged(pair)?;
        self.second.set(pair);

        Ok(pair)
    }
}
</code></pre>
<p>This method, given a value to append, creates a new <code>Pair</code> whose member <code>first</code>
points at the value, then sets the <code>second</code> of the <code>&amp;self</code> <code>Pair</code> to that new
<code>Pair</code> instance. This is in support of s-expression notation <code>(a b)</code> which
describes a linked-list of <code>Pair</code>s arranged, in pseudo-Rust:</p>
<pre><code>Pair {
    first: a,
    second: Pair {
        first: b,
        second: nil,
    },
}
</code></pre>
<p>The second method is for directly setting the value of the <code>second</code> for
s-expression dot-notation style: <code>(a . b)</code> is represented by <code>first</code> pointing
at <code>a</code>, dotted with <code>b</code> which is pointed at by <code>second</code>. In our pseudo
representation:</p>
<pre><code>Pair {
    first: a,
    second: b,
}
</code></pre>
<p>The implementation is simply:</p>
<pre><code class="language-rust ignore">impl Pair {
    pub fn dot&lt;'guard&gt;(&amp;self, value: TaggedScopedPtr&lt;'guard&gt;) {
        self.second.set(value);
    }
}
</code></pre>
<p>The only other piece to add, since <code>Pair</code> must be able to be passed into
our allocator API, is the <code>AllocObject</code> impl for <code>Pair</code>:</p>
<pre><code class="language-rust ignore">impl AllocObject&lt;TypeList&gt; for Pair {
    const TYPE_ID: TypeList = TypeList::Pair;
}
</code></pre>
<p>This impl pattern will repeat for every type in <code>TypeList</code> so it'll be a great
candidate for a macro.</p>
<p>And that's it! We have a cons-cell style <code>Pair</code> type and some elementary
methods for creating and allocating them.</p>
<p>Now, back to <code>Symbol</code>, which seems like it should be even simpler, but as we'll
see has some nuance to it.</p>
<h2><a class="header" href="#symbols-and-pointers" id="symbols-and-pointers">Symbols and pointers</a></h2>
<p>Let's recap the definition of <code>Symbol</code> and that it is the raw members of a
<code>&amp;str</code>:</p>
<pre><code class="language-rust ignore">#[derive(Copy, Clone)]
pub struct Symbol {
    name_ptr: *const u8,
    name_len: usize,
}
</code></pre>
<p>By this definition, a symbol has a name string, but does not own the string
itself. What means this?</p>
<p>Symbols are in fact pointers to interned strings. Since each symbol points
to a unique string, we can identify a symbol by it's pointer value rather than
needing to look up the string itself.</p>
<p>However, symbols do need to be discovered by their string name, and symbol
pointers must dereference to return their string form. i.e. a we need a
bidirectional mapping of string to pointer and pointer to string.</p>
<p>In our implementation, we use a <code>HashMap&lt;String, RawPtr&lt;Symbol&gt;&gt;</code> to map from
name strings to symbol pointers, while the <code>Symbol</code> object itself points back
to the name string.</p>
<p>This is encapsulated in a <code>SymbolMap</code> struct:</p>
<pre><code class="language-rust ignore">pub struct SymbolMap {
    map: RefCell&lt;HashMap&lt;String, RawPtr&lt;Symbol&gt;&gt;&gt;,
    arena: Arena,
}
</code></pre>
<p>where we use <code>RefCell</code> to wrap operations in interior mutability, just like
all other allocator functionality.</p>
<p>The second struct member <code>Arena</code> requires further explanation: since symbols are
unique strings that can be identified and compared by their pointer values,
these pointer values must remain static throughout the program lifetime.
Thus, <code>Symbol</code> objects cannot be managed by a heap that might perform object
relocation. We need a separate heap type for objects that are never
moved or freed unil the program ends, the <code>Arena</code> type.</p>
<p>The <code>Arena</code> type is simple. It, like <code>Heap</code>, wraps <code>StickyImmixHeap</code> but
unlike <code>Heap</code>, it will never run garbage collection.</p>
<pre><code class="language-rust ignore">pub struct Arena {
    heap: StickyImmixHeap&lt;ArenaHeader&gt;,
}
</code></pre>
<p>The <code>ArenaHeader</code> is a simple object header type to fulfill the allocator
API requirements but whose methods will never be needed.</p>
<p>Allocating a <code>Symbol</code> will use the <code>Arena::alloc()</code> method which calls through
to the <code>StickyImmixHeap</code> instance.</p>
<p>We'll add a method for getting a <code>Symbol</code> from it's name string to the
<code>SymbolMap</code> at the allocator API level:</p>
<pre><code class="language-rust ignore">impl SymbolMap {
    pub fn lookup(&amp;self, name: &amp;str) -&gt; RawPtr&lt;Symbol&gt; {
        {
            if let Some(ptr) = self.map.borrow().get(name) {
                return *ptr;
            }
        }

        let name = String::from(name);
        let ptr = self.arena.alloc(Symbol::new(&amp;name)).unwrap();
        self.map.borrow_mut().insert(name, ptr);
        ptr
    }
}
</code></pre>
<p>Then we'll add wrappers to the <code>Heap</code> and <code>MutatorView</code> impls to scope-restrict
access:</p>
<pre><code class="language-rust ignore">impl Heap {
    fn lookup_sym(&amp;self, name: &amp;str) -&gt; TaggedPtr {
        TaggedPtr::symbol(self.syms.lookup(name))
    }
}
</code></pre>
<p>and</p>
<pre><code class="language-rust ignore">impl&lt;'memory&gt; MutatorView&lt;'memory&gt; {
    pub fn lookup_sym(&amp;self, name: &amp;str) -&gt; TaggedScopedPtr&lt;'_&gt; {
        TaggedScopedPtr::new(self, self.heap.lookup_sym(name))
    }
}
</code></pre>
<p>This scope restriction is absolutely necessary, despite these objects never
being freed or moved during runtime. This is because <code>Symbol</code>, as a standalone
struct, remains unsafe to use with it's raw <code>&amp;str</code> components. These components
can only safely be accessed when there is a guarantee that the backing
<code>Hashmap</code> is still in existence, which is only when the <code>MutatorView</code> is
accessible.</p>
<p>Two methods on <code>Symbol</code> guard access to the <code>&amp;str</code>, one unsafe to reassemble
the <code>&amp;str</code> from raw components, the other safe when given a <code>MutatorScope</code>
guard instance.</p>
<pre><code class="language-rust ignore">impl Symbol {
    pub unsafe fn unguarded_as_str&lt;'desired_lifetime&gt;(&amp;self) -&gt; &amp;'desired_lifetime str {
        let slice = slice::from_raw_parts(self.name_ptr, self.name_len);
        str::from_utf8(slice).unwrap()
    }

    pub fn as_str&lt;'guard&gt;(&amp;self, _guard: &amp;'guard dyn MutatorScope) -&gt; &amp;'guard str {
        unsafe { self.unguarded_as_str() }
    }
}
</code></pre>
<p>Finally, to make <code>Symbol</code>s allocatable in the Sticky Immix heap, we need to
implement <code>AllocObject</code> for it:</p>
<pre><code class="language-rust ignore">impl AllocObject&lt;TypeList&gt; for Symbol {
    const TYPE_ID: TypeList = TypeList::Symbol;
}
</code></pre>
<h2><a class="header" href="#moving-on-swiftly" id="moving-on-swiftly">Moving on swiftly</a></h2>
<p>Now we've got the elemental pieces of s-expressions, lists and symbols, we can
move on to parsing s-expression strings.</p>
<p>Since the focus of this book is the underlying mechanisms of memory management
in Rust and the details of runtime implementation, parsing will receive less
attention. We'll make it quick!</p>
<h1><a class="header" href="#parsing-s-expressions" id="parsing-s-expressions">Parsing s-expressions</a></h1>
<p>We'll make this quick. It's not the main focus of this book and the topic is
better served by seeking out other resources that can do it justice.</p>
<p>In service of keeping it short, we're parsing s-expressions and we'll start
by considering only symbols and parentheses. We could hardly make it simpler.</p>
<h2><a class="header" href="#the-interface" id="the-interface">The interface</a></h2>
<p>The interface we want should take a <code>&amp;str</code> and return a <code>TaggedScopedPtr</code>.
We want the tagged version of the scoped ptr because the return value might
point to either a <code>Pair</code> or a <code>Symbol</code>. Examples of valid input are:</p>
<ul>
<li><code>a-symbol</code>: a <code>Symbol</code> with name &quot;a-symbol&quot;</li>
<li><code>(this is a list)</code>: a linked list of <code>Pair</code>s, each with the <code>first</code> value
pointing to a <code>Symbol</code></li>
<li><code>(this (is a nested) list)</code>: a linked list, as above, containing a nested
linked list</li>
<li><code>(this () is a nil symbol)</code>: the two characters <code>()</code> together are equivalent
to the special symbol <code>nil</code>, also the value <code>0</code> in our <code>TaggedPtr</code> type</li>
<li><code>(one . pair)</code>: a single <code>Pair</code> instance with <code>first</code> pointing at the <code>Symbol</code>
for &quot;one&quot; and <code>second</code> at the <code>Symbol</code> for &quot;two&quot;</li>
</ul>
<p>Our internal implementation is split into tokenizing and then parsing the
token stream. Tokenizing takes the <code>&amp;str</code> input and returns a <code>Vec&lt;Token&gt;</code>
on success:</p>
<pre><code class="language-rust ignore">fn tokenize(input: &amp;str) -&gt; Result&lt;Vec&lt;Token&gt;, RuntimeError&gt;;
</code></pre>
<p>The return <code>Vec&lt;Token&gt;</code> is an intermediate, throwaway value, and does not
interact with our Sticky Immix heap. Parsing takes the <code>Vec&lt;Token&gt;</code> and
returns a <code>TaggedScopedPtr</code> on success:</p>
<pre><code class="language-rust ignore">fn parse_tokens&lt;'guard&gt;(
    mem: &amp;'guard MutatorView,
    tokens: Vec&lt;Token&gt;,
) -&gt; Result&lt;TaggedScopedPtr&lt;'guard&gt;, RuntimeError&gt;;
</code></pre>
<h2><a class="header" href="#tokens-a-short-description" id="tokens-a-short-description">Tokens, a short description</a></h2>
<p>The full set of tokens we will consider parsing is:</p>
<pre><code class="language-rust ignore">#[derive(Debug, PartialEq)]
pub enum TokenType {
    OpenParen,
    CloseParen,
    Symbol(String),
    Dot,
    Text(String),
    Quote,
}
</code></pre>
<p>We combine this enum with a source input position indicator to compose the
<code>Token</code> type. This source position is defined as:</p>
<pre><code class="language-rust ignore">#[derive(Copy, Clone, Debug, PartialEq)]
pub struct SourcePos {
    pub line: u32,
    pub column: u32,
}
</code></pre>
<p>And whenever it is available to return as part of an error, error messages can
be printed with the relevant source code line.</p>
<p>The <code>Token</code> type;</p>
<pre><code class="language-rust ignore">#[derive(Debug, PartialEq)]
pub struct Token {
    pub pos: SourcePos,
    pub token: TokenType,
}
</code></pre>
<h2><a class="header" href="#parsing-a-short-description" id="parsing-a-short-description">Parsing, a short description</a></h2>
<p>The key to quickly writing a parser in Rust is the <code>std::iter::Peekable</code>
iterator which can be obtained from the <code>Vec&lt;Token&gt;</code> instance with
<code>tokens.iter().peekable()</code>. This iterator has a <code>peek()</code> method that allows
you to look at the next <code>Token</code> instance without advancing the iterator.</p>
<p>Our parser, a hand-written recursive descent parser, uses this iterator type
to look ahead to the next token to identify primarily whether the next token
is valid in combination with the current token, or to know how to recurse
next without consuming the token yet.</p>
<p>For example, an open paren <code>(</code> followed by a symbol would start a new <code>Pair</code>
linked list, recursing into a new parser function call, but if it is
immediately followed by a close paren <code>)</code>, that is <code>()</code>, it is equivalent to
the symbol <code>nil</code>, while otherwise <code>)</code> <em>terminates</em> a <code>Pair</code> linked list and
causes the current parsing function instance to return.</p>
<p>Another case is the <code>.</code> operator, which is only valid in the following pattern:
<code>(a b c . d)</code> where <code>a</code>, <code>b</code>, <code>c</code>, and <code>d</code> must be symbols or nested lists.
A <code>.</code> must be followed by a single expression followed by a <code>)</code>.</p>
<p>Tokenizing and parsing are wrapped in a function that takes the input <code>&amp;str</code>
and gives back the <code>TaggedScopedPtr</code>:</p>
<pre><code class="language-rust ignore">pub fn parse&lt;'guard&gt;(
    mem: &amp;'guard MutatorView,
    input: &amp;str,
) -&gt; Result&lt;TaggedScopedPtr&lt;'guard&gt;, RuntimeError&gt; {
    parse_tokens(mem, tokenize(input)?)
}
</code></pre>
<p>Notice that this function and <code>parse_tokens()</code> require the
<code>mem: &amp;'guard MutatorView</code> parameter. Parsing creates <code>Symbol</code> and <code>Pair</code>
instances in our Sticky Immix heap and so requires the scope-restricted
<code>MutatorView</code> instance.</p>
<p>This is all we'll say on parsing s-expressions. In the next chapter we'll do
something altogether more informative with regards to memory management
and it'll be necessary by the time we're ready to compile: arrays!</p>
<h1><a class="header" href="#arrays" id="arrays">Arrays</a></h1>
<p>Before we get to the basics of compilation, we need another data structure:
the humble array. The first use for arrays will be to store the bytecode
sequences that the compiler generates.</p>
<p>Rust already provides <code>Vec</code> but as we're implementing everything in terms of our
memory management abstraction, we cannot directly use <code>Vec</code>. Rust does not
(yet) expose the ability to specify a custom allocator type as part of <code>Vec</code>,
nor are we interested in replacing the global allocator.</p>
<p>Our only option is to write our own version of <code>Vec</code>! Fortunately we can
learn a lot from <code>Vec</code> itself and it's underlying implementation. Jump over to
the <a href="https://doc.rust-lang.org/nomicon/vec.html">Rustonomicon</a> for a primer on the internals of <code>Vec</code>.</p>
<p>The first thing we'll learn is to split the implementation into a <code>RawArray&lt;T&gt;</code>
type and an <code>Array&lt;T&gt;</code> type. <code>RawArray&lt;T&gt;</code> will provide an unsafe abstraction
while <code>Array&lt;T&gt;</code> will make a safe layer over it.</p>
<h2><a class="header" href="#rawarray" id="rawarray">RawArray</a></h2>
<p>If you've just come back from <em>Implementing Vec</em> in the Nomicon, you'll
recognize what we're doing below with <code>RawArray&lt;T&gt;</code>:</p>
<pre><code class="language-rust ignore">pub struct RawArray&lt;T: Sized&gt; {
    /// Count of T-sized objects that can fit in the array
    capacity: ArraySize,
    ptr: Option&lt;NonNull&lt;T&gt;&gt;,
}
</code></pre>
<p>Instead of <code>Unique&lt;T&gt;</code> for the pointer, we're using <code>Option&lt;NonNull&lt;T&gt;&gt;</code>.
One simple reason is that <code>Unique&lt;T&gt;</code> is likely to be permanently unstable and
only available internally to <code>std</code> collections. The other is that we can
avoid allocating the backing store if no capacity is requested yet, setting
the value of <code>ptr</code> to <code>None</code>.</p>
<p>For when we <em>do</em> know the desired capacity, there is
<code>RawArray&lt;T&gt;::with_capacity()</code>. This method, because it allocates, requires
access to the <code>MutatorView</code> instance. If you'll recall from the chapter on
the allocation API, the API provides an array allocation method with
signature:</p>
<pre><code class="language-rust ignore">AllocRaw::alloc_array(&amp;self, size_bytes: ArraySize) -&gt; Result&lt;RawPtr&lt;u8&gt;, AllocError&gt;;
</code></pre>
<p>This method is wrapped on the interpreter side by <code>Heap</code> and <code>MutatorView</code> and
in both cases the return value remains, simply, <code>RawPtr&lt;u8&gt;</code> in the success
case. It's up to <code>RawArray&lt;T&gt;</code> to receive the <code>RawPtr&lt;u8&gt;</code> value and maintain
it safely. Here's <code>with_capcity()</code>, now:</p>
<pre><code class="language-rust ignore">    pub fn with_capacity&lt;'scope&gt;(
        mem: &amp;'scope MutatorView,
        capacity: u32,
    ) -&gt; Result&lt;RawArray&lt;T&gt;, RuntimeError&gt; {
        // convert to bytes, checking for possible overflow of ArraySize limit
        let capacity_bytes = capacity
            .checked_mul(size_of::&lt;T&gt;() as ArraySize)
            .ok_or(RuntimeError::new(ErrorKind::BadAllocationRequest))?;

        Ok(RawArray {
            capacity,
            ptr: NonNull::new(mem.alloc_array(capacity_bytes)?.as_ptr() as *mut T),
        })
    }
</code></pre>
<h3><a class="header" href="#resizing" id="resizing">Resizing</a></h3>
<p>If a <code>RawArray&lt;T&gt;</code>'s content will exceed it's capacity, there is
<code>RawArray&lt;T&gt;::resize()</code>. It allocates a new backing array using the
<code>MutatorView</code> method <code>alloc_array()</code> and copies the content of the old
over to the new, finally swapping in the new backing array for the old.</p>
<p>The code for this is straightforward but a little longer, go check it out
in <code>interpreter/src/rawarray.rs</code>.</p>
<h3><a class="header" href="#accessing" id="accessing">Accessing</a></h3>
<p>Since <code>RawArray&lt;T&gt;</code> will be wrapped by <code>Array&lt;T&gt;</code>, we need a couple more
methods to access the raw memory:</p>
<pre><code class="language-rust ignore">impl&lt;T: Sized&gt; RawArray&lt;T&gt; {
    pub fn capacity(&amp;self) -&gt; ArraySize {
        self.capacity
    }

    pub fn as_ptr(&amp;self) -&gt; Option&lt;*const T&gt; {
        match self.ptr {
            Some(ptr) =&gt; Some(ptr.as_ptr()),
            None =&gt; None,
        }
    }
}
</code></pre>
<p>And that's it! Now for the safe wrapper.</p>
<h2><a class="header" href="#array" id="array">Array</a></h2>
<p>The definition of the struct wrapping <code>RawArray&lt;T&gt;</code> is as follows:</p>
<pre><code class="language-rust ignore">#[derive(Clone)]
pub struct Array&lt;T: Sized + Clone&gt; {
    length: Cell&lt;ArraySize&gt;,
    data: Cell&lt;RawArray&lt;T&gt;&gt;,
    borrow: Cell&lt;BorrowFlag&gt;,
}
</code></pre>
<p>Here we have three members:</p>
<ul>
<li><code>length</code> - the length of the array</li>
<li><code>data</code> - the <code>RawArray&lt;T&gt;</code> being wrapped</li>
<li><code>borrow</code> - a flag serving as a runtime borrow check, allowing <code>RefCell</code>
runtime semantics, since we're in a world of interior mutability patterns</li>
</ul>
<p>We have a method to create a new array - <code>Array::alloc()</code></p>
<pre><code class="language-rust ignore">impl&lt;T: Sized + Clone&gt; Array&lt;T&gt; {
    pub fn alloc&lt;'guard&gt;(
        mem: &amp;'guard MutatorView,
    ) -&gt; Result&lt;ScopedPtr&lt;'guard, Array&lt;T&gt;&gt;, RuntimeError&gt;
    where
        Array&lt;T&gt;: AllocObject&lt;TypeList&gt;,
    {
        mem.alloc(Array::new())
    }
}
</code></pre>
<p>In fact we'll extend this pattern of a method named &quot;alloc&quot; to any data
structure for convenience sake.</p>
<p>There are many more methods for <code>Array&lt;T&gt;</code> and it would be exhausting to be
exhaustive. Let's go over the core methods used to read and write elements
and then an example use case.</p>
<h3><a class="header" href="#reading-and-writing" id="reading-and-writing">Reading and writing</a></h3>
<p>First of all, we need a function that takes an array index and returns a
pointer to a memory location, if the index is within bounds:</p>
<pre><code class="language-rust ignore">impl&lt;T: Sized + Clone&gt; Array&lt;T&gt; {
    fn get_offset(&amp;self, index: ArraySize) -&gt; Result&lt;*mut T, RuntimeError&gt; {
        if index &gt;= self.length.get() {
            Err(RuntimeError::new(ErrorKind::BoundsError))
        } else {
            let ptr = self
                .data
                .get()
                .as_ptr()
                .ok_or_else(|| RuntimeError::new(ErrorKind::BoundsError))?;

            let dest_ptr = unsafe { ptr.offset(index as isize) as *mut T };

            Ok(dest_ptr)
        }
    }
}
</code></pre>
<p>There are two bounds checks here - firstly, the index should be within the
(likely non-zero) length values; secondly, the <code>RawArray&lt;T&gt;</code> instance
should have a backing array allocated. If either of these checks fail, the
result is an error. If these checks pass, we can be confident that there
is array backing memory and that we can return a valid pointer to somewhere
inside that memory block.</p>
<p>For reading a value in an array, we need two methods:</p>
<ol>
<li>one that handles move/copy semantics and returns a value</li>
<li>one that handles reference semantics and returns a reference to the original
value in it's location in the backing memory</li>
</ol>
<p>First, then:</p>
<pre><code class="language-rust ignore">impl&lt;T: Sized + Clone&gt; Array&lt;T&gt; {
    fn read&lt;'guard&gt;(
        &amp;self,
        _guard: &amp;'guard dyn MutatorScope,
        index: ArraySize,
    ) -&gt; Result&lt;T, RuntimeError&gt; {
        unsafe {
            let dest = self.get_offset(index)?;
            Ok(read(dest))
        }
    }
}
</code></pre>
<p>and secondly:</p>
<pre><code class="language-rust ignore">impl&lt;T: Sized + Clone&gt; Array&lt;T&gt; {
    pub fn read_ref&lt;'guard&gt;(
        &amp;self,
        _guard: &amp;'guard dyn MutatorScope,
        index: ArraySize,
    ) -&gt; Result&lt;&amp;T, RuntimeError&gt; {
        unsafe {
            let dest = self.get_offset(index)?;
            Ok(&amp;*dest as &amp;T)
        }
    }
}
</code></pre>
<p>Writing, or copying, an object to an array is implemented as simply as follows:</p>
<pre><code class="language-rust ignore">impl&lt;T: Sized + Clone&gt; Array&lt;T&gt; {
    pub fn read_ref&lt;'guard&gt;(
        &amp;self,
        _guard: &amp;'guard dyn MutatorScope,
        index: ArraySize,
    ) -&gt; Result&lt;&amp;T, RuntimeError&gt; {
        unsafe {
            let dest = self.get_offset(index)?;
            Ok(&amp;*dest as &amp;T)
        }
    }
}
</code></pre>
<p>These simple functions should only be used internally by <code>Array&lt;T&gt;</code> impl
methods. We have numerous methods that wrap the above in more appropriate
semantics for values of <code>T</code> in <code>Array&lt;T&gt;</code>.</p>
<h3><a class="header" href="#the-array-interfaces" id="the-array-interfaces">The Array interfaces</a></h3>
<p>To define the interfaces to the Array, and other collection types, we define a
number of traits. For example, a collection that behaves as a stack implements
<code>StackContainer</code>; a numerically indexable type implements <code>IndexedContainer</code>,
and so on. As we'll see, there is some nuance, though, when it comes to a
difference between collections of non-pointer types and collections of pointer
types.</p>
<p>For our example, we will describe the stack interfaces of <code>Array&lt;T&gt;</code>.</p>
<p>First, the general case trait, with methods for accessing values stored in the
array (non-pointer types):</p>
<pre><code class="language-rust ignore">pub trait StackContainer&lt;T: Sized + Clone&gt;: Container&lt;T&gt; {
    /// Push can trigger an underlying array resize, hence it requires the ability to allocate
    fn push&lt;'guard&gt;(&amp;self, mem: &amp;'guard MutatorView, item: T) -&gt; Result&lt;(), RuntimeError&gt;;

    /// Pop returns a bounds error if the container is empty, otherwise moves the last item of the
    /// array out to the caller.
    fn pop&lt;'guard&gt;(&amp;self, _guard: &amp;'guard dyn MutatorScope) -&gt; Result&lt;T, RuntimeError&gt;;

    /// Return the value at the top of the stack without removing it
    fn top&lt;'guard&gt;(&amp;self, _guard: &amp;'guard dyn MutatorScope) -&gt; Result&lt;T, RuntimeError&gt;;
}
</code></pre>
<p>These are unremarkable functions, by now we're familiar with the references to
<code>MutatorScope</code> and <code>MutatorView</code> in method parameter lists.</p>
<p>In any instance of <code>Array&lt;T&gt;</code>, <code>T</code> need only implement <code>Clone</code> and cannot be
dynamically sized. Thus <code>T</code> can be any primitive type or any straightforward
struct.</p>
<p>What if we want to store pointers to other objects? For example, if we want a
heterogenous array, such as Python's <code>List</code> type, what would we provide in
place of <code>T</code>? The answer is to use the <code>TaggedCellPtr</code> type. However,
an <code>Array&lt;TaggedCellPtr</code>, because we want to interface with pointers and
use the memory access abstractions provided, can be made a little more
ergonomic. For that reason, we have separate traits for containers of type
<code>Container&lt;TaggedCellPtr</code>. In the case of the stack interface this looks like:</p>
<pre><code class="language-rust ignore">pub trait StackAnyContainer: StackContainer&lt;TaggedCellPtr&gt; {
    /// Push can trigger an underlying array resize, hence it requires the ability to allocate
    fn push&lt;'guard&gt;(
        &amp;self,
        mem: &amp;'guard MutatorView,
        item: TaggedScopedPtr&lt;'guard&gt;,
    ) -&gt; Result&lt;(), RuntimeError&gt;;

    /// Pop returns a bounds error if the container is empty, otherwise moves the last item of the
    /// array out to the caller.
    fn pop&lt;'guard&gt;(
        &amp;self,
        _guard: &amp;'guard dyn MutatorScope,
    ) -&gt; Result&lt;TaggedScopedPtr&lt;'guard&gt;, RuntimeError&gt;;

    /// Return the value at the top of the stack without removing it
    fn top&lt;'guard&gt;(
        &amp;self,
        _guard: &amp;'guard dyn MutatorScope,
    ) -&gt; Result&lt;TaggedScopedPtr&lt;'guard&gt;, RuntimeError&gt;;
}
</code></pre>
<p>As you can see, these methods, while for <code>T = TaggedCellPtr</code>, provide an
interface based on passing and returning <code>TaggedScopedPtr</code>.</p>
<p>Let's look at the implementation of one of these methods - <code>push()</code>  - for
both <code>StackContainer</code> and <code>StackAnyContainer</code>.</p>
<p>Here's the code for <code>StackContainer::push()</code>:</p>
<pre><code class="language-rust ignore">impl&lt;T: Sized + Clone&gt; StackContainer&lt;T&gt; for Array&lt;T&gt; {
    fn push&lt;'guard&gt;(&amp;self, mem: &amp;'guard MutatorView, item: T) -&gt; Result&lt;(), RuntimeError&gt; {
        if self.borrow.get() != INTERIOR_ONLY {
            return Err(RuntimeError::new(ErrorKind::MutableBorrowError));
        }

        let length = self.length.get();
        let mut array = self.data.get(); // Takes a copy

        let capacity = array.capacity();

        if length == capacity {
            if capacity == 0 {
                array.resize(mem, DEFAULT_ARRAY_SIZE)?;
            } else {
                array.resize(mem, default_array_growth(capacity)?)?;
            }
            // Replace the struct's copy with the resized RawArray object
            self.data.set(array);
        }

        self.length.set(length + 1);
        self.write(mem, length, item)?;
        Ok(())
    }
}
</code></pre>
<p>In summary, the order of operations is:</p>
<ol>
<li>Check that a runtime borrow isn't in progress. If it is, return an error.</li>
<li>Since we must implement interior mutability, the member <code>data</code> of the
<code>Array&lt;T&gt;</code> struct is a <code>Cell</code>. We have to <code>get()</code> the content in order
to use it.</li>
<li>We then ask whether the array backing store needs to be grown. If so,
we resize the <code>RawArray&lt;T&gt;</code> and, since it's kept in a <code>Cell</code> on <code>Array&lt;T&gt;</code>,
we have to <code>set()</code> value back into <code>data</code> to save the change.</li>
<li>Now we have an <code>RawArray&lt;T&gt;</code> that has enough capacity, the length is
incremented and the object to be pushed is written to the next memory
location using the internal <code>Array&lt;T&gt;::write()</code> method detailed earlier.</li>
</ol>
<p>Fortunately we can implement <code>StackAnyContainer::push()</code> in terms of
<code>StackContainer::push()</code>:</p>
<pre><code class="language-rust ignore">impl StackAnyContainer for Array&lt;TaggedCellPtr&gt; {
    fn push&lt;'guard&gt;(
        &amp;self,
        mem: &amp;'guard MutatorView,
        item: TaggedScopedPtr&lt;'guard&gt;,
    ) -&gt; Result&lt;(), RuntimeError&gt; {
        StackContainer::&lt;TaggedCellPtr&gt;::push(self, mem, TaggedCellPtr::new_with(item))
    }
}
</code></pre>
<h3><a class="header" href="#one-last-thing" id="one-last-thing">One last thing</a></h3>
<p>To more easily differentiate arrays of type <code>Array&lt;T&gt;</code> from arrays of type
<code>Array&lt;TaggedCellPtr&gt;</code>, we make a type alias <code>List</code> where:</p>
<pre><code class="language-rust ignore">pub type List = Array&lt;TaggedCellPtr&gt;;
</code></pre>
<h2><a class="header" href="#in-conclusion" id="in-conclusion">In conclusion</a></h2>
<p>We referenced how <code>Vec</code> is implemented internally and followed the same pattern
of defining a <code>RawArray&lt;T&gt;</code> unsafe layer with a safe <code>Array&lt;T&gt;</code> wrapper. Then
we looked into the stack interface for <code>Array&lt;T&gt;</code> and the implementation of
<code>push()</code>.</p>
<p>There is more to arrays, of course - indexed access the most obvious, and also
a few convenience methods. See the source code in <code>interpreter/src/array.rs</code>
for the full detail.</p>
<p>In the next chapter we'll put <code>Array&lt;T&gt;</code> to use in a <code>Bytecode</code> type!</p>
<h1><a class="header" href="#bytecode" id="bytecode">Bytecode</a></h1>
<p>In this chapter we will look at a bytecode compilation target. We'll combine
this with a section on the virtual machine interface to the bytecode data
structure.</p>
<p>We won't go much into detail on each bytecode operation, that will be more
usefully covered in the compiler and virtual machine chapters. Here, we'll
describe the data structures involved. As such, this will be one of our
shorter chapters. Let's go!</p>
<h2><a class="header" href="#design-questions" id="design-questions">Design questions</a></h2>
<p>Now that we're talking bytecode, we're at the point of choosing what type of
virtual machine we will be compiling for. The most common type is stack-based
where operands are pushed and popped on and off the stack. This requires
instructions for pushing and popping, with instructions in-between for operating
on values on the stack.</p>
<p>We'll be implementing a register-based VM though. The inspiration for this
comes from Lua 5<sup class="footnote-reference"><a href="#1">1</a></sup> which implements a fixed-width bytecode register VM. While
stack based VMs are typically claimed to be simpler, we'll see that the Lua
way of allocating registers per function also has an inherent simplicity and
has performance gains over a stack VM, at least for an interpreted
non jit-compiled VM.</p>
<p>Given register based, fixed-width bytecode, each opcode must reference the
register numbers that it operates on. Thus, for an (untyped) addition
operation <code>x = a + b</code>, each of <code>x</code>, <code>a</code> and <code>b</code> must be associated with a
register.</p>
<p>Following Lua, encoding this as a fixed width opcode typically looks like
encoding the operator and operands as 8 bit values packed into a 32 bit opcode
word. That implies, given 8 bits, that there can be a theoretical maximum of
256 registers for a function call. For the addition above, this encoding
might look like this:</p>
<pre><code class="language-ignore">   32.....24......16.......8.......0
    [reg a ][reg b ][reg x ][Add   ]
</code></pre>
<p>where the first 8 bits contain the operator, in this case &quot;Add&quot;, and the
other three 8 bit slots in the 32 bit word each contain a register number.</p>
<p>For some operators, we will need to encode values larger than 8 bits. As
we will still need space for an operator and a destination register, that
leaves a maximum of 16 bits for larger values.</p>
<h2><a class="header" href="#opcodes" id="opcodes">Opcodes</a></h2>
<p>We have options in how we describe opcodes in Rust.</p>
<ol>
<li>Each opcode represented by a u32
<ul>
<li>Pros: encoding flexibility, it's just a set of bits</li>
<li>Cons: bit shift and masking operations to encode and decode operator
and operands. This isn't necessarily a big deal but it doesn't allow
us to leverage the Rust type system to avoid encoding mistakes</li>
</ul>
</li>
<li>Each opcode represented by an enum discriminant
<ul>
<li>Pros: operators and operands baked as Rust types at compile time, type
safe encoding; no bit operations needed</li>
<li>Cons: encoding scheme limited to what an enum can represent</li>
</ul>
</li>
</ol>
<p>The ability to leverage the compiler to prevent opcode encoding errors is
attractive and we won't have any need for complex encodings. We'll use an enum
to represent all possible opcodes and their operands.</p>
<p>Since a Rust enum can contain named values within each variant, this is what
we use to most tightly define our opcodes.</p>
<h3><a class="header" href="#opcode-size" id="opcode-size">Opcode size</a></h3>
<p>Since we're using <code>enum</code> instead of a directly size-controlled type such as u32
for our opcodes, we have to be more careful about making sure our opcode type
doesn't take up more space than is necessary.  32 bits is ideal for reasons
stated earlier (8 bits for the operator and 8 bits for three operands each.)</p>
<p>Let's do an experiment.</p>
<p>First, we need to define a register as an 8 bit value. We'll also define an
inline literal integer as 16 bits.</p>
<pre><code class="language-rust ignore">type Register = u8;
type LiteralInteger = i16;
</code></pre>
<p>Then we'll create an opcode enum with a few variants that might be typical:</p>
<pre><code class="language-rust ignore">#[derive(Copy, Clone)]
enum Opcode {
    Add {
        dest: Register,
        a: Register,
        b: Register
    },
    LoadLiteral {
        dest: Register,
        value: LiteralInteger
    }
}
</code></pre>
<p>It should be obvious that with an enum like this we can safely pass compiled
bytecode from the compiler to the VM. It should also be clear that this, by
allowing use of <code>match</code> statements, will be very ergonomic to work with.</p>
<p>Theoretically, if we never have more than 256 variants, our variants never have
more than 3 <code>Register</code> values (or one <code>Register</code> and one <code>LiteralInteger</code> sized
value), the compiler should be able to pack <code>Opcode</code> into 32 bits.</p>
<p>Our test: we hope the output of the following code to be <code>4</code> - 4 bytes or 32
bits.</p>
<pre><code class="language-rust ignore">use std::mem::size_of;

fn main() {
    println!(&quot;Size of Opcode is {}&quot;, size_of::&lt;Opcode&gt;());
}
</code></pre>
<p>And indeed when we run this, we get <code>Size of Opcode is 4</code>!</p>
<p>To keep an eye on this situation, we'll put this check into a unit test:</p>
<pre><code class="language-rust ignore">    #[test]
    fn test_opcode_is_32_bits() {
        // An Opcode should be 32 bits; anything bigger and we've mis-defined some
        // variant
        assert!(size_of::&lt;Opcode&gt;() == 4);
    }
</code></pre>
<p>Now, let's put these <code>Opcode</code>s into an array.</p>
<h2><a class="header" href="#an-array-of-opcode" id="an-array-of-opcode">An array of Opcode</a></h2>
<p>We can define this array easily, given that <code>Array&lt;T&gt;</code> is a generic type:</p>
<pre><code class="language-rust ignore">pub type ArrayOpcode = Array&lt;Opcode&gt;;
</code></pre>
<p>Is this enough to define bytecode? Not quite. We've accommodated 16 bit
literal signed integers, but all kinds of other types can be literals.
We need some way of referencing any literal type in bytecode. For that
we add a <code>Literals</code> type, which is just:</p>
<pre><code class="language-rust ignore">pub type Literals = List;
</code></pre>
<p>Any opcode that loads a literal (other than a 16 bit signed integer) will
need to reference an object in the <code>Literals</code> list. This is easy enough:
just as there's a <code>LiteralInteger</code>, we have <code>LiteralId</code> defined as</p>
<pre><code class="language-rust ignore">pub type LiteralId = u16;
</code></pre>
<p>This id is an index into the <code>Literals</code> list.  This isn't the most efficient
scheme or encoding, but given a preference for fixed 32 bit opcodes, it will
also keep things simple.</p>
<p>The <code>ByteCode</code> type, finally, is a composition of <code>ArrayOpcode</code> and <code>Literals</code>:</p>
<pre><code class="language-rust ignore">#[derive(Clone)]
pub struct ByteCode {
    code: ArrayOpcode,
    literals: Literals,
}
</code></pre>
<h2><a class="header" href="#bytecode-compiler-support" id="bytecode-compiler-support">Bytecode compiler support</a></h2>
<p>There are a few methods implemented for <code>ByteCode</code>:</p>
<ol>
<li><code>fn push&lt;'guard&gt;(&amp;self, mem: &amp;'MutatorView, op: Opcode) -&gt; Result&lt;(), RuntimeError&gt;</code>
This function pushes a new opcode into the <code>ArrayOpcode</code> instance.</li>
<li>
<pre><code class="language-rust ignore">fn update_jump_offset&lt;'guard&gt;(
    &amp;self,
    mem: &amp;'guard MutatorView,
    instruction: ArraySize,
    offset: JumpOffset,
) -&gt; Result&lt;(), RuntimeError&gt;
</code></pre>
This function, given an instruction index into the <code>ArrayOpcode</code> instance,
and given that the instruction at that index is a type of jump instruction,
sets the relative jump offset of the instruction to the given offset.
This is necessary because forward jumps cannot be calculated until all the
in-between instructions have been compiled first.</li>
<li>
<pre><code class="language-rust ignore">fn push_lit&lt;'guard&gt;(
    &amp;self,
    mem: &amp;'guard MutatorView,
    literal: TaggedScopedPtr
) -&gt; Result&lt;LiteralId, RuntimeError&gt;
</code></pre>
This function pushes a literal on to the <code>Literals</code> list and returns the
index - the id - of the item.</li>
<li>
<pre><code class="language-rust ignore">fn push_loadlit&lt;'guard&gt;(
    &amp;self,
    mem: &amp;'guard MutatorView,
    dest: Register,
    literal_id: LiteralId,
) -&gt; Result&lt;(), RuntimeError&gt;
</code></pre>
After pushing a literal into the <code>Literals</code> list, the corresponding load
instruction should be pushed into the <code>ArrayOpcode</code> list.</li>
</ol>
<p><code>ByteCode</code> and it's functions combined with the <code>Opcode</code> enum are enough to
build a compiler for.</p>
<h2><a class="header" href="#bytecode-execution-support" id="bytecode-execution-support">Bytecode execution support</a></h2>
<p>The previous section described a handful of functions for our compiler to use
to build a <code>ByteCode</code> structure.</p>
<p>We'll need a different set of functions for our virtual machine to access
<code>ByteCode</code> from an execution standpoint.</p>
<p>The execution view of bytecode is of a contiguous sequence of instructions and
an instruction pointer. We're going to create a separate <code>ByteCode</code> instance
for each function that gets compiled, so our execution model will have to
be able to jump between <code>ByteCode</code> instances. We'll need a new struct to
represent that:</p>
<pre><code class="language-rust ignore">pub struct InstructionStream {
    instructions: CellPtr&lt;ByteCode&gt;,
    ip: Cell&lt;ArraySize&gt;,
}
</code></pre>
<p>In this definition, the pointer <code>instructions</code> can be updated to point at any
<code>ByteCode</code> instance. This allows us to switch between functions by managing
different <code>ByteCode</code> pointers as part of a stack of call frames. In support
of this we have:</p>
<pre><code class="language-rust ignore">impl InstructionStream {
    pub fn switch_frame(&amp;self, code: ScopedPtr&lt;'_, ByteCode&gt;, ip: ArraySize) {
        self.instructions.set(code);
        self.ip.set(ip);
    }
}
</code></pre>
<p>Of course, the main function needed during execution is to retrieve the next
opcode. Ideally, we can keep a pointer that points directly at the next opcode
such that only a single dereference and pointer increment is needed to get
the opcode and advance the instruction pointer. Our implementation is less
efficient for now, requiring a dereference of 1. the <code>ByteCode</code> instance and
then 2. the <code>ArrayOpcode</code> instance and finally 3. an indexing into the
<code>ArrayOpcode</code> instance:</p>
<pre><code class="language-rust ignore">    pub fn get_next_opcode&lt;'guard&gt;(
        &amp;self,
        guard: &amp;'guard dyn MutatorScope,
    ) -&gt; Result&lt;Opcode, RuntimeError&gt; {
        let instr = self
            .instructions
            .get(guard)
            .code
            .get(guard, self.ip.get())?;
        self.ip.set(self.ip.get() + 1);
        Ok(instr)
    }
</code></pre>
<h2><a class="header" href="#conclusion-1" id="conclusion-1">Conclusion</a></h2>
<p>The full <code>Opcode</code> definition can be found in <code>interpreter/src/bytecode.rs</code>.</p>
<p>As we work toward implementing a compiler, the next data structure we need is
a dictionary or hash map. This will also build on the foundational
<code>RawArray&lt;T&gt;</code> implementation. Let's go on to that now!</p>
<hr />
<div class="footnote-definition" id="1"><sup class="footnote-definition-label">1</sup>
<p>Roberto Ierusalimschy et al, <a href="https://www.lua.org/doc/jucs05.pdf">The Implementation of Lua 5.0</a></p>
</div>
<h1><a class="header" href="#dicts" id="dicts">Dicts</a></h1>
<p>The implementation of dicts, or hash tables, is going to combine a reuse of the
<a href="./chapter-interp-arrays.html">RawArray</a>
type and closely follow the <a href="http://craftinginterpreters.com/hash-tables.html">Crafting Interpreters</a> design:</p>
<ul>
<li>open addressing</li>
<li>linear probing</li>
<li>FNV hashing</li>
</ul>
<p>Go read the corresponding chapter in Crafting Interpreters and then come
back here. We won't duplicate much of Bob's excellent explanation of the above
terms and we'll assume you are familiar with his chapter when reading
ours.</p>
<h2><a class="header" href="#code-design" id="code-design">Code design</a></h2>
<p>A <code>Dict</code> in our interpreter will allow any hashable value as a key and any
type as a value. We'll store pointers to the key and the value together in
a struct <code>DictItem</code>.</p>
<p>Here, we'll also introduce the single diversion from
Crafting Interpreters' implementation in that we'll cache the hash value and
use it as part of a tombstone indicator. This adds an extra word
per entry but we will also take the stance that if two keys have
the same hash value then the keys are equal. This simplifies our implementation
as we won't need to implement object equality comparisons just yet.</p>
<pre><code class="language-rust ignore">#[derive(Clone)]
pub struct DictItem {
    key: TaggedCellPtr,
    value: TaggedCellPtr,
    hash: u64,
}
</code></pre>
<p>The <code>Dict</code> itself mirrors Crafting Interpreters' implementation of a count of
used entries and an array of entries. Since tombstones are counted as used
entries, we'll add a separate <code>length</code> that excludes tombstones so we can
accurately report the number of items in a dict.</p>
<pre><code class="language-rust ignore">pub struct Dict {
    /// Number of items stored
    length: Cell&lt;ArraySize&gt;,
    /// Total count of items plus tombstones
    used_entries: Cell&lt;ArraySize&gt;,
    /// Backing array for key/value entries
    data: Cell&lt;RawArray&lt;DictItem&gt;&gt;,
}
</code></pre>
<h2><a class="header" href="#hashing" id="hashing">Hashing</a></h2>
<p>To implement our compiler we will need to be able to hash the <code>Symbol</code> type and
integers (inline in tagged pointers.)</p>
<p>The Rust standard library defines trait <code>std::hash::Hash</code> that must be
implemented by types that want to be hashed. This trait requires the type to
implement method <code>fn hash&lt;H&gt;(&amp;self, state: &amp;mut H) where H: Hasher</code>.</p>
<p>This signature requires a reference to the type <code>&amp;self</code> to access it's data.
In our world, this is insufficient: we also require a <code>&amp;MutatorScope</code>
lifetime to access an object. We will have to wrap <code>std::hash::Hash</code> in our
own trait that extends, essentially the same signature, with this scope
guard parameter. This trait is named <code>Hashable</code>:</p>
<pre><code class="language-rust ignore">/// Similar to Hash but for use in a mutator lifetime-limited scope
pub trait Hashable {
    fn hash&lt;'guard, H: Hasher&gt;(&amp;self, _guard: &amp;'guard dyn MutatorScope, hasher: &amp;mut H);
}
</code></pre>
<p>We can implement this trait for <code>Symbol</code> - it's a straightforward wrap of
calling <code>Hash::hash()</code>:</p>
<pre><code class="language-rust ignore">impl Hashable for Symbol {
    fn hash&lt;'guard, H: Hasher&gt;(&amp;self, guard: &amp;'guard dyn MutatorScope, h: &amp;mut H) {
        self.as_str(guard).hash(h)
    }
}
</code></pre>
<p>Then finally, because this is all for a dynamically typed interpreter, we'll
write a function that can take any type - a <code>TaggedScopedPtr</code> - and attempt
to return a 64 bit hash value from it:</p>
<pre><code class="language-rust ignore">fn hash_key&lt;'guard&gt;(
    guard: &amp;'guard dyn MutatorScope,
    key: TaggedScopedPtr&lt;'guard&gt;,
) -&gt; Result&lt;u64, RuntimeError&gt; {
    match *key {
        Value::Symbol(s) =&gt; {
            let mut hasher = FnvHasher::default();
            s.hash(guard, &amp;mut hasher);
            Ok(hasher.finish())
        }
        Value::Number(n) =&gt; Ok(n as u64),
        _ =&gt; Err(RuntimeError::new(ErrorKind::UnhashableError)),
    }
}
</code></pre>
<p>Now we can take a <code>Symbol</code> or a tagged integer and use them as keys in our
<code>Dict</code>.</p>
<h2><a class="header" href="#finding-an-entry" id="finding-an-entry">Finding an entry</a></h2>
<p>The methods that a dictionary typically provides, lookup, insertion and
deletion, all hinge around one internal function, <code>find_entry()</code>.</p>
<p>This function scans the internal <code>RawArray&lt;DictItem&gt;</code> array for a slot that
matches the hash value argument. It may find an exact match for an existing
key-value entry; if it does not, it will return the first available slot for
the hash value, whether an empty never-before used slot or the tombstone
entry of a formerly used slot.</p>
<p>A tombstone, remember, is a slot that previously held a key-value pair but
has been deleted. These slots must be specially marked so that when searching
for an entry that generated a hash for an earlier slot but had to be inserted
at a later slot, we know to keep looking rather than stop searching at the
empty slot of a deleted entry.</p>
<table><thead><tr><th>Slot</th><th>Content</th></tr></thead><tbody>
<tr><td>n - 1</td><td>empty</td></tr>
<tr><td>n</td><td>X: hash % capacity == n</td></tr>
<tr><td>n + 1</td><td>tombstone</td></tr>
<tr><td>n + 2</td><td>Y: hash % capacity == n</td></tr>
<tr><td>n + 3</td><td>empty</td></tr>
</tbody></table>
<p>For example, in the above table:</p>
<ul>
<li>Key <code>X</code>'s hash maps to slot <code>n</code>.</li>
<li>At some point another entry was inserted at slot <code>n + 1</code>.</li>
<li>Then <code>Y</code>, with hash mapping also to slot <code>n</code>, was inserted, but had to be
bumped to slot <code>n + 2</code> because the previous two slots were occupied.</li>
<li>Then the entry at slot <code>n + 1</code> was deleted and marked as a tombstone.</li>
</ul>
<p>If slot <code>n + 1</code> was simply marked as <code>empty</code> after it's occupant was deleted,
then when searching for <code>Y</code> we wouldn't know to keep searching and find <code>Y</code> in
slot <code>n + 2</code>. Hence, deleted entries are marked differently to empty slots.</p>
<p>Here is the code for the Find Entry function:</p>
<pre><code class="language-rust ignore">/// Given a key, generate the hash and search for an entry that either matches this hash
/// or the next available blank entry.
fn find_entry&lt;'guard&gt;(
    _guard: &amp;'guard dyn MutatorScope,
    data: &amp;RawArray&lt;DictItem&gt;,
    hash: u64,
) -&gt; Result&lt;&amp;'guard mut DictItem, RuntimeError&gt; {
    // get raw pointer to base of array
    let ptr = data
        .as_ptr()
        .ok_or(RuntimeError::new(ErrorKind::BoundsError))?;

    // calculate the starting index into `data` to begin scanning at
    let mut index = (hash % data.capacity() as u64) as ArraySize;

    // the first tombstone we find will be saved here
    let mut tombstone: Option&lt;&amp;mut DictItem&gt; = None;

    loop {
        let entry = unsafe { &amp;mut *(ptr.offset(index as isize) as *mut DictItem) as &amp;mut DictItem };

        if entry.hash == TOMBSTONE &amp;&amp; entry.key.is_nil() {
            // this is a tombstone: save the first tombstone reference we find
            if tombstone.is_none() {
                tombstone = Some(entry);
            }
        } else if entry.hash == hash {
            // this is an exact match slot
            return Ok(entry);
        } else if entry.key.is_nil() {
            // this is a non-tombstone empty slot
            if let Some(earlier_entry) = tombstone {
                // if we recorded a tombstone, return _that_ slot to be reused
                return Ok(earlier_entry);
            } else {
                return Ok(entry);
            }
        }

        // increment the index, wrapping back to 0 when we get to the end of the array
        index = (index + 1) % data.capacity();
    }
}
</code></pre>
<p>To begin with, it calculates the index in the array from which to start
searching. Then it iterates over the internal array, examining each entry's
hash and key as it goes.</p>
<ul>
<li>The first tombstone that is encountered is saved. This may turn out to be the
entry that should be returned if an exact hash match isn't found by the time
a never-before used slot is reached. We want to reuse tombstone entries, of
course.</li>
<li>If no tombstone was found and we reach a never-before used slot, return
that slot.</li>
<li>If an exact match is found, return that slot of course.</li>
</ul>
<h2><a class="header" href="#the-external-api" id="the-external-api">The external API</a></h2>
<p>Just as we defined some conainer traits for <code>Array&lt;T&gt;</code> to define access to
arrays based on stack or indexed style access, we'll define a container trait
for <code>Dict</code>:</p>
<pre><code class="language-rust ignore">pub trait HashIndexedAnyContainer {
    /// Return a pointer to to the object associated with the given key.
    /// Absence of an association should return an error.
    fn lookup&lt;'guard&gt;(
        &amp;self,
        guard: &amp;'guard dyn MutatorScope,
        key: TaggedScopedPtr,
    ) -&gt; Result&lt;TaggedScopedPtr&lt;'guard&gt;, RuntimeError&gt;;

    /// Associate a key with a value.
    fn assoc&lt;'guard&gt;(
        &amp;self,
        mem: &amp;'guard MutatorView,
        key: TaggedScopedPtr&lt;'guard&gt;,
        value: TaggedScopedPtr&lt;'guard&gt;,
    ) -&gt; Result&lt;(), RuntimeError&gt;;

    /// Remove an association by its key.
    fn dissoc&lt;'guard&gt;(
        &amp;self,
        guard: &amp;'guard dyn MutatorScope,
        key: TaggedScopedPtr,
    ) -&gt; Result&lt;TaggedScopedPtr&lt;'guard&gt;, RuntimeError&gt;;

    /// Returns true if the key exists in the container.
    fn exists&lt;'guard&gt;(
        &amp;self,
        guard: &amp;'guard dyn MutatorScope,
        key: TaggedScopedPtr,
    ) -&gt; Result&lt;bool, RuntimeError&gt;;
}
</code></pre>
<p>This trait contains the external API that <code>Dict</code> will expose for managing
keys and values. The implementation of each of these methods will be in terms
of the <code>find_entry()</code> function described above. Let's look at a couple of the
more complex examples, <code>assoc()</code> and <code>dissoc()</code>.</p>
<h3><a class="header" href="#assoc" id="assoc">assoc</a></h3>
<pre><code class="language-rust ignore">impl HashIndexedAnyContainer for Dict {
    fn assoc&lt;'guard&gt;(
        &amp;self,
        mem: &amp;'guard MutatorView,
        key: TaggedScopedPtr&lt;'guard&gt;,
        value: TaggedScopedPtr&lt;'guard&gt;,
    ) -&gt; Result&lt;(), RuntimeError&gt; {
        let hash = hash_key(mem, key)?;

        let mut data = self.data.get();
        // check the load factor (what percentage of the capacity is or has been used)
        if needs_to_grow(self.used_entries.get() + 1, data.capacity()) {
            // create a new, larger, backing array, and copy all existing entries over
            self.grow_capacity(mem)?;
            data = self.data.get();
        }

        // find the slot whose entry matches the hash or is the nearest available entry
        let entry = find_entry(mem, &amp;data, hash)?;

        // update counters if necessary
        if entry.key.is_nil() {
            // if `key` is nil, this entry is unused: increment the length
            self.length.set(self.length.get() + 1);
            if entry.hash == 0 {
                // if `hash` is 0, this entry has _never_ been used: increment the count
                // of used entries
                self.used_entries.set(self.used_entries.get() + 1);
            }
        }

        // finally, write the key, value and hash to the entry
        entry.key.set(key);
        entry.value.set(value);
        entry.hash = hash;

        Ok(())
    }
}
</code></pre>
<h3><a class="header" href="#dissoc" id="dissoc">dissoc</a></h3>
<pre><code class="language-rust ignore">impl HashIndexedAnyContainer for Dict {
    fn dissoc&lt;'guard&gt;(
        &amp;self,
        guard: &amp;'guard dyn MutatorScope,
        key: TaggedScopedPtr,
    ) -&gt; Result&lt;TaggedScopedPtr&lt;'guard&gt;, RuntimeError&gt; {
        let hash = hash_key(guard, key)?;

        let data = self.data.get();
        let entry = find_entry(guard, &amp;data, hash)?;

        if entry.key.is_nil() {
            // a nil key means the key was not found in the Dict
            return Err(RuntimeError::new(ErrorKind::KeyError));
        }

        // decrement the length but not the `used_entries` count
        self.length.set(self.length.get() - 1);

        // write the &quot;tombstone&quot; markers to the entry
        entry.key.set_to_nil();
        entry.hash = TOMBSTONE;

        // return the value that was associated with the key
        Ok(entry.value.get(guard))
    }
}
</code></pre>
<p>As you can see, once <code>find_entry()</code> is implemented as a separate function,
these methods become fairly easy to comprehend.</p>
<h2><a class="header" href="#conclusion-2" id="conclusion-2">Conclusion</a></h2>
<p>If you <em>haven't</em> read Bob Nystron's chapter on <a href="http://craftinginterpreters.com/hash-tables.html">hash tables</a> in Crafting 
Interpreters we encourage you to do so: it will help make sense of this 
chapter.</p>
<p>Now, we'll transition to some compiler and virtual machine design before
we continue with code implementation.</p>
<h1><a class="header" href="#virtual-machine-architecture-and-design" id="virtual-machine-architecture-and-design">Virtual Machine: Architecture and Design</a></h1>
<p>In this short chapter we will outline our virtual machine design choices. These
are substantially a matter of pragmatic dynamic language implementation points
and as such, borrow heavily from uncomplicated prior work such as Lua 5 and 
Crafting Interpreters.</p>
<h2><a class="header" href="#bytecode-1" id="bytecode-1">Bytecode</a></h2>
<p>We already discussed our Lua-inspired bytecode in a <a href="./chapter-interp-bytecode.html">previous
chapter</a>. To recap, we are using 32 bit
fixed-width opcodes with space for 8 bit register identifiers and 16 bit
literals.</p>
<h2><a class="header" href="#the-stack" id="the-stack">The stack</a></h2>
<p>Following the example of <a href="http://craftinginterpreters.com/calls-and-functions.html#call-frames">Crafting Interpreters</a> we'll maintain two separate
stack data structures:</p>
<ul>
<li>the register stack for storing stack values</li>
<li>the call frame stack</li>
</ul>
<p>In our case, these are best separated out because the register stack will be
composed entirely of <code>TaggedCellPtr</code>s.</p>
<p>To store call frames on the register stack we would have to either:</p>
<ol>
<li>allocate every stack frame on the heap with pointers to them from the
register stack </li>
<li>or coerce a call frame <code>struct</code> type into the register stack type</li>
</ol>
<p>Neither of these is attractive so we will maintain the call frame stack as an
independent data structure.</p>
<h3><a class="header" href="#the-register-stack" id="the-register-stack">The register stack</a></h3>
<p>The register stack is a homogeneous array of <code>TaggedCellPtr</code>s. Thus, no object
is allocated directly on the stack, all objects are heap allocated and the stack
only consists of pointers to heap objects. The exception is literal integers
that fit within the range allowed by a tagged pointer.</p>
<p>Since this is a register virtual machine, not following stack push and pop
semantics, and bytecode operands are limited to 8 bit register indexes, a
function is limited to addressing a maximum of 256 contiguous registers. </p>
<p>Due to function call nesting, the register stack may naturally grow much more
than a length of 256. </p>
<p>This requires us to implement a sliding window into the register stack which
will move as functions are called and return. The call frame stack will contain
the stack base pointer for each function call. We can then happily make use a
Rust slice to implement the window of 256 contiguous stack slots which a
function call is limited to.</p>
<h3><a class="header" href="#the-call-frame-stack" id="the-call-frame-stack">The call frame stack</a></h3>
<p>A call frame needs to store three critical data points:</p>
<ul>
<li>a pointer to the function being executed</li>
<li>the return instruction pointer when a nested function is called</li>
<li>the stack base pointer for the function call</li>
</ul>
<p>These three items can form a simple struct and we can define an
<code>Array&lt;CallFrame&gt;</code> type for optimum performance.</p>
<h2><a class="header" href="#global-values" id="global-values">Global values</a></h2>
<p>To store global values, we have all we need: the <code>Dict</code> type that maps <code>Symbol</code>s
to another value. The VM will, of course, have an abstraction over the internal
<code>Dict</code> to enforce <code>Symbol</code>s only as keys.</p>
<h2><a class="header" href="#closures" id="closures">Closures</a></h2>
<p>In the classic upvalues implementation from Lua 5, followed also by <a href="http://craftinginterpreters.com/closures.html">Crafting
Interpreters</a>, a linked list of upvalues is used to map stack locations to
shared variables.</p>
<p>In every respect but one, our implementation will be similar.</p>
<p>In our implementation, we'll use the <code>Dict</code> type that we already have available
to do this mapping of stack locations to shared variables. </p>
<p>As the language and compiler will implement lexical scoping, the compiler will
have static knowledge of the <em>relative</em> stack locations of closed-over variables
and can generate the appropriate bytecode operands for the virtual machine to
calculate the absolute stack locations at runtime. Thus, absolute stack
locations can be mapped to <code>Upvalue</code> objects and so a <code>Dict</code> can be employed to
facilitate the mapping. This obviates the need to implement a linked list data
structure.</p>
<p>The compiler must issue instructions to tell the VM when to make a closure data
structure. It can do so, of course, because simple analysis shows whether
a function references nonlocal bindings. A closure data structure as generated
by the compiler must reference the function that will be called and the list of
relative stack locations that correspond to each nonlocal binding. </p>
<p>The VM, when executing the instruction to make a closure, will calculate the
absolute stack locations for each nonlocal binding and create the closure
environment - a <code>List&lt;Upvalue&gt;</code>. VM instructions within the function code, as in
Lua, indirectly reference nonlocal bindings by indexing into this environment.</p>
<h2><a class="header" href="#partial-functions" id="partial-functions">Partial functions</a></h2>
<p>Here is one point where we will introduce a less common construct in our virtual
machine. Functions will be first class, that is they are objects that can be
passed around as values and arguments. On top of that, we'll allow passing
insufficient arguments to a function when it is called. The return value of
such an operation will, instead of an error, be a <code>Partial</code> instance. This value
must carry with it the arguments given and a pointer to the function waiting to
be called.</p>
<p>This is insufficient for a fully featured currying implementation but is an
interesting extension to first class functions, especially as it allows us to
not <em>require</em> lambdas to be constructed syntactically every time they might be
used.</p>
<p>By that we mean the following: if we have a function <code>(def mul (x y) (* x y))</code>,
to turn that into a function that multiplies a number by 3 we'd normally have to
define a second function, or lambda, <code>(lambda (x) (mul x 3))</code> and call it
instead. However, with a simple partial function implementation we can avoid the
lambda definition and call <code>(mul 3)</code> directly, which will collect the function
pointer for <code>mul</code> and argument <code>3</code> into a <code>Partial</code> and wait for the final
argument before calling into the function <code>mul</code> with both required arguments.</p>
<blockquote>
<p><em><strong>Note:</strong></em> We can use the same struct for both closures and partial
functions. A closure is a yet-to-be-called function carrying a list of
references to values. or a list of values. A partial is a yet-to-be-called
function carrying a list of arguments. They look very similar, and it's
possible, of course, to partially apply arguments to a closure.</p>
</blockquote>
<h2><a class="header" href="#instruction-dispatch" id="instruction-dispatch">Instruction dispatch</a></h2>
<p>In dispatch, one optimal outcome is to minimize the machine code overhead
between each VM instruction code.  This overhead, where the next VM instruction
is fetched, decoded and mapped to the entry point of the instruction code, is
the dispatch code.  The other axis of optimization is code ergonomics.</p>
<p>Prior <a href="https://pliniker.github.io/post/dispatchers/">research</a> into implementing dispatch in Rust concludes that simple
switch-style dispatch is the only cross-platform construct we can reasonably
make use of. Other mechanisms come with undesirable complexity or are platform
dependent. For the most part, with modern CPU branch prediction, the overhead
of switch dispatch is small.</p>
<p>What this looks like: a single <code>match</code> expression with a pattern to represent
each bytecode discriminant, all wrapped in a loop. To illustrate:</p>
<pre><code class="language-rust ignore">loop {
    let opcode = get_next_opcode();
    match opcode {
        Opcode::Add(a, x, y) =&gt; { ... },
        Opcode::Call(f, r, p) =&gt; { ... },
    }
}
</code></pre>
<h2><a class="header" href="#thats-it" id="thats-it">That's it!</a></h2>
<p>Next we'll look at the counterpart of VM design - compiler design.</p>
<h1><a class="header" href="#virtual-machine-implementation" id="virtual-machine-implementation">Virtual Machine: Implementation</a></h1>
<p>In this chapter we'll dive into some of the more interesting and important
implementation details of our virtual machine.</p>
<p>To begin with, we'll lay out a struct for a single thread of execution. This
struct should contain everything needed to execute the output of the compiler.</p>
<pre><code class="language-rust ignore">pub struct Thread {
    /// An array of CallFrames
    frames: CellPtr&lt;CallFrameList&gt;,
    /// An array of pointers any object type
    stack: CellPtr&lt;List&gt;,
    /// The current stack base pointer
    stack_base: Cell&lt;ArraySize&gt;,
    /// A dict that should only contain Number keys and Upvalue values. This is a mapping of
    /// absolute stack indeces to Upvalue objects where stack values are closed over.
    upvalues: CellPtr&lt;Dict&gt;,
    /// A dict that should only contain Symbol keys but any type as values
    globals: CellPtr&lt;Dict&gt;,
    /// The current instruction location
    instr: CellPtr&lt;InstructionStream&gt;,
}
</code></pre>
<p>Here we see every data structure needed to represent:</p>
<ul>
<li>function call frames</li>
<li>stack values</li>
<li>closed-over stack values (Upvalues)</li>
<li>global values</li>
<li>bytecode to execute</li>
</ul>
<p>The VM's primary operation is to iterate through instructions, executing each
in sequence. The outermost control struture is, therefore, a loop containing
a <code>match</code> expression.</p>
<p>Here is a code extract of the opening lines of this match operation. The
function shown is a member of the <code>Thread</code> struct. It evaluates the next
instruction and is called in a loop by an outer function. We'll look at several
extracts from this function in this chapter.</p>
<pre><code class="language-rust ignore">    /// Execute the next instruction in the current instruction stream
    fn eval_next_instr&lt;'guard&gt;(
        &amp;self,
        mem: &amp;'guard MutatorView,
    ) -&gt; Result&lt;EvalStatus&lt;'guard&gt;, RuntimeError&gt; {
        // TODO not all these locals are required in every opcode - optimize and get them only
        // where needed
        let frames = self.frames.get(mem);
        let stack = self.stack.get(mem);
        let globals = self.globals.get(mem);
        let instr = self.instr.get(mem);

        // Establish a 256-register window into the stack from the stack base
        stack.access_slice(mem, |full_stack| {
            let stack_base = self.stack_base.get() as usize;
            let window = &amp;mut full_stack[stack_base..stack_base + 256];

            // Fetch the next instruction and identify it
            let opcode = instr.get_next_opcode(mem)?;

            match opcode {
                // Do nothing.
                Opcode::NoOp =&gt; return Ok(EvalStatus::Pending),

                ...
</code></pre>
<p>The function obtains a slice view of the register stack, then narrows that down
to a 256 register window for the current function.</p>
<p>Then it fetches the next opcode and using <code>match</code>, decodes it.</p>
<p>Let's take a closer look at the stack.</p>
<h2><a class="header" href="#the-stack-1" id="the-stack-1">The stack</a></h2>
<p>While some runtimes and compilers, particularly low-level languages, have a
single stack that represents both function call information and local variables,
our high-level runtime splits the stack into:</p>
<ol>
<li>a stack of <code>CallFrame</code> objects containing function call and return
information</li>
<li>and a register stack for local variables.</li>
</ol>
<p>Let's look at each in turn.</p>
<h3><a class="header" href="#the-register-stack-1" id="the-register-stack-1">The register stack</a></h3>
<p>In our <code>Thread</code> struct, the register stack is represented by the two members:</p>
<pre><code class="language-rust ignore">pub struct Thread {
    ...
    stack: CellPtr&lt;List&gt;,
    stack_base: Cell&lt;ArraySize&gt;,
    ...
}
</code></pre>
<p>Remember that the <code>List</code> type is defined as <code>Array&lt;TaggedCellPtr&gt;</code> and is
therefore an array of tagged pointers. Thus, the register stack is a homogenous
array of word sized values that are pointers to objects on the heap or values
that can be inlined in the tagged pointer word.</p>
<p>We also have a <code>stack_base</code> variable to quickly retrieve the offset into <code>stack</code>
that indicates the beginning of the window of 256 registers that the current
function has for it's local variables.</p>
<h3><a class="header" href="#the-call-frame-stack-1" id="the-call-frame-stack-1">The call frame stack</a></h3>
<p>In our <code>Thread</code> struct, the call frame stack is represented by the members:</p>
<pre><code class="language-rust ignore">pub struct Thread {
    ...
    frames: CellPtr&lt;CallFrameList&gt;,
    instr: CellPtr&lt;InstructionStream&gt;,
    ...
}
</code></pre>
<p>A <code>CallFrame</code> and an array of them are defined as:</p>
<pre><code class="language-rust ignore">#[derive(Clone)]
pub struct CallFrame {
    /// Pointer to the Function being executed
    function: CellPtr&lt;Function&gt;,
    /// Return IP when returning from a nested function call
    ip: Cell&lt;ArraySize&gt;,
    /// Stack base - index into the register stack where register window for this function begins
    base: ArraySize,
}

pub type CallFrameList = Array&lt;CallFrame&gt;;
</code></pre>
<p>A <code>CallFrame</code> contains all the information needed to resume a function when
a nested function call returns:</p>
<ul>
<li>a <code>Function</code> object, which references the <code>Bytecode</code> comprising the
function</li>
<li>the return instruction pointer</li>
<li>the stack base index for the function's stack register window</li>
</ul>
<p>On every function call, a <code>CallFrame</code> instance is pushed on to the <code>Thread</code>'s
<code>frames</code> stack and on every return from a function, the top <code>CallFrame</code> is
popped off the stack.</p>
<p>Additionally, we keep a pointer to the current executing function (the function
represented by the top <code>CallFrame</code>) with the member <code>instr: CellPtr&lt;InstructionStream&gt;</code>.</p>
<p>For a review of the definition of <code>InstructionStream</code> see the
<a href="./chapter-interp-bytecode.html">bytecode</a> chapter where we defined it as
a pair of values - a <code>ByteCode</code> reference and a pointer to the next <code>Opcode</code>
to fetch.</p>
<p>The VM keeps the <code>InstructionStream</code> object pointing at the same <code>ByteCode</code>
object as is pointed at by the <code>Function</code> in the <code>CallFrame</code> at the top of
the call frame stack. Thus, when a call frame is popped off the stack, the
<code>InstructionStream</code> is updated with the <code>ByteCode</code> and instruction pointer
from the <code>CallFrame</code> at the new stack top; and similarly when a function
is called <em>into</em> and a new <code>CallFrame</code> is pushed on to the stack.</p>
<h2><a class="header" href="#functions-and-function-calls" id="functions-and-function-calls">Functions and function calls</a></h2>
<h3><a class="header" href="#function-objects" id="function-objects">Function objects</a></h3>
<p>Since we've mentioned <code>Function</code> objects above, let's now have a look at the
definition.</p>
<pre><code class="language-rust ignore">#[derive(Clone)]
pub struct Function {
    /// name could be a Symbol, or nil if it is an anonymous fn
    name: TaggedCellPtr,
    /// Number of arguments required to activate the function
    arity: u8,
    /// Instructions comprising the function code
    code: CellPtr&lt;ByteCode&gt;,
    /// Param names are stored for introspection of a function signature
    param_names: CellPtr&lt;List&gt;,
    /// List of (CallFrame-index: u8 | Window-index: u8) relative offsets from this function's
    /// declaration where nonlocal variables will be found. Needed when creating a closure. May be
    /// nil
    nonlocal_refs: TaggedCellPtr,
}
</code></pre>
<p>Instances of <code>Function</code> are produced by the compiler, one for each function
definition that is compiled, including nested function definitions.</p>
<p>A <code>Function</code> object is a simple collection of values, some of which may be
<code>nil</code>. Any member represented by a <code>TaggedCellPtr</code> may, of course, contain
a <code>nil</code> value.</p>
<p>Thus the function may be anonymous, represented by a <code>nil</code> name value.</p>
<p>While the function name is optional, the parameter names are always included.
Though they do not need to be known in order to execute the function, they are
useful for representing the function in string form if the programmer needs to
introspect a function object.</p>
<p>Members that are <em>required</em> to execute the function are the arity, the
<code>ByteCode</code> and any nonlocal references.</p>
<p>Nonlocal references are an optional list of <code>(relative_stack_frame, register)</code>
tuples, provided by the compiler, that are needed to locate nonlocal variables
on the register stack. These are, of course, a key component of implementing
closures.</p>
<p>We'll talk about closures shortly, but before we do, we'll extend <code>Function</code>s
with partial application of arguments.</p>
<h3><a class="header" href="#partial-functions-1" id="partial-functions-1">Partial functions</a></h3>
<p>A partial function application takes a subset of the arguments required to
make a function call. These arguments must be stored for later.</p>
<p>Thus, a <code>Partial</code> object references the <code>Function</code> to be called and a list
of arguments to give it when the call is finally executed.</p>
<p>Below is the definition of <code>Partial</code>. Note that it also contains a possible
closure environment which, again, we'll arrive at momentarily.</p>
<pre><code class="language-rust ignore">#[derive(Clone)]
pub struct Partial {
    /// Remaining number of arguments required to activate the function
    arity: u8,
    /// Number of arguments already applied
    used: u8,
    /// List of argument values already applied
    args: CellPtr&lt;List&gt;,
    /// Closure environment - must be either nil or a List of Upvalues
    env: TaggedCellPtr,
    /// Function that will be activated when all arguments are applied
    func: CellPtr&lt;Function&gt;,
}
</code></pre>
<p>The <code>arity</code> and <code>used</code> members indicate how many arguments are expected and how
many have been given. These are provided directly in this struct rather than
requiring dereferencing the <code>arity</code> on the <code>Function</code> object and the length of
the <code>args</code> list. This is for convenience and performance.</p>
<p>Each time more arguments are added to a <code>Partial</code>, a new <code>Partial</code> instance must
be allocated and the existing arguments copied over. A <code>Partial</code> object, once
created, is immutable.</p>
<h3><a class="header" href="#closures-1" id="closures-1">Closures</a></h3>
<p>Closures and partial applications have, at an abstract level, something in
common: they both reference values that the function will need when it is
finally called.</p>
<p>It's also possible, of course, to have a partially applied closure.</p>
<p>We can extend the <code>Partial</code> definition with a closure environment so that we can
use the same object type everywhere to represent a function pointer, applied
arguments and closure environment as needed.</p>
<h4><a class="header" href="#compiling-a-closure" id="compiling-a-closure">Compiling a closure</a></h4>
<p>The compiler, because it keeps track of variable names and scopes, knows when a
<code>Function</code> references nonlocal variables. After such a function is defined, the
compiler emits a <code>MakeClosure</code> instruction.</p>
<h4><a class="header" href="#referencing-the-stack-with-upvalues" id="referencing-the-stack-with-upvalues">Referencing the stack with upvalues</a></h4>
<p>The VM, when it executes <code>MakeClosure</code>, creates a new <code>Partial</code> object.  It
then iterates over the list of nonlocal references and allocates an <code>Upvalue</code>
object for each, which are added to the <code>env</code> member on the <code>Partial</code> object.</p>
<p>The below code extract is from the function <code>Thread::eval_next_instr()</code> in
the <code>MakeClosure</code> instruction decode and execution block.</p>
<p>The two operands of the <code>MakeClosure</code> operation - <code>dest</code> and <code>function</code> - are
registers. <code>function</code> points at the <code>Function</code> to be given an environment and
made into a closure <code>Partial</code> instance; the pointer to this instance will be
written to the <code>dest</code> register.</p>
<pre><code class="language-rust ignore">                // This operation should be generated by the compiler after a function definition
                // inside another function but only if the nested function refers to nonlocal
                // variables.
                // The result of this operation is a Partial with a closure environment
                Opcode::MakeClosure { dest, function } =&gt; {
                    // 1. iter over function nonlocals
                    //   - calculate absolute stack offset for each
                    //   - find existing or create new Upvalue for each
                    //   - create closure environment with list of Upvalues
                    // 2. create new Partial with environment
                    // 3. set dest to Partial
                    let function_ptr = window[function as usize].get(mem);
                    if let Value::Function(f) = *function_ptr {
                        let nonlocals = f.nonlocals(mem);
                        // Create an environment array for upvalues
                        let env = List::alloc_with_capacity(mem, nonlocals.length())?;

                        // Iter over function nonlocals, calculating absolute stack offset for each
                        nonlocals.access_slice(mem, |nonlocals| -&gt; Result&lt;(), RuntimeError&gt; {
                            for compound in nonlocals {
                                // extract 8 bit register and call frame values from 16 bit nonlocal
                                // descriptors
                                let frame_offset = (*compound &gt;&gt; 8) as ArraySize;
                                let window_offset = (*compound &amp; 0xff) as ArraySize;

                                // look back frame_offset frames and add the register number to
                                // calculate the absolute stack position of the value
                                let frame = frames.get(mem, frames.length() - frame_offset)?;
                                let location = frame.base + window_offset;

                                // look up, or create, the Upvalue for the location, and add it to
                                // the environment
                                let (_, upvalue) = self.upvalue_lookup_or_alloc(mem, location)?;
                                StackAnyContainer::push(&amp;*env, mem, upvalue.as_tagged(mem))?;
                            }

                            Ok(())
                        })?;

                        // Instantiate a Partial function application from the closure environment
                        // and set the destination register
                        let partial = Partial::alloc(mem, f, Some(env), &amp;[])?;
                        window[dest as usize].set(partial.as_tagged(mem));
                    } else {
                        return Err(err_eval(&quot;Cannot make a closure from a non-Function type&quot;));
                    }
                }
</code></pre>
<p>The <code>Upvalue</code> struct itself is defined as:</p>
<pre><code class="language-rust ignore">#[derive(Clone)]
pub struct Upvalue {
    // Upvalue location can't be a pointer because it would be a pointer into the dynamically
    // alloocated stack List - the pointer would be invalidated if the stack gets reallocated.
    value: TaggedCellPtr,
    closed: Cell&lt;bool&gt;,
    location: ArraySize,
}
</code></pre>
<p>An <code>Upvalue</code> is an object that references an absolute register stack location
(that is the <code>location</code> member.)</p>
<p>The initial value of <code>closed</code> is <code>false</code>. In this state, the location on the
stack that contains the variable <em>must</em> be a valid location. That is, the stack
can not have been unwound yet. If the closure is called, <code>Upvalue</code>s in this
state are simply an indirection between the function and the variable on the
register stack.</p>
<p>The compiler is able to keep track of variables and whether they are closed
over. It emits bytecode instructions to close <code>Upvalue</code> objects when variables
on the stack go out of scope.</p>
<p>This instruction, <code>CloseUpvalues</code>, copies the variable from the register stack
to the <code>value</code> member of the <code>Upvalue</code> object and sets <code>closed</code> to <code>true</code>.</p>
<p>From then on, when the closure reads or writes to this variable, the value on
the <code>Upvalue</code> object is modified rather than the location on the register stack.</p>
<h2><a class="header" href="#global-values-1" id="global-values-1">Global values</a></h2>
<pre><code class="language-rust ignore">pub struct Thread {
    ...
    globals: CellPtr&lt;Dict&gt;,
    ...
}
</code></pre>
<p>The outermost scope of a program's values and functions are the global values.
We can manage these with an instance of a <code>Dict</code>. While a <code>Dict</code> can use any
hashable value as a key, internally the VM will only allow <code>Symbol</code>s to be
keys. That is, globals must be named objects.</p>
<h1><a class="header" href="#next" id="next">Next...</a></h1>
<p>Let's dive into the compiler!</p>
<h1><a class="header" href="#compiler-design" id="compiler-design">Compiler: Design</a></h1>
<p>Drawing from the <a href="./chapter-interp-vm-design.html">VM design</a>, the compiler must
support the following language constructs:</p>
<ul>
<li>function definitions</li>
<li>anonymous functions</li>
<li>function calls</li>
<li>lexical scoping</li>
<li>closures</li>
<li>local variables</li>
<li>global variables</li>
<li>expressions</li>
</ul>
<p>This is a minimally critical set of features that any further language
constructs can be built on while ensuring that our compiler remains easy to
understand for the purposes of this book.</p>
<p><a href="./chapter-interp-parsing.html">Our parser, recall</a>, reads in s-expression syntax
and produces a nested <code>Pair</code> and <code>Symbol</code> based abstract syntax tree. Adding
other types - integers, strings, arrays etc - is mostly a matter of expanding
the parser.  The compiler as described here, being for a dynamically typed
language, will support them without refactoring.</p>
<h2><a class="header" href="#evalapply" id="evalapply">Eval/apply</a></h2>
<p>Our compiler design is based on the <em>eval/apply</em> pattern.</p>
<p>In this pattern we recursively descend into the <code>Pair</code> AST, calling <em>eval</em> on
the root node of the expression to be compiled.</p>
<p><em>Eval</em> is, of course, short for &quot;evaluate&quot; - we want to evaluate the given
expression. In the case of a compiler, we don't want the result yet, rather
the sequence of instructions that will generate the result.</p>
<p>More concretely, <em>eval</em> looks at the node in the AST it is given and if it
resolves to fetching a value for a variable, it generates that instruction;
otherwise if it is a compound expression, the arguments are evaluated and then
the function and arguments are passed to <em>apply</em>, which generates appropriate
function call instructions.</p>
<h3><a class="header" href="#designing-an-eval-function" id="designing-an-eval-function">Designing an Eval function</a></h3>
<p><em>Eval</em> looks at the given node and attempts to generate an instruction for it
that would resolve the node to a value - that is, evaluate it.</p>
<h4><a class="header" href="#symbols" id="symbols">Symbols</a></h4>
<p>If the node is a special symbol, such as <code>nil</code> or <code>true</code>, then it is treated as
a literal and an instruction is generated to load that literal symbol into the
next available register.</p>
<p>Otherwise if the node is any other symbol, it is assumed to be bound to a value
(it must be a variable) and an instruction is generated for fetching the value
into a register.</p>
<p>Variables come in three kinds: local, nonlocal or global.</p>
<p><strong>Local</strong>: the symbol has been declared earlier in the expression (either it is
a function parameter or it was declared using <code>let</code>) and the compiler already
has a record of it. The symbol is already associated with a local register
index and a simple register copy instruction is generated.</p>
<p><strong>Nonlocal</strong>: the symbol has been bound in a parent nesting function. Again,
the compiler already has a record of the declaration, which register is
associated with the symbol and which relative call frame will contain that
register. An upvalue lookup instruction is generated.</p>
<p><strong>Global</strong>: if the symbol isn't found as a local binding or a nonlocal binding,
it is assumed to be a global, and a late-binding global lookup instruction is
generated. In the event the programmer has misspelled a variable name, this is
possibly the instruction that will be generated and the programmer will see an
unknown-variable error at runtime.</p>
<h4><a class="header" href="#expressions-and-function-calls" id="expressions-and-function-calls">Expressions and function calls</a></h4>
<p>When <em>eval</em> is passed a <code>Pair</code>, this represents the beginning of an expression,
a function call. A composition of things.</p>
<p>In s-expression syntax, all expressions and function calls looks like
<code>(function_name arg1 arg2)</code>.  That is parsed into a <code>Pair</code> tree, which takes
the form:</p>
<pre><code>Pair(
  Symbol(function_name),
  Pair(
    Symbol(arg1),
    Pair(
      Symbol(arg2),
      nil
    )
  )
)
</code></pre>
<p>It is <em>apply</em>'s job to handle this case, so <em>eval</em> extracts the first and
second values from the outermost <code>Pair</code> and passes them into apply. In more
general terms, <em>eval</em> calls <em>apply</em> with the function name and the argument
list and leaves the rest up to <em>apply</em>.</p>
<h3><a class="header" href="#designing-an-apply-function" id="designing-an-apply-function">Designing an Apply function</a></h3>
<p><em>Apply</em> takes a function name and a list of arguments. First it recurses into
<em>eval</em> for each argument expression, then  generates instructions to call the
function with the argument results.</p>
<h4><a class="header" href="#calling-functions" id="calling-functions">Calling functions</a></h4>
<p>Functions are either built into to the language and VM or are
library/user-defined functions composed of other functions.</p>
<p>In every case, the simplified pattern for function calls is:</p>
<ul>
<li>allocate a register to write the return value into</li>
<li><em>eval</em> each of the arguments in sequence, allocating their resulting values
into consequent registers</li>
<li>compile the function call opcode, giving it the number of argument registers
it should expect</li>
</ul>
<p>Compiling a call to a builtin function might translate directly to a dedicated
bytecode operation. For example, querying whether a value is <code>nil</code> with builtin
function <code>nil?</code> compiles 1:1 to a bytecode operation that directly represents
that query.</p>
<p>Compiling a call to a user defined function is a more involved. In it's more
general form, supporting first class functions and closures, a function call
requires two additional pointers to be placed in registers. The complete
function call register allocation looks like this:</p>
<table><thead><tr><th>Register</th><th>Use</th></tr></thead><tbody>
<tr><td>0</td><td>reserved for return value</td></tr>
<tr><td>1</td><td>reserved for closure environment pointer</td></tr>
<tr><td>2</td><td>first argument</td></tr>
<tr><td>3</td><td>second argument</td></tr>
<tr><td>...</td><td></td></tr>
<tr><td>n</td><td>function pointer</td></tr>
</tbody></table>
<p>If a closure is called, the closure object itself contains a pointer to it's
environment and the function to call and those pointers can be copied over to
registers. Otherwise, the closure environment pointer will be a <code>nil</code> pointer.</p>
<p>The VM, when entering a new function, will represent the return value register
always as the zeroth register.</p>
<p>When the function call returns, all registers except the return value are
discarded.</p>
<h4><a class="header" href="#compiling-functions" id="compiling-functions">Compiling functions</a></h4>
<p>Let's look at a simple function definition:</p>
<pre><code>(def is_true (x)
  (is? x true))
</code></pre>
<p>This function has a name <code>is_true</code>, takes one argument <code>x</code> and evaluates one
expression <code>(is? x true)</code>.</p>
<p>The same function may be written without a name:</p>
<pre><code>(lambda (x) (is? x true))
</code></pre>
<p>Compiling a function requires a few inputs:</p>
<ul>
<li>an optional reference to a parent nesting function</li>
<li>an optional function name</li>
<li>a list of argument names</li>
<li>a list of expressions that will compute the return value</li>
</ul>
<p>The desired output is a data structure that combines:</p>
<ul>
<li>the optional function name</li>
<li>the argument names</li>
<li>the compiled bytecode</li>
</ul>
<p>First, a scope structure is established. A scope is a lexical block in which
variables are bound and unbound. In the compiler, this structure is simply a
mapping of variable name to the register number that contains the value.</p>
<p>The first variables to be bound in the function's scope are the argument names.
The compiler, given the list of argument names to the function and the order in
which the arguments are given, associates each argument name with the register
number that will contain it's value. As we saw above, these are predictably and
reliably registers 2 and upward, one for each argument.</p>
<p>A scope may have a parent scope if the function is defined within another
function. This is how nonlocal variable references will be looked up. We will
go further into that when we discuss closures.</p>
<p>The second step is to <em>eval</em> each expression in the function, assigning the
result to register 0, the preallocated return value register. The result of
compiling each expression via <em>eval</em> is bytecode.</p>
<p>Thirdly and finally, a function object is instantiated, given it's name, the
argument names and the bytecode.</p>
<h4><a class="header" href="#compiling-closures" id="compiling-closures">Compiling closures</a></h4>
<p>During compilation of the expressions within a function, if any of those
expressions reference nonlocal variables (that is, variables not declared
within the scope of the function) then the function object needs additional
data to describe how to access those nonlocal variables at runtime.</p>
<p>In the below example, the anonymous inner function references the parameter
<code>n</code> to the outer function, <code>n</code>. When the inner function is returned, the value
of <code>n</code> must be carried with it even after the stack scope of the outer function
is popped and later overwritten with values for other functions.</p>
<pre><code>(def make_adder (n)
  (lambda (x) (+ x n))
)
</code></pre>
<p><em>Eval</em>, when presented with a symbol to evaluate that has not been declared in
the function scope, searches outer scopes next. If a binding is found in an
outer scope, a nonlocal reference is added to the function's <em>local</em> scope
that points to the outer scope and a <code>GetUpvalue</code> instruction is compiled.</p>
<p>This nonlocal reference is a combination of two values: a count of stack
frames to skip over to find the outer scope variable and the register offset in
that stack frame.</p>
<p>Non-local references are added to the function object that is returned by the
function compiler. The VM will use these to identify the absolute location on
the stack where a nonlocal variable should be read from and create upvalue
objects at runtime when a variable is closed over.</p>
<h4><a class="header" href="#compiling-let" id="compiling-let">Compiling let</a></h4>
<p>Let is the declaration of variables and assigning values: the binding of
values, or the results of expressions, to symbols. Secondly, it provides
space to evaluate expressions that incorporate those variables.</p>
<p>Here we bind the result of <code>(make_adder 3)</code> - a function - to the symbol
<code>add_3</code> and then call <code>add_3</code> with argument <code>4</code>.</p>
<pre><code>(let ((add_3 (make_adder 3)))
  (add_3 4))
</code></pre>
<p>The result of the entire <code>let</code> expression should be <code>7</code>.</p>
<p>Compiling <code>let</code> simply introduces additional scopes within a function scope.
That is, instead of a function containing a single scope for all it's
variables, scopes are nested. A stack of scopes is needed, with the parameters
occupying the outermost scope.</p>
<p>First a new scope is pushed on to the scope stack and each symbol being bound
is added to the new scope.</p>
<p>To generate code, a result register is reserved and a register for each binding
is reserved.</p>
<p>Finally, each expression is evaluated and the scope is popped, removing the
bindings from view.</p>
<h2><a class="header" href="#register-allocation" id="register-allocation">Register allocation</a></h2>
<p>A function call may make use of no more than 256 registers. Recall from earlier
that the 0th register is reserved for the function return value and subsequent
registers are reserved for the function arguments.</p>
<p>Beyond these initial registers the compiler uses a simple strategy in register
allocation: if a variable (a parameter or a <code>let</code> binding) is declared, it is
allocated a register based on a stack discipline. Thus, variables are
essentially pushed and popped off the register stack as they come into and out
of scope.</p>
<p>This strategy primarily ensures code simplicity - there is no register
allocation optimization.</p>
<h2><a class="header" href="#cest-tout" id="cest-tout">C'est tout!</a></h2>
<p>That covers the VM and compiler design at an overview level. We've glossed over
a lot of detail but the next chapters will expose the implementation detail.
Get ready!</p>
<h1><a class="header" href="#compiler-implementation" id="compiler-implementation">Compiler: Implementation</a></h1>
<p>Before we get into eval and apply let's consider how we will support variables
and lexical scoping.</p>
<h2><a class="header" href="#variables-and-scopes" id="variables-and-scopes">Variables and Scopes</a></h2>
<p>As seen in the previous chapter, variable accesses come in three types, as far
as the compiler and VM are concerned: local, nonlocal and global. Each access
uses a different bytecode operation, and so the compiler must be able to
determine what operations to emit at compile time.</p>
<p>Given that we have named function parameters and <code>let</code>, we have syntax for
explicit variable declaration within function definitions. This means that we
can easily keep track of whether a variable reference is local, nonlocal or
global.</p>
<p>If a variable wasn't declared as a parameter or in a <code>let</code> block, it
must be global and global variables are accessed dynamically by name.</p>
<p>As far as local and nonlocal variables are concerned, the VM does not care
about or consider their names. At the VM level, local and nonlocal variables
are numbered registers. That is, each function's local variables are mapped to
a register numbered between 2 and 255. The compiler must generate the mapping
from variable names to register numbers.</p>
<p>For generating and maintaining mappings, we need data structures for keeping
track of:</p>
<ul>
<li>function local variables and their mappings to register numbers</li>
<li>references to nonlocal variables and their relative stack offsets</li>
<li>nested scopes within functions</li>
</ul>
<h3><a class="header" href="#named-variables" id="named-variables">Named variables</a></h3>
<p>Our first data structure will define a register based variable:</p>
<pre><code class="language-rust ignore">/// A variable is a named register. It has compile time metadata about how it is used by closures.
struct Variable {
    register: Register,
    closed_over: Cell&lt;bool&gt;,
}
</code></pre>
<p>For every named, non-global variable (created by defining function parameters
and <code>let</code> blocks) a <code>Variable</code> instance is created in the compiler.</p>
<p>The member <code>closed_over</code> defaults to <code>false</code>. If the compiler detects that the
variable must escape the stack as part of a closure, this flag will be flipped
to <code>true</code> (it cannot be set back to <code>false</code>.)</p>
<h3><a class="header" href="#scope-structure" id="scope-structure">Scope structure</a></h3>
<p>The data structures that manage nesting of scopes and looking up a <code>Variable</code>
by name are defined here.</p>
<pre><code class="language-rust ignore">/// A Scope contains a set of local variable to register bindings
struct Scope {
    /// symbol -&gt; variable mapping
    bindings: HashMap&lt;String, Variable&gt;,
}

/// A nonlocal reference will turn in to an Upvalue at VM runtime.
/// This struct stores the non-zero frame offset and register values of a parent function call
/// frame where a binding will be located.
struct Nonlocal {
    upvalue_id: u8,
    frame_offset: u8,
    frame_register: u8,
}

/// A Variables instance represents a set of nested variable binding scopes for a single function
/// definition.
struct Variables&lt;'parent&gt; {
    /// The parent function's variables.
    parent: Option&lt;&amp;'parent Variables&lt;'parent&gt;&gt;,
    /// Nested scopes, starting with parameters/arguments on the outermost scope and let scopes on
    /// the inside.
    scopes: Vec&lt;Scope&gt;,
    /// Mapping of referenced nonlocal nonglobal variables and their upvalue indexes and where to
    /// find them on the stack.
    nonlocals: RefCell&lt;HashMap&lt;String, Nonlocal&gt;&gt;,
    /// The next upvalue index to assign when a new nonlocal is encountered.
    next_upvalue: Cell&lt;u8&gt;,
}
</code></pre>
<p>For every function defined, the compiler maintains an instance of the type
<code>Variables</code>.</p>
<p>Each function's <code>Variables</code> has a stack of <code>Scope</code> instances, each of which has
it's own set of name to <code>Variable</code> register number mappings.  The outermost
<code>Scope</code> contains the mapping of function parameters to registers.</p>
<p>A nested function's <code>Variables</code>, when the function refers to a nesting
function's variable, builds a mapping of nonlocal variable name to relative
stack position of that variable. This is a <code>NonLocal</code> - a relative stack frame
offset and the register number within that stack frame of the variable.</p>
<p>In summary, under these definitions:</p>
<ul>
<li>A <code>Nonlocal</code> instance caches a relative stack location of a nonlocal variable
for compiling upvalues</li>
<li><code>Scope</code> manages the mapping of a variable name to the <code>Variable</code> register
number within a single scope</li>
<li><code>Variables</code> maintains all the nested scopes for a function during compilation
and caches all the nonlocal references. It also keeps a reference to a parent
nesting function if there is one, in order to handle lexically scoped
lookups.</li>
</ul>
<h3><a class="header" href="#retrieving-named-variables" id="retrieving-named-variables">Retrieving named variables</a></h3>
<p>Whenever a variable is referenced in source code, the mapping to it's register
must be looked up. The result of a lookup is <code>Option&lt;Binding&gt;</code>.</p>
<pre><code class="language-rust ignore">/// A binding can be either local or via an upvalue depending on how a closure refers to it.
#[derive(Copy, Clone, PartialEq)]
enum Binding {
    /// An local variable is local to a function scope
    Local(Register),
    /// An Upvalue is an indirection for pointing at a nonlocal variable on the stack
    Upvalue(UpvalueId),
}
</code></pre>
<p>The lookup process checks the local function scopes first.</p>
<p>If the variable is found to be declared there, <code>Some(Local)</code> enum variant is
returned. In terms of bytecode, this will translate to a direct register
reference.</p>
<p>Next, any outer function scopes are searched. If the variable is found in any
outer scope, <code>Some(Upvalue)</code> variant is returned. The compiler will emit
instructions to copy the value refered to by the upvalue into a function-local
temporary register.</p>
<p>If the lookup for the variable returns <code>None</code>, a global lookup instruction is
emitted that will dynamically look up the variable name in the global namespace
and copy the result into a function-local temporary register or raise an error
if the binding does not exist.</p>
<h2><a class="header" href="#evaluation" id="evaluation">Evaluation</a></h2>
<p>We've just somewhat described what happens in the lower levels of <em>eval</em>. Let's
finish the job and put <em>eval</em> in a code context. Here is the definition of a
function compilation data structure:</p>
<pre><code class="language-rust ignore">struct Compiler&lt;'parent&gt; {
    bytecode: CellPtr&lt;ByteCode&gt;,
    /// Next available register slot.
    next_reg: Register,
    /// Optional function name
    name: Option&lt;String&gt;,
    /// Function-local nested scopes bindings list (including parameters at outer level)
    vars: Variables&lt;'parent&gt;,
}
</code></pre>
<p>The two interesting members are</p>
<ul>
<li><code>bytecode</code>, which is an instance of <a href="./chapter-interp-bytecode.html">ByteCode</a></li>
<li><code>vars</code>, an instance of <code>Variables</code> which we've described above. This instance
will be the outermost scope of the <code>let</code> or function block being compiled.</li>
</ul>
<p>The main entrypoint to this structure is the function <code>compile_function()</code>:</p>
<pre><code class="language-rust ignore">    fn compile_function&lt;'guard&gt;(
        mut self,
        mem: &amp;'guard MutatorView,
        name: TaggedScopedPtr&lt;'guard&gt;,
        params: &amp;[TaggedScopedPtr&lt;'guard&gt;],
        exprs: &amp;[TaggedScopedPtr&lt;'guard&gt;],
    ) -&gt; Result&lt;ScopedPtr&lt;'guard, Function&gt;, RuntimeError&gt; {
        ...
    }
</code></pre>
<p>This function will set up a <code>Variables</code> scope with the given parameters and call
into function <code>compile_eval()</code> for each expression in the function. The full
definition of <code>compile_eval()</code> is below, and we'll go into the details of
<code>compile_function()</code> later.</p>
<pre><code class="language-rust ignore">    fn compile_eval&lt;'guard&gt;(
        &amp;mut self,
        mem: &amp;'guard MutatorView,
        ast_node: TaggedScopedPtr&lt;'guard&gt;,
    ) -&gt; Result&lt;Register, RuntimeError&gt; {
        match *ast_node {
            Value::Pair(p) =&gt; self.compile_apply(mem, p.first.get(mem), p.second.get(mem)),
            Value::Symbol(s) =&gt; {
                match s.as_str(mem) {
                    &quot;nil&quot; =&gt; {
                        let dest = self.acquire_reg();
                        self.push(mem, Opcode::LoadNil { dest })?;
                        Ok(dest)
                    }

                    &quot;true&quot; =&gt; self.push_load_literal(mem, mem.lookup_sym(&quot;true&quot;)),

                    // Search scopes for a binding; if none do a global lookup
                    _ =&gt; {
                        match self.vars.lookup_binding(ast_node)? {
                            Some(Binding::Local(register)) =&gt; Ok(register),

                            Some(Binding::Upvalue(upvalue_id)) =&gt; {
                                // Retrieve the value via Upvalue indirection
                                let dest = self.acquire_reg();
                                self.push(
                                    mem,
                                    Opcode::GetUpvalue {
                                        dest,
                                        src: upvalue_id,
                                    },
                                )?;
                                Ok(dest)
                            }

                            None =&gt; {
                                // Otherwise do a late-binding global lookup
                                let name = self.push_load_literal(mem, ast_node)?;
                                let dest = name; // reuse the register
                                self.push(mem, Opcode::LoadGlobal { dest, name })?;
                                Ok(dest)
                            }
                        }
                    }
                }
            }

            _ =&gt; self.push_load_literal(mem, ast_node),
        }
    }
</code></pre>
<p>Note that the return type is <code>Result&lt;Register, RuntimeError&gt;</code>. That is, a
successful <em>eval</em> will return a register where the result will be stored at
runtime.</p>
<p>In the function body, the match branches fall into three categories:</p>
<ul>
<li>keywords literals (<code>nil</code>, <code>true</code>)</li>
<li>all other literals</li>
<li>named variables represented by <code>Symbol</code>s</li>
</ul>
<p>What's in the evaluation of the <code>Symbol</code> AST type? Locals, nonlocals and
globals!</p>
<p>We can see the generation of special opcodes for retrieving nonlocal and global
values here, whereas a local will resolve directly to an existing register
without the need to generate any additional opcodes.</p>
<h2><a class="header" href="#application" id="application">Application</a></h2>
<p>To evaluate a function call, we switch over to <em>apply</em>:</p>
<pre><code class="language-rust ignore">        match *ast_node {
            ...

            Value::Pair(p) =&gt; self.compile_apply(mem, p.first.get(mem), p.second.get(mem)),

            ...
        }
</code></pre>
<p>This is the evaluation of the <code>Pair</code> AST type. This represents, visually, the
syntax <code>(function_name arg1 arg2 argN)</code> which is, of course, a function call.
<em>Eval</em> cannot tell us the value of a function call, the function must be applied
to it's arguments first. Into <em>apply</em> we recurse.</p>
<p>The first argument to <code>compile_apply()</code> is the function name <code>Symbol</code>, the
second argument is the list of function arguments.</p>
<p>Since we included the full <code>compile_eval()</code> function earlier, it wouldn't be
fair to leave out the definition of <code>compile_apply()</code>. Here it is:</p>
<pre><code class="language-rust ignore">    fn compile_apply&lt;'guard&gt;(
        &amp;mut self,
        mem: &amp;'guard MutatorView,
        function: TaggedScopedPtr&lt;'guard&gt;,
        args: TaggedScopedPtr&lt;'guard&gt;,
    ) -&gt; Result&lt;Register, RuntimeError&gt; {
        match *function {
            Value::Symbol(s) =&gt; match s.as_str(mem) {
                &quot;quote&quot; =&gt; self.push_load_literal(mem, value_from_1_pair(mem, args)?),
                &quot;atom?&quot; =&gt; self.push_op2(mem, args, |dest, test| Opcode::IsAtom { dest, test }),
                &quot;nil?&quot; =&gt; self.push_op2(mem, args, |dest, test| Opcode::IsNil { dest, test }),
                &quot;car&quot; =&gt; self.push_op2(mem, args, |dest, reg| Opcode::FirstOfPair { dest, reg }),
                &quot;cdr&quot; =&gt; self.push_op2(mem, args, |dest, reg| Opcode::SecondOfPair { dest, reg }),
                &quot;cons&quot; =&gt; self.push_op3(mem, args, |dest, reg1, reg2| Opcode::MakePair {
                    dest,
                    reg1,
                    reg2,
                }),
                &quot;cond&quot; =&gt; self.compile_apply_cond(mem, args),
                &quot;is?&quot; =&gt; self.push_op3(mem, args, |dest, test1, test2| Opcode::IsIdentical {
                    dest,
                    test1,
                    test2,
                }),
                &quot;set&quot; =&gt; self.compile_apply_assign(mem, args),
                &quot;def&quot; =&gt; self.compile_named_function(mem, args),
                &quot;lambda&quot; =&gt; self.compile_anonymous_function(mem, args),
                &quot;\\&quot; =&gt; self.compile_anonymous_function(mem, args),
                &quot;let&quot; =&gt; self.compile_apply_let(mem, args),
                _ =&gt; self.compile_apply_call(mem, function, args),
            },

            // Here we allow the value in the function position to be evaluated dynamically
            _ =&gt; self.compile_apply_call(mem, function, args),
        }
    }
</code></pre>
<p>The <code>function</code> parameter is expected to be a <code>Symbol</code>, that is, have a <em>name</em>
represented by a <code>Symbol</code>. Thus, the function is <code>match</code>ed on the <code>Symbol</code>.</p>
<h3><a class="header" href="#caling-nil" id="caling-nil">Caling nil?</a></h3>
<p>Let's follow the compilation of a simple function: <code>nil?</code>. This is where we'll
start seeing some of the deeper details of compilation, such as register
allocation and</p>
<pre><code class="language-rust ignore">                ...
                &quot;nil?&quot; =&gt; self.push_op2(mem, args, |dest, test| Opcode::IsNil { dest, test }),
                ...
</code></pre>
<p>The function <code>nil?</code> takes a single argument and returns:</p>
<ul>
<li>the <code>Symbol</code> for <code>true</code> if the value of the argument is <code>nil</code></li>
<li><code>nil</code> if the argument is <em>not</em> <code>nil</code>.</li>
</ul>
<p>In compiling this function call, a single bytecode opcode will be pushed on to
the <code>ByteCode</code> array. This is done in the <code>Compiler::push_op2()</code> function. It is
named <code>push_op2</code> because the opcode takes two operands: an argument register
and a result destination register. This function is used to compile all simple
function calls that follow the pattern of one argument, one result value. Here
is <code>push_op2()</code>:</p>
<pre><code class="language-rust ignore">    fn push_op2&lt;'guard, F&gt;(
        &amp;mut self,
        mem: &amp;'guard MutatorView,
        params: TaggedScopedPtr&lt;'guard&gt;,
        f: F,
    ) -&gt; Result&lt;Register, RuntimeError&gt;
    where
        F: Fn(Register, Register) -&gt; Opcode,
    {
        let result = self.acquire_reg();
        let reg1 = self.compile_eval(mem, value_from_1_pair(mem, params)?)?;
        self.bytecode.get(mem).push(mem, f(result, reg1))?;
        Ok(result)
    }
</code></pre>
<p>Let's break the function body down, line by line:</p>
<ol>
<li>
<p><code>let result = self.acquire_reg();</code></p>
<ul>
<li><code>self.acquire_reg()</code>: is called to get an unused register. In this case, we
need a register to store the result value in. This register acquisition
follows a stack approach. Registers are acquired (pushed on to the stack
window) as new variables are declared within a scope, and popped when the
scope is exited.</li>
<li>The type of <code>result</code> is <code>Register</code> which is an alias for <code>u8</code> - an
unsigned int from 0 to 255.</li>
</ul>
</li>
<li>
<p><code>let reg1 = self.compile_eval(mem, value_from_1_pair(mem, params)?)?;</code></p>
<ul>
<li><code>value_from_1_pair(mem, params)?</code>: inspects the argument list and returns
the argument if there is a single one, otherwise returns an error.</li>
<li><code>self.compile_eval(mem, &lt;arg&gt;)?</code>: recurses into the argument to compile it
down to a something that can be applied to the function call.</li>
<li><code>let reg1 = &lt;value&gt;;</code>: where <code>reg1</code> will be the argument register to the
opcode.</li>
</ul>
</li>
<li>
<p><code>self.bytecode.get(mem).push(mem, f(result, reg1))?;</code></p>
<ul>
<li><code>f(result, reg1)</code>: calls function <code>f</code> that will return the opcode with
operands applied in <code>ByteCode</code> format.</li>
<li>In the case of calling <code>nil?</code>, the argument <code>f</code> is:
<ul>
<li><code>|dest, test| Opcode::IsNil { dest, test }</code></li>
</ul>
</li>
<li><code>self.bytecode.get(mem).push(mem, &lt;opcode&gt;)?;</code>: gets the <code>ByteCode</code>
reference and pushes the opcode on to the end of the bytecode array.</li>
</ul>
</li>
<li>
<p><code>Ok(result)</code></p>
<ul>
<li>the result register is returned to the <code>compile_apply()</code> function</li>
</ul>
</li>
</ol>
<p>... and <code>compile_apply()</code> itself returns the result register to <em>it's</em> caller.</p>
<p>The pattern for compiling function application, more generally, is this:</p>
<ul>
<li>acquire a result register</li>
<li>acquire any temporary intermediate result registers</li>
<li>recurse into arguments to compile <em>them</em> first</li>
<li>emit bytecode for the function, pushing opcodes on to the bytecode array and
putting the final result in the result register</li>
<li>release any intermediate registers</li>
<li>return the result register number</li>
</ul>
<p>Compiling <code>nil?</code> was hopefully quite simple. Let's look at something much more
involved, now.</p>
<h3><a class="header" href="#compiling-anonymous-functions" id="compiling-anonymous-functions">Compiling anonymous functions</a></h3>
<p>An anonymous function is defined, syntactically, as:</p>
<pre><code class="language-ignore">(lambda (param1 param2)
  (expr1)
  (expr2)
  (return-expr))
</code></pre>
<p>There are 0 or more parameters and 1 or more expresssions in the body of the
function. The last expression of the body provides the return value.</p>
<p>Function compilation is initiated by <em>apply</em>. This is because a function is a
compound expression and cannot be reduced to a value by a single <em>eval</em>. Here's
the line in <code>compile_apply()</code> that calls anonymous function compilation:</p>
<pre><code class="language-rust ignore">                ...
                &quot;lambda&quot; =&gt; self.compile_anonymous_function(mem, args),
                ...
</code></pre>
<p>Let's look at the type signature of <code>compile_anonymous_function()</code>:</p>
<pre><code class="language-rust ignore">    fn compile_anonymous_function&lt;'guard&gt;(
        &amp;mut self,
        mem: &amp;'guard MutatorView,
        params: TaggedScopedPtr&lt;'guard&gt;,
    ) -&gt; Result&lt;Register, RuntimeError&gt; {
</code></pre>
<p>The <code>params</code> parameter will be expected to be a <code>Pair</code> list: firstly, a list of
parameter names, followed by function body expressions.</p>
<p>The return value from is the same as all the other compilation functions so far:
<code>Result&lt;Register&gt;</code>. The compiled code will return a pointer to the function
object in a register.</p>
<p>Here is the function in full:</p>
<pre><code class="language-rust ignore">    fn compile_anonymous_function&lt;'guard&gt;(
        &amp;mut self,
        mem: &amp;'guard MutatorView,
        params: TaggedScopedPtr&lt;'guard&gt;,
    ) -&gt; Result&lt;Register, RuntimeError&gt; {
        let items = vec_from_pairs(mem, params)?;

        if items.len() &lt; 2 {
            return Err(err_eval(
                &quot;An anonymous function definition must have at least (lambda (params) expr)&quot;,
            ));
        }

        // a function consists of (name (params) expr1 .. exprn)
        let fn_params = vec_from_pairs(mem, items[0])?;
        let fn_exprs = &amp;items[1..];

        // compile the function to a Function object
        let fn_object = compile_function(mem, Some(&amp;self.vars), mem.nil(), &amp;fn_params, fn_exprs)?;

        // load the function object as a literal
        let dest = self.push_load_literal(mem, fn_object)?;

        // if fn_object has nonlocal refs, compile a MakeClosure instruction in addition, replacing
        // the Function register with a Partial with a closure environment
        match *fn_object {
            Value::Function(f) =&gt; {
                if f.is_closure() {
                    self.push(
                        mem,
                        Opcode::MakeClosure {
                            function: dest,
                            dest,
                        },
                    )?;
                }
            }
            // 's gotta be a function
            _ =&gt; unreachable!(),
        }

        Ok(dest)
    }
</code></pre>
<p>After converting <code>Pair</code> lists to <code>Vec</code>s for convenience (wherein parameter names
and function body expressions are separated) the process calls into function
<code>compile_function()</code>, which brings us full circle to <em>eval</em>.</p>
<p>In <code>compile_function()</code>, below:</p>
<ol>
<li>a <code>Scope</code> is instantiated and the parameters are pushed on to this outermost
scope.</li>
<li>the function body expressions are iterated over, <em>eval</em>-ing each one</li>
<li>any upvalues that will be closed over as the compiled-function exits and goes
out of scope have upvalue instructions generated</li>
<li>a <code>Function</code> object is returned with all details necessary to running the
function in the VM environment</li>
</ol>
<p>Here is <code>compile_function()</code>:</p>
<pre><code class="language-rust ignore">    fn compile_function&lt;'guard&gt;(
        mut self,
        mem: &amp;'guard MutatorView,
        name: TaggedScopedPtr&lt;'guard&gt;,
        params: &amp;[TaggedScopedPtr&lt;'guard&gt;],
        exprs: &amp;[TaggedScopedPtr&lt;'guard&gt;],
    ) -&gt; Result&lt;ScopedPtr&lt;'guard, Function&gt;, RuntimeError&gt; {
        // validate function name
        self.name = match *name {
            Value::Symbol(s) =&gt; Some(String::from(s.as_str(mem))),
            Value::Nil =&gt; None,
            _ =&gt; {
                return Err(err_eval(
                    &quot;A function name may be nil (anonymous) or a symbol (named)&quot;,
                ))
            }
        };
        let fn_name = name;

        // validate arity
        if params.len() &gt; 254 {
            return Err(err_eval(&quot;A function cannot have more than 254 parameters&quot;));
        }
        // put params into a list for the Function object
        let fn_params = List::from_slice(mem, params)?;

        // also assign params to the first level function scope and give each one a register
        let mut param_scope = Scope::new();
        self.next_reg = param_scope.push_bindings(params, self.next_reg)?;
        self.vars.scopes.push(param_scope);

        // validate expression list
        if exprs.len() == 0 {
            return Err(err_eval(&quot;A function must have at least one expression&quot;));
        }

        // compile expressions
        let mut result_reg = 0;
        for expr in exprs.iter() {
            result_reg = self.compile_eval(mem, *expr)?;
        }

        // pop parameter scope
        let closing_instructions = self.vars.pop_scope();
        for opcode in &amp;closing_instructions {
            self.push(mem, *opcode)?;
        }

        // finish with a return
        let fn_bytecode = self.bytecode.get(mem);
        fn_bytecode.push(mem, Opcode::Return { reg: result_reg })?;

        let fn_nonlocals = self.vars.get_nonlocals(mem)?;

        Ok(Function::alloc(
            mem,
            fn_name,
            fn_params,
            fn_bytecode,
            fn_nonlocals,
        )?)
    }
</code></pre>
<p>Note that in addition to generating upvalue instructions as the
compiled-function goes out of scope, the calling compiler function
<code>compile_anonymous_function()</code> will issue a <code>MakeClosure</code> opcode such that a
closure object is put in the return register rather than a direct <code>Function</code>
object reference.</p>
<p>In our language, a closure object is represented by the <code>Partial</code> data structure</p>
<ul>
<li>a struct that represents a <code>Function</code> object pointer plus closed over values
and/or partially applied arguments. This data structure was described in the
chapter <a href="./chapter-interp-vm-impl.html">Virtual Machine: Implementation</a>.</li>
</ul>
<p>Thus ends our tour of our interpreter.</p>
<h2><a class="header" href="#concluding-remarks" id="concluding-remarks">Concluding remarks</a></h2>
<p>In this section, we've looked at a ground-up compiler and virtual machine
implementation within a memory-safe allocation system.</p>
<p>There is, of course, much more to explore in the VM and compiler source code.
The reader is encouraged to experiment with running and modifying the source.</p>
<h1><a class="header" href="#404---this-chapter-has-not-yet-been-written" id="404---this-chapter-has-not-yet-been-written">404 - this chapter has not yet been written</a></h1>
<h1><a class="header" href="#404---this-chapter-has-not-yet-been-written-1" id="404---this-chapter-has-not-yet-been-written-1">404 - this chapter has not yet been written</a></h1>
<h1><a class="header" href="#404---this-chapter-has-not-yet-been-written-2" id="404---this-chapter-has-not-yet-been-written-2">404 - this chapter has not yet been written</a></h1>
<h1><a class="header" href="#404---this-chapter-has-not-yet-been-written-3" id="404---this-chapter-has-not-yet-been-written-3">404 - this chapter has not yet been written</a></h1>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                

                
            </nav>

        </div>

        

        

        

        
        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        

        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        
        
        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
        
        

    </body>
</html>
